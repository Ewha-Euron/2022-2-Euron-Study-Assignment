{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhUHfXkPAORh"
      },
      "source": [
        "📌 week5 내용 주차에 해당되는 과제는 Glove 모델 실습, NER task 실습, Dependency Parsing task 실습으로 구성되어 있습니다. (**참고** : **제출은 week6 branch 복습과제로!**)\n",
        "\n",
        "📌 위키독스의 딥러닝을 이용한 자연어 처리 입문 교재 실습, 캐글 노트북 등의 자료로 구성되어있는 과제입니다. \n",
        "\n",
        "📌 안내된 링크에 맞추어 **직접 코드를 따라 치면서 (필사)** 해당 nlp task 의 기본적인 라이브러리와 메서드를 숙지해보시면 좋을 것 같습니다😊 필수라고 체크한 부분은 과제에 반드시 포함시켜주시고, 선택으로 체크한 부분은 자율적으로 스터디 하시면 됩니다.\n",
        "\n",
        "📌 궁금한 사항은 깃허브 이슈나, 카톡방, 세션 발표 시작 이전 시간 등을 활용하여 자유롭게 공유해주세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XjTSbcxBB6o",
        "outputId": "b26abeed-235d-42a3-c6a4-c5cb4e902c65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "# nltk colab 환경에서 실행시 필요한 코드입니다. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vPZn15zBHIv"
      },
      "source": [
        "### 1️⃣ **Glove**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P11biHcUuBaH"
      },
      "source": [
        "👀 **내용 복습** \n",
        "* 스탠포드 대학에서 개발한 카운트 기반과 예측 기반을 모두 사용하는 단어 임베딩 방법론 \n",
        "* word2vec 의 단점을 보완해서 나온 모델 \n",
        "* glove model 의 **input 은 반드시 동시등장행렬 형태**여야 한다 ⭐\n",
        "\n",
        "![1](https://www.dropbox.com/s/nz0ji4yzre56ifv/word_presentation.png?raw=1) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "🤔 한국어 예제는 없는 것 같습니다. 논문에서는 k-Glove 로 소개되는 연구가 있긴 한데, 좀 더 알아봐야 할 것 같아요!\n",
        "\n",
        "➕ [논문1](https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=NPAP13255003&dbt=NPAP)\n",
        "\n",
        "\n",
        "➕[논문2](https://scienceon.kisti.re.kr/commons/util/originalView.do?cn=CFKO201832073078664&oCn=NPAP13255064&dbt=CFKO&journal=NPRO00383361&keyword=%ED%95%9C%EA%B5%AD%EC%96%B4%20%EB%8C%80%ED%99%94%20%EC%97%94%EC%A7%84%EC%97%90%EC%84%9C%EC%9D%98%20%EB%AC%B8%EC%9E%A5%EB%B6%84%EB%A5%98)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asGcGy6fBM1E"
      },
      "source": [
        "🔹 **1-(1)** glove python\n",
        "\n",
        "* [실습 : basic code](https://wikidocs.net/22885) 👉 필수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V31NoJdu5t3p",
        "outputId": "eda0d932-8bd4-4b2e-c3e9-47970e4da1d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting glove_python_binary\n",
            "  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 31.5 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 92 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 102 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 112 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 122 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 133 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 143 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 153 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 163 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 174 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 184 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 194 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 204 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 215 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 225 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 235 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 245 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 256 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 266 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 276 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 286 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 296 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 307 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 317 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 327 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 337 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 348 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 358 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 368 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 378 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 389 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 399 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 409 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 419 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 430 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 440 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 450 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 460 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 471 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 481 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 491 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 501 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 512 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 522 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 532 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 542 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 552 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 563 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 573 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 583 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 593 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 604 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 614 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 624 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 634 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 645 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 655 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 665 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 675 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 686 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 696 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 706 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 716 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 727 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 737 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 747 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 757 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 768 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 778 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 788 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 798 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 808 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 819 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 829 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 839 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 849 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 860 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 870 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 880 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 890 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 901 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 911 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 921 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 931 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 942 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 948 kB 14.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.21.5)\n",
            "Installing collected packages: glove-python-binary\n",
            "Successfully installed glove-python-binary-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install glove_python_binary\n",
        "\n",
        "import re\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 데이터 다운로드\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")\n",
        "\n",
        "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "target_text = etree.parse(targetXML)\n",
        "\n",
        "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
        "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
        "sent_text = sent_tokenize(content_text)\n",
        "\n",
        "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta6QgoKO5uXJ"
      },
      "outputs": [],
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus() \n",
        "\n",
        "# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n",
        "corpus.fit(result, window=5)\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "\n",
        "# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "\n",
        "print(glove.most_similar(\"man\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ADfVM9lO9NE"
      },
      "source": [
        "🔹 **1-(2)** pre-trained glove \n",
        "\n",
        "* **사전학습모델** : 임의의 값으로 초기화하던 모델의 가중치들을 다른 문제에 학습시킨 가중치들로 초기화하는 방법이다.사전 학습한 가중치를 활용해 학습하고자 하는 본래 문제를 하위문제라고 한다. \n",
        "\n",
        "* [실습 : 문장의 긍부정을 판단하는 감성 분류 모델 만들기](https://wikidocs.net/33793) 👉 필수\n",
        "  * [설명참고](https://omicro03.medium.com/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-16%EC%9D%BC%EC%B0%A8-pre-trained-word-embedding-bb30db424a35)\n",
        "* pre-trained data 를 가져오는데 시간이 오래걸림\n",
        "* kaggle 대회에서 주로 이 방식을 많이 사용함\n",
        "  * [참고](https://lsjsj92.tistory.com/455)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlngY35O53sk"
      },
      "outputs": [],
      "source": [
        "# 2) 임베딩 층 사용하기\n",
        "\n",
        "# 문장의 긍, 부정을 판단하는 감성 분류 모델을 만들어봅시다\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
        "y_train = [1, 0, 0, 1, 1, 0, 1]\n",
        "\n",
        "# 케라스의 토크나이저 사용하여 단어집합 만들고 그 크기 확인하기\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1 # padding 고려\n",
        "print('단어 집합', vocab_size)\n",
        "\n",
        "# 각 문장에 대해서 정수 인코딩을 수행합니다.\n",
        "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print('정수 인코딩 결과 : ', X_encoded)\n",
        "\n",
        "#가장 길이가 긴 문장의 길이를 구합니다. \n",
        "max_len = max(len(l) for l in X_encoded)\n",
        "print('최대길이 :', max_len)\n",
        "\n",
        "# 최대 길이로 모든 샘플에 대해서 패딩을 진행합니다.\n",
        "X_train = pad_sequences(X_encoded, maxlen= max_len, padding='post')\n",
        "y_train = np.array(y_train)\n",
        "print('패딩 결과:')\n",
        "print(X_train)\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tnesorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "embedding_dim = 4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train,y_train, epochs=100, verbose=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2. 사전 훈련된 워드 임베딩(Pre-Trained Word Embedding) 사용하기\n",
        "\n",
        "'''케라스의 Embedding()을 사용하여 처음부터 임베딩 벡터값을 학습하기도 하지만, 때로는 이미 \n",
        "훈련되어져 있는 워드 임베딩을 가져와서 이를 임베딩 벡터로 사용하기도 합니다. 훈련 데이터가 적은 상황이라면\n",
        " 케라스의 Embedding()으로 해당 문제를 풀기에 최적화 된 임베딩 벡터값을 얻는 것이 쉽지 않습니다. \n",
        " 이 경우 해당 문제에 특화된 것은 아니지만 보다 많은 훈련 데이터로 이미 Word2Vec이나 GloVe 등으로 \n",
        " 학습되어져 있는 임베딩 벡터들을 사용하는 것이 성능의 개선을 가져올 수 있습니다.\n",
        "사전 훈련된 GloVe와 Word2Vec 임베딩을 사용해서 모델을 훈련시키는 실습을 진행해봅시다.'''\n",
        "\n",
        "# 훈련 데이터는 앞서 사용했던 데이터에 동일한 전처리까지 진행된 상태라고 가정하겠습니다.\n",
        "\n",
        "print(X_train)\n",
        "print(y_train)\n",
        "\n",
        "\n",
        "# 1) 사전 훈련된 GloVe 사용하기\n",
        "from urllib.request import urlretrieve, urlopen\n",
        "import gzip\n",
        "import zipfile\n",
        "\n",
        "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
        "zf = zipfile.ZipFile('glove.6B.zip')\n",
        "zf.extractall() \n",
        "zf.close()\n",
        "\n",
        "# glove.6B.100d.txt에 있는 모든 임베딩 벡터들을 불러옵니다. 파이썬의 자료구조 딕셔너리(dictionary)를 사용하며, 로드한 임베딩 벡터의 개수를 확인합니다.\n",
        "\n",
        "embedding_dict = dict()\n",
        "\n",
        "f= open('glove.6B.100d.txt', encoding='utf8')\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "\n",
        "\n",
        "    # 100개의 값을 가지는 array로 변환\n",
        "    word_vector_arr = np.asarray(word_vector[1:],dtype='float32')\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "\n",
        "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))\n",
        "\n",
        "# 총 40만개의 임베딩 벡터가 존재합니다. 임의의 단어 'respectable'의 임베딩 벡터값과 크기를 출력해봅니다.\n",
        "print(embedding_dict['respectable'])\n",
        "print('벡터의 차원 수 :',len(embedding_dict['respectable']))\n",
        "\n",
        "# 벡터값이 출력되며 벡터의 차원 수는 100입니다. 풀고자 하는 문제의 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성합니다. 이 행렬의 값은 전부 0으로 채웁니다. 이 행렬에 사전 훈련된 임베딩 값을 넣어줄 것입니다.\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "print('임베딩 행렬의 크기(shape) :',np.shape(embedding_matrix)\n",
        "\n",
        "# 기존 데이터의 각 단어와 맵핑된 정수값을 확인해봅시다.\n",
        "\n",
        "print(tokenizer.word_index.items())\n",
        "\n",
        "# 단어 'great'의 맵핑된 정수는 2입니다.\n",
        "\n",
        "print('단어 great의 맵핑된 정수 :',tokenizer.word_index['great']\n",
        "\n",
        "# 사전 훈련된 GloVe에서 'great'의 벡터값을 확인합니다.\n",
        "\n",
        "print(embedding_dict['great'])\n",
        "\n",
        "\n",
        "# 단어 집합의 모든 단어에 대해서 사전 훈련된 GloVe의 임베딩 벡터들을 맵핑한 후 'great'의 벡터값이 의도한 인덱스의 위치에 삽입되었는지 확인해보겠습니다\n",
        "\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
        "    vector_value = embedding_dict.get(word)\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value\n",
        "\n",
        "# embedding_matrix의 인덱스 2에서의 값을 확인합니다.\n",
        "embedding_matrix[2]\n",
        "\n",
        "\n",
        "# 이전에 확인한 사전에 훈련된 GloVe에서의 'great'의 벡터값과 일치합니다. \n",
        "# 이제 Embedding layer에 embedding_matrix를 초기값으로 설정합니다. \n",
        "# 현재 실습에서 사전 훈련된 워드 임베딩을 100차원의 값인 것으로 사용하고 있기 때문에 \n",
        "# 임베딩 층의 output_dim의 인자값으로 100을 주어야 합니다. 그리고 사전 훈련된 워드 임베딩을 \n",
        "# 그대로 사용할 경우 추가 훈련을 하지 않는다는 의미에서 trainable의 인자값을 False로 선택할 수 있습니다.\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "output_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, output_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_wcrE5PtLMI"
      },
      "source": [
        "🔹 **1-(3)** fine tuning glove\n",
        "* 미세조정 : 사전 학습한 모든 가중치와 더불어 하위 문제를 위한 최소한의 가중치를 추가해 모델을 추가로 학습하는 방법이다. \n",
        "\n",
        "* fine tuning 이 필요한 경우 \n",
        "  * pretrained model 에 데이터셋에 있는 단어가 포함되지 않은 경우 \n",
        "  * 데이터 집합이 너무 작아서 전체 모델을 훈련시키기 어려운 경우 \n",
        "\n",
        "* [Mittens 라이브러리로 fine tuning](https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39) 👉 필수\n",
        "  *  GloVe 임베딩을 fine-tuning 하기 위한 파이썬 라이브러리\n",
        "  * [github](https://github.com/roamanalytics/mittens)\n",
        "\n",
        "* [한국어 소설 텍스트 데이터 미세조정 모델 학습 - GPT2](https://m.blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=horajjan&logNo=222104684132&categoryNo=120&proxyReferer=) 👉 선택 (glove 모델 예제는 아닙니다. fine-tuning 에 초점을 두어서 참고해주시면 좋을 것 같습니다.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDBDn64S58U5"
      },
      "outputs": [],
      "source": [
        "# Mittens를 사용하여 GloVe 임베딩 미세 조정\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.corpus import brown\n",
        "from mittens import GloVe, Mittens\n",
        "from sklearn.feature_extraction import stop_words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def glove2dict(glove_filename):\n",
        "    with open(glove_filename, encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
        "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
        "                for line in reader}\n",
        "    return embed\n",
        "\n",
        "glove_path = \"glove.6B.50d.txt\"\n",
        "pre_glove = glove2dict(glove_path)\n",
        "\n",
        "## Data preprocessing\n",
        "# 단어의 동시 발생 매트릭스를 작성하기 전에 데이터 세트에서 사전 처리를 수행해 보겠습니다.\n",
        "\n",
        "sw = list(stop_words.ENGLISH_STOP_WORDS)\n",
        "brown_data = brown.words()[:200000]\n",
        "brown_nonstop = [token.lower() for token in brown_data if (token.lower() not in sw)]\n",
        "oov = [token for token in brown_nonstop if token not in pre_glove.keys()]\n",
        "\n",
        "# oov represents the vocabulary not present in pretrained glove. \n",
        "# The co-occurrence matrix is built from oovs. \n",
        "# It is a sparse matrix, requiring a space complexity of O(n^2). \n",
        "# Thus sometimes the really rare oov words has to be filtered out to save space. This is an optional step.\n",
        "\n",
        "def get_rareoov(xdict, val):\n",
        "    return [k for (k,v) in Counter(xdict).items() if v<=val]\n",
        "oov_rare = get_rareoov(oov, 1)\n",
        "corp_vocab = list(set(oov) - set(oov_rare))\n",
        "\n",
        "# remove those rare oovs, if needed and prepare the dataset\n",
        "\n",
        "brown_tokens = [token for token in brown_nonstop if token not in oov_rare]\n",
        "brown_doc = [' '.join(brown_tokens)]\n",
        "corp_vocab = list(set(oov))\n",
        "\n",
        "## Building co-occurrence matrix:\n",
        "# We need word-word co-occurrence not the usual term-document matrix. \n",
        "# sklearn’s CountVectorizer transforms the document into word-doc matrix. \n",
        "# The matrix multiplication Xt*X gives the word-word co-occurrence matrix\n",
        "\n",
        "cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
        "X = cv.fit_transform(brown_doc)\n",
        "Xc = (X.T * X)\n",
        "Xc.setdiag(0)\n",
        "coocc_ar = Xc.toarray()\n",
        "\n",
        "## Fine-tuning the mittens model\n",
        "\n",
        "mittens_model = Mittens(n=50, max_iter=1000)\n",
        "new_embeddings = mittens_model.fit(\n",
        "    coocc_ar,\n",
        "    vocab=corp_vocab,\n",
        "    initial_embedding_dict= pre_glove)\n",
        "\n",
        "## Save the model as pickle for future use.\n",
        "\n",
        "newglove = dict(zip(corp_vocab, new_embeddings))\n",
        "f = open(\"repo_glove.pkl\",\"wb\")\n",
        "pickle.dump(newglove, f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHiR5mN4577l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_-OB9Siga3G"
      },
      "source": [
        "* (참고) word2vec pretrained example\n",
        "\n",
        "➕ [word2vec 사전학습 모델 -한국어1](http://doc.mindscale.kr/km/unstructured/11.html)\n",
        "\n",
        "➕ [word2vec 사전학습 - 한국어2](https://monetd.github.io/python/nlp/Word-Embedding-Word2Vec-%EC%8B%A4%EC%8A%B5/#%ED%95%9C%EA%B5%AD%EC%96%B4-word2vec-%EB%A7%8C%EB%93%A4%EA%B8%B0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUWWDwdiPLS9"
      },
      "source": [
        "### **2️⃣ NER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N0B4VknPkTk"
      },
      "source": [
        "👀 **내용 복습** \n",
        "* 개체명 인식을 사용하면 코퍼스로부터 어떤 단어가 사람, 장소, 조직 등을 의미하는 단어인지를 찾을 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWgla1BuPRqJ"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "🔹 **2-(1)** NER task by nltk library\n",
        "\n",
        "\n",
        "* nltk 에서는 개체명 인식기 (NER chunker) 를 지원하고 있다. \n",
        "* ne_chunk 는 개체명을 태깅하기 위해서 앞서 품사 태깅 pos_tag 가 수행되어야 한다. \n",
        "\n",
        "\n",
        "📌 [basic code](https://wikidocs.net/30682) 👉 필수 \n",
        "\n",
        "📌 [BIO 표현, LSTM을 활용한 NER 실습](https://wikidocs.net/24682) 👉 선택\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diaZweMyAxJz"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "sentence = \"James is working at Disney in London\"\n",
        "# 토큰화 후 품사 태깅\n",
        "tokenized_sentence = pos_tag(word_tokenize(sentence))\n",
        "print(tokenized_sentence)\n",
        "\n",
        "# 개체명 인식\n",
        "ner_sentence = ne_chunk(tokenized_sentence)\n",
        "print(ner_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1k09tKha3Lgi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPX-WtSvPmm6"
      },
      "source": [
        "🔹 **2-(2)** NER task by spacy library\n",
        "\n",
        "\n",
        "* spaCy 는 자연어처리를 위한 파이썬 기반의 오픈 소스 라이브러리로 다음과 같은 기능을 제공한다. \n",
        "  * Tokenization \n",
        "  * POS tagging \n",
        "  * Lemmatization \n",
        "  * Sentence Boundary Detection (SBD)\n",
        "  * Named Entity Recognition (NER)\n",
        "  * Similarity\n",
        "  * Text Classification\n",
        "  * Rule-based Matching\n",
        "  * Training\n",
        "  * Serialization\n",
        "\n",
        "* spaCy 와 NER\n",
        "  * .ents → .label_\n",
        "\n",
        "\n",
        "📌 [basic code](https://frhyme.github.io/python-lib/nlp_spacy_1/) 👉 필수 (NER 부분만)\n",
        "\n",
        "📌 [kaggle_Custom NER using SpaCy](https://www.kaggle.com/code/amarsharma768/custom-ner-using-spacy/notebook) 👉 선택\n",
        "\n",
        "  * 훈련되지 않은 데이터 세트에 명명된 엔티티를 학습하는 방법 : 이력서 pdf 데이터 활용 \n",
        "  * manually labelled \n",
        "\n",
        "📌 [한국어 NER](https://github.com/monologg/KoBERT-NER) 👉 참고하면 좋을 자료\n",
        "\n",
        "➕ [참고](http://aispiration.com/nlp2/nlp-ner-python.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WXjRfz-qP0Xx"
      },
      "outputs": [],
      "source": [
        "import spacy \n",
        "## spacy에서 내가 원하는 언어의 모델을 가져오기\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "## 문장을 nlp에 넘기기\n",
        "doc = nlp('Apple is looking at buyin at U.K startup for $1 billion.')\n",
        "print(type(doc)) \n",
        "print(doc)\n",
        "print(list(doc))## 리스트로 바꾸면 tokenize한 결과가 나오고 \n",
        "print(type(doc[0]))## 리스트 가장 앞에 있는 값은 Token이다.\n",
        "\n",
        "\n",
        "## part of speech tagging \n",
        "\n",
        "temp_str = \"\"\"\n",
        "Text: The original word text.\n",
        "Lemma: The base form of the word.(축약한 상태)\n",
        "POS: The simple part-of-speech tag.\n",
        "Tag: The detailed part-of-speech tag.\n",
        "Dep: Syntactic dependency, i.e. the relation between tokens.\n",
        "Shape: The word shape – capitalisation, punctuation, digits.\n",
        "is alpha: Is the token an alpha character?\n",
        "is stop: Is the token part of a stop list, i.e. the most common words of the language?\n",
        "\"\"\".strip()\n",
        "print(temp_str)\n",
        "print(\"==\"*40)\n",
        "\n",
        "str_format = \"{:>10}\"*8\n",
        "print(str_format.format(*temp_dict.keys()))\n",
        "print(\"==\"*40)\n",
        "\n",
        "doc = nlp('Apple is looking at buying U.K. startup for $1 billion')\n",
        "for token in doc:\n",
        "    print(str_format.format(token.text, token.lemma_, token.pos_, token.tag_, \n",
        "                            token.dep_, token.shape_, str(token.is_alpha), str(token.is_stop)))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "008-V5QsQG25"
      },
      "source": [
        "###**3️⃣ Dependency Parsing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQfcodHQQPlt"
      },
      "source": [
        "👀 **내용 복습** \n",
        "* 문장의 전체적인 구성/구조 보다는 각 개별단어 간의 '의존관계' 또는 '수식관계' 와 같은 단어간 관계를 파악하는 것이 목적인 NLP Task\n",
        "* 문장 해석의 모호성을 없애기 위해 Parsing 을 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJLAzZnbRNlL"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "🔹 **3-(1)** Dependency Parsing by spacy library\n",
        "\n",
        "\n",
        "* [basic](https://frhyme.github.io/python-lib/nlp_spacy_1/#navigating-parse-tree) 👉 dependecy parsing 부분만 필수\n",
        "* .dep_ 메서드\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbQEYt76bJXz"
      },
      "outputs": [],
      "source": [
        "# noun chunking\n",
        "\n",
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        " \n",
        "## noun_chunks의 경우는 token 클래스도 아니고, Doc 클래스도 아니다. \n",
        "## Span이라는 클래스는 그냥 Doc와 비슷함, 일종의 복합어 개념.\n",
        "noun_chunks = doc.noun_chunks\n",
        "print(type(noun_chunks))\n",
        "noun_chunk = list(noun_chunks)[0]\n",
        "print(type(noun_chunk))\n",
        "token = noun_chunk[0]\n",
        "print(type(token))\n",
        "\n",
        "print(\"==\"*30)\n",
        "print(\"\"\"\n",
        "Text: The original noun chunk text.\n",
        "Root text: The original text of the word connecting the noun chunk to the rest of the parse.\n",
        "Root dep: Dependency relation connecting the root to its head.\n",
        "Root head text: The text of the root token's head.\n",
        "\"\"\".strip())\n",
        "print(\"==\"*30)\n",
        "str_format = \"{:>25}\"*4\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(str_format.format(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9QAEsrLAxHP"
      },
      "outputs": [],
      "source": [
        "# navigating parse tree\n",
        "\n",
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "for tok in doc:\n",
        "    print(tok.text)\n",
        "    children = list(tok.children)\n",
        "    print('children:', children, 'head:', tok.head if tok.head != tok else \"!this is root node\")\n",
        "    print(\"==\"*16)\n",
        "\n",
        "\n",
        "# 네트워크 표현 \n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "nG = nx.Graph()\n",
        "doc[2] ## root node\n",
        "\n",
        "def add_n_to_g(inputG, tok):\n",
        "    inputG.add_node(tok)\n",
        "    children = list(tok.children)\n",
        "    if children != []:\n",
        "        inputG.add_nodes_from(children)\n",
        "        for c in children:\n",
        "            inputG.add_edges_from([(tok, c, {'dependency':c.dep_})])\n",
        "            add_n_to_g(inputG, c)\n",
        "add_n_to_g(nG, doc[2])\n",
        "print(nG.nodes(data=True))\n",
        "print(\"==\"*20)\n",
        "for e in nG.edges(data=True):\n",
        "    print(f\"{e[0]}, {e[1]}, ### dependency: {e[2]['dependency']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQD5oiGgRfHe"
      },
      "source": [
        "🔹 **3-(2)** Spacy (kaggle) \n",
        "\n",
        "* 캐글 노트북 환경에서 실습해보는 것을 권장드립니다!\n",
        "\n",
        "* [kaggle_spaCy](https://www.kaggle.com/code/nirant/hitchhiker-s-guide-to-nlp-in-spacy) 👉 필수\n",
        "  * 도날드 트럼프 트위터 트윗 내용 데이터 분석\n",
        "\n",
        "\n",
        "👀 **노트북 키포인트** \n",
        "  1. spacy.display 메서드를 사용한 NER 시각화 \n",
        "  2. Tagging 을 통한 트럼프 트윗 분석 : noun_chunks 는 dependency graph를 고려하여, noun phrase를 뽑아준다. \n",
        "  3. [spacy Match](https://yujuwon.tistory.com/entry/spaCy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-Rule-based-Matching) : 직접 문장/단어 패턴을 등록하여 parsing\n",
        "  4. Question and answering task using Dependency Parsing\n",
        "    * spacy display :  ``style = 'dep'``\n",
        "    * .dep_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuHGKITRbKYq"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "# python -m spacy download en_vectors_web_lg\n",
        "\n",
        "tweets = pd.read_csv(\"../input/all-djtrum-tweets/all_djt_tweets.csv\")\n",
        "\n",
        "def explain_text_entities(text):\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        print(f'Entity: {ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')\n",
        "\n",
        "\n",
        "explain_text_entities(tweets['text'][9])\n",
        "\n",
        "one_sentence = tweets['text'][0]\n",
        "doc = nlp(one_sentence)\n",
        "spacy.displacy.render(doc, style='ent',jupyter=True)\n",
        "\n",
        "\n",
        "# Redacting Names\n",
        "def redact_names(text):\n",
        "    doc = nlp(text)\n",
        "    redacted_sentence = []\n",
        "    for ent in doc.ents:\n",
        "        ent.merge()\n",
        "    for token in doc:\n",
        "        if token.ent_type_ == \"PERSON\":\n",
        "            redacted_sentence.append(\"[REDACTED]\")\n",
        "        else:\n",
        "            redacted_sentence.append(token.string)\n",
        "    return \"\".join(redacted_sentence)\n",
        "\n",
        "printmd(\"**Before**\", color=\"blue\")\n",
        "one_sentence = tweets['text'][450]\n",
        "doc = nlp(one_sentence)\n",
        "spacy.displacy.render(doc, style='ent',jupyter=True)\n",
        "printmd(\"**After**\", color=\"blue\")\n",
        "one_sentence = redact_names(tweets['text'][450])\n",
        "doc = nlp(one_sentence)\n",
        "spacy.displacy.render(doc, style='ent',jupyter=True)\n",
        "\n",
        "printmd(\"Notice that `Obama W.H.` was removed\", color=\"#6290c8\")\n",
        "\n",
        "# Part-of-Speech Tagging\n",
        "\n",
        "example_text = tweets['text'][9]\n",
        "doc = nlp(example_text)\n",
        "spacy.displacy.render(doc, style='ent', jupyter=True)\n",
        "\n",
        "for idx, sentence in enumerate(doc.sents):\n",
        "    for noun in sentence.noun_chunks:\n",
        "        print(f\"sentence {idx+1} has noun chunk '{noun}'\")\n",
        "\n",
        "one_sentence = tweets['text'][300]\n",
        "doc = nlp(one_sentence)\n",
        "spacy.displacy.render(doc, style='ent', jupyter=True)\n",
        "\n",
        "for token in doc:\n",
        "    print(token, token.pos_)\n",
        "\n",
        "\n",
        "# What does Trump talk about?\n",
        "text = tweets['text'].str.cat(sep=' ')\n",
        "max_length = 1000000-1\n",
        "text = text[:max_length]\n",
        "\n",
        "# removing URLs and '&amp' substrings using regex\n",
        "import re\n",
        "url_reg  = r'[a-z]*[:.]+\\S+'\n",
        "text   = re.sub(url_reg, '', text)\n",
        "noise_reg = r'\\&amp'\n",
        "text   = re.sub(noise_reg, '', text)\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "items_of_interest = list(doc.noun_chunks)\n",
        "# each element in this list is spaCy's inbuilt `Span`, which is not useful for us\n",
        "items_of_interest = [str(x) for x in items_of_interest]\n",
        "# so we've converted it to string\n",
        "\n",
        "# Let's remove these filler words and try again!\n",
        "trump_topics = []\n",
        "for token in doc:\n",
        "    if (not token.is_stop) and (token.pos_ == \"NOUN\") and (len(str(token))>2):\n",
        "        trump_topics.append(token)\n",
        "        \n",
        "trump_topics = [str(x) for x in trump_topics]\n",
        "\n",
        "\n",
        "# Exploring Entities\n",
        "\n",
        "trump_topics = []\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ not in [\"PERCENT\", \"CARDINAL\", \"DATE\"]:\n",
        "#         print(ent.text,ent.label_)\n",
        "        trump_topics.append(ent.text.strip())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMArOUrxAJ8K"
      },
      "outputs": [],
      "source": [
        "#Using Linguistic annotations with spaCy Match\n",
        "\n",
        "from spacy.matcher import Matcher\n",
        "# doc = nlp(text)\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matched_sents = [] \n",
        "\n",
        "def collect_sents(matcher, doc, i, matches, label='MATCH'):\n",
        "    \"\"\"\n",
        "    Function to help reformat data for displacy visualization\n",
        "    \"\"\"\n",
        "    match_id, start, end = matches[i]\n",
        "    span = doc[start : end]  # matched span\n",
        "    sent = span.sent  # sentence containing matched span\n",
        "    \n",
        "    # append mock entity for match in displaCy style to matched_sents\n",
        "    \n",
        "    if doc.vocab.strings[match_id] == 'DEMOCRATS':  # don't forget to get string!\n",
        "        match_ents = [{'start': span.start_char - sent.start_char,\n",
        "                   'end': span.end_char - sent.start_char,\n",
        "                   'label': 'DEMOCRATS'}]\n",
        "        matched_sents.append({'text': sent.text, 'ents': match_ents })\n",
        "    elif doc.vocab.strings[match_id] == 'RUSSIA':  # don't forget to get string!\n",
        "        match_ents = [{'start': span.start_char - sent.start_char,\n",
        "               'end': span.end_char - sent.start_char,\n",
        "               'label': 'RUSSIA'}]\n",
        "        matched_sents.append({'text': sent.text, 'ents': match_ents })\n",
        "    elif doc.vocab.strings[match_id] == 'I':  # don't forget to get string!\n",
        "        match_ents = [{'start': span.start_char - sent.start_char,\n",
        "               'end': span.end_char - sent.start_char,\n",
        "               'label': 'NARC'}]\n",
        "        matched_sents.append({'text': sent.text, 'ents': match_ents })\n",
        "    \n",
        "# declare different patterns\n",
        "russia_pattern = [{'LOWER': 'russia'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'},\n",
        "           {'POS': 'ADJ'}]\n",
        "democrats_pattern = [{'LOWER': 'democrats'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'},\n",
        "           {'POS': 'ADJ'}]\n",
        "i_pattern = [{'LOWER': 'i'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'},\n",
        "           {'POS': 'ADJ'}]\n",
        "\n",
        "matcher.add('DEMOCRATS', collect_sents, democrats_pattern)  # add pattern\n",
        "matcher.add('RUSSIA', collect_sents, russia_pattern)  # add pattern\n",
        "matcher.add('I', collect_sents, i_pattern)  # add pattern\n",
        "matches = matcher(doc)\n",
        "\n",
        "spacy.displacy.render(matched_sents, style='ent', manual=True, jupyter=True,  options = {'colors': {'NARC': '#6290c8', 'RUSSIA': '#cc2936', 'DEMOCRATS':'#f2cd5d'}})\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}