{"cells":[{"cell_type":"markdown","metadata":{"id":"QhUHfXkPAORh"},"source":["📌 week5 내용 주차에 해당되는 과제는 Glove 모델 실습, NER task 실습, Dependency Parsing task 실습으로 구성되어 있습니다. (**참고** : **제출은 week6 branch 복습과제로!**)\n","\n","📌 위키독스의 딥러닝을 이용한 자연어 처리 입문 교재 실습, 캐글 노트북 등의 자료로 구성되어있는 과제입니다. \n","\n","📌 안내된 링크에 맞추어 **직접 코드를 따라 치면서 (필사)** 해당 nlp task 의 기본적인 라이브러리와 메서드를 숙지해보시면 좋을 것 같습니다😊 필수라고 체크한 부분은 과제에 반드시 포함시켜주시고, 선택으로 체크한 부분은 자율적으로 스터디 하시면 됩니다.\n","\n","📌 궁금한 사항은 깃허브 이슈나, 카톡방, 세션 발표 시작 이전 시간 등을 활용하여 자유롭게 공유해주세요!"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1665412546728,"user":{"displayName":"­김경민(엘텍공과대학 소프트웨어학부)","userId":"06637781437683440716"},"user_tz":-540},"id":"3XjTSbcxBB6o","outputId":"31b2f024-725d-42fb-a0fb-39b044f0b645"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["import nltk\n","# nltk colab 환경에서 실행시 필요한 코드입니다. \n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')"]},{"cell_type":"markdown","metadata":{"id":"-vPZn15zBHIv"},"source":["### 1️⃣ **Glove**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"P11biHcUuBaH"},"source":["👀 **내용 복습** \n","* 스탠포드 대학에서 개발한 카운트 기반과 예측 기반을 모두 사용하는 단어 임베딩 방법론 \n","* word2vec 의 단점을 보완해서 나온 모델 \n","* glove model 의 **input 은 반드시 동시등장행렬 형태**여야 한다 ⭐\n","\n","![1](https://www.dropbox.com/s/nz0ji4yzre56ifv/word_presentation.png?raw=1) \n","\n","\n","\n","\n","🤔 한국어 예제는 없는 것 같습니다. 논문에서는 k-Glove 로 소개되는 연구가 있긴 한데, 좀 더 알아봐야 할 것 같아요!\n","\n","➕ [논문1](https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=NPAP13255003&dbt=NPAP)\n","\n","\n","➕[논문2](https://scienceon.kisti.re.kr/commons/util/originalView.do?cn=CFKO201832073078664&oCn=NPAP13255064&dbt=CFKO&journal=NPRO00383361&keyword=%ED%95%9C%EA%B5%AD%EC%96%B4%20%EB%8C%80%ED%99%94%20%EC%97%94%EC%A7%84%EC%97%90%EC%84%9C%EC%9D%98%20%EB%AC%B8%EC%9E%A5%EB%B6%84%EB%A5%98)"]},{"cell_type":"markdown","metadata":{"id":"asGcGy6fBM1E"},"source":["🔹 **1-(1)** glove python\n","\n","* [실습 : basic code](https://wikidocs.net/22885) 👉 필수"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V31NoJdu5t3p","outputId":"f124123d-9e9f-406f-ae91-82c40259076f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: glove_python_binary in /usr/local/lib/python3.7/dist-packages (0.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.7.3)\n"]}],"source":["!pip install glove_python_binary\n","\n","import re\n","import urllib.request\n","import zipfile\n","from lxml import etree\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","# 데이터 다운로드\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")\n","\n","targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n","target_text = etree.parse(targetXML)\n","\n","# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n","parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n","\n","# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n","# 해당 코드는 괄호로 구성된 내용을 제거.\n","content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n","\n","# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n","sent_text = sent_tokenize(content_text)\n","\n","# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n","normalized_text = []\n","for string in sent_text:\n","     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n","     normalized_text.append(tokens)\n","\n","# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n","result = [word_tokenize(sentence) for sentence in normalized_text]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ta6QgoKO5uXJ"},"outputs":[],"source":["from glove import Corpus, Glove\n","\n","corpus = Corpus() \n","\n","# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n","corpus.fit(result, window=5)\n","glove = Glove(no_components=100, learning_rate=0.05)\n","\n","# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n","glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n","glove.add_dictionary(corpus.dictionary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZlngY35O53sk"},"outputs":[],"source":["print(glove.most_similar(\"man\"))"]},{"cell_type":"markdown","metadata":{"id":"3ADfVM9lO9NE"},"source":["🔹 **1-(2)** pre-trained glove \n","\n","* **사전학습모델** : 임의의 값으로 초기화하던 모델의 가중치들을 다른 문제에 학습시킨 가중치들로 초기화하는 방법이다.사전 학습한 가중치를 활용해 학습하고자 하는 본래 문제를 하위문제라고 한다. \n","\n","* [실습 : 문장의 긍부정을 판단하는 감성 분류 모델 만들기](https://wikidocs.net/33793) 👉 필수\n","  * [설명참고](https://omicro03.medium.com/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-16%EC%9D%BC%EC%B0%A8-pre-trained-word-embedding-bb30db424a35)\n","* pre-trained data 를 가져오는데 시간이 오래걸림\n","* kaggle 대회에서 주로 이 방식을 많이 사용함\n","  * [참고](https://lsjsj92.tistory.com/455)"]},{"cell_type":"code","source":[],"metadata":{"id":"CVw2vvTIW54w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_wcrE5PtLMI"},"source":["🔹 **1-(3)** fine tuning glove\n","* 미세조정 : 사전 학습한 모든 가중치와 더불어 하위 문제를 위한 최소한의 가중치를 추가해 모델을 추가로 학습하는 방법이다. \n","\n","* fine tuning 이 필요한 경우 \n","  * pretrained model 에 데이터셋에 있는 단어가 포함되지 않은 경우 \n","  * 데이터 집합이 너무 작아서 전체 모델을 훈련시키기 어려운 경우 \n","\n","* [Mittens 라이브러리로 fine tuning](https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39) 👉 필수\n","  *  GloVe 임베딩을 fine-tuning 하기 위한 파이썬 라이브러리\n","  * [github](https://github.com/roamanalytics/mittens)\n","\n","* [한국어 소설 텍스트 데이터 미세조정 모델 학습 - GPT2](https://m.blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=horajjan&logNo=222104684132&categoryNo=120&proxyReferer=) 👉 선택 (glove 모델 예제는 아닙니다. fine-tuning 에 초점을 두어서 참고해주시면 좋을 것 같습니다.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FHiR5mN4577l"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"I_-OB9Siga3G"},"source":["* (참고) word2vec pretrained example\n","\n","➕ [word2vec 사전학습 모델 -한국어1](http://doc.mindscale.kr/km/unstructured/11.html)\n","\n","➕ [word2vec 사전학습 - 한국어2](https://monetd.github.io/python/nlp/Word-Embedding-Word2Vec-%EC%8B%A4%EC%8A%B5/#%ED%95%9C%EA%B5%AD%EC%96%B4-word2vec-%EB%A7%8C%EB%93%A4%EA%B8%B0)"]},{"cell_type":"markdown","metadata":{"id":"xUWWDwdiPLS9"},"source":["### **2️⃣ NER**"]},{"cell_type":"markdown","metadata":{"id":"9N0B4VknPkTk"},"source":["👀 **내용 복습** \n","* 개체명 인식을 사용하면 코퍼스로부터 어떤 단어가 사람, 장소, 조직 등을 의미하는 단어인지를 찾을 수 있다. "]},{"cell_type":"markdown","metadata":{"id":"QWgla1BuPRqJ"},"source":["\n","\n","\n","🔹 **2-(1)** NER task by nltk library\n","\n","\n","* nltk 에서는 개체명 인식기 (NER chunker) 를 지원하고 있다. \n","* ne_chunk 는 개체명을 태깅하기 위해서 앞서 품사 태깅 pos_tag 가 수행되어야 한다. \n","\n","\n","📌 [basic code](https://wikidocs.net/30682) 👉 필수 \n","\n","📌 [BIO 표현, LSTM을 활용한 NER 실습](https://wikidocs.net/24682) 👉 선택\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"diaZweMyAxJz"},"outputs":[],"source":["from nltk import word_tokenize, pos_tag, ne_chunk\n","\n","sentence = \"James is working at Disney in London\"\n","# 토큰화 후 품사 태깅\n","tokenized_sentence = pos_tag(word_tokenize(sentence))\n","print(tokenized_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1k09tKha3Lgi"},"outputs":[],"source":["# 개체명 인식\n","ner_sentence = ne_chunk(tokenized_sentence)\n","print(ner_sentence)"]},{"cell_type":"markdown","metadata":{"id":"TPX-WtSvPmm6"},"source":["🔹 **2-(2)** NER task by spacy library\n","\n","\n","* spaCy 는 자연어처리를 위한 파이썬 기반의 오픈 소스 라이브러리로 다음과 같은 기능을 제공한다. \n","  * Tokenization \n","  * POS tagging \n","  * Lemmatization \n","  * Sentence Boundary Detection (SBD)\n","  * Named Entity Recognition (NER)\n","  * Similarity\n","  * Text Classification\n","  * Rule-based Matching\n","  * Training\n","  * Serialization\n","\n","* spaCy 와 NER\n","  * .ents → .label_\n","\n","\n","📌 [basic code](https://frhyme.github.io/python-lib/nlp_spacy_1/) 👉 필수 (NER 부분만)\n","\n","📌 [kaggle_Custom NER using SpaCy](https://www.kaggle.com/code/amarsharma768/custom-ner-using-spacy/notebook) 👉 선택\n","\n","  * 훈련되지 않은 데이터 세트에 명명된 엔티티를 학습하는 방법 : 이력서 pdf 데이터 활용 \n","  * manually labelled \n","\n","📌 [한국어 NER](https://github.com/monologg/KoBERT-NER) 👉 참고하면 좋을 자료\n","\n","➕ [참고](http://aispiration.com/nlp2/nlp-ner-python.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WXjRfz-qP0Xx"},"outputs":[],"source":["import spacy \n","nlp = spacy.load('en_core_web_sm')\n","doc = nlp('Apple is looking at buyin at U.K startup for $1 billion.')\n","print(type(doc))\n","print(doc)\n","print(list(doc))\n","print(type(doc[0]))\n","\n","\n","## part of speech tagging \n","\n","temp_str = \"\"\"\n","Text: The original word text.\n","Lemma: The base form of the word.(축약한 상태)\n","POS: The simple part-of-speech tag.\n","Tag: The detailed part-of-speech tag.\n","Dep: Syntactic dependency, i.e. the relation between tokens.\n","Shape: The word shape – capitalisation, punctuation, digits.\n","is alpha: Is the token an alpha character?\n","is stop: Is the token part of a stop list, i.e. the most common words of the language?\n","\"\"\".strip()\n","print(temp_str)\n","print(\"==\"*40)\n","\n","str_format = \"{:>10}\"*8\n","print(str_format.format(*temp_dict.keys()))\n","print(\"==\"*40)\n","\n","doc = nlp('Apple is looking at buying U.K. startup for $1 billion')\n","for token in doc:\n","    print(str_format.format(token.text, token.lemma_, token.pos_, token.tag_, \n","                            token.dep_, token.shape_, str(token.is_alpha), str(token.is_stop)))"]},{"cell_type":"markdown","metadata":{"id":"008-V5QsQG25"},"source":["###**3️⃣ Dependency Parsing**"]},{"cell_type":"markdown","metadata":{"id":"oQfcodHQQPlt"},"source":["👀 **내용 복습** \n","* 문장의 전체적인 구성/구조 보다는 각 개별단어 간의 '의존관계' 또는 '수식관계' 와 같은 단어간 관계를 파악하는 것이 목적인 NLP Task\n","* 문장 해석의 모호성을 없애기 위해 Parsing 을 한다."]},{"cell_type":"markdown","metadata":{"id":"mJLAzZnbRNlL"},"source":["\n","\n","\n","🔹 **3-(1)** Dependency Parsing by spacy library\n","\n","\n","* [basic](https://frhyme.github.io/python-lib/nlp_spacy_1/#navigating-parse-tree) 👉 dependecy parsing 부분만 필수\n","* .dep_ 메서드\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HbQEYt76bJXz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9QAEsrLAxHP"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"XQD5oiGgRfHe"},"source":["🔹 **3-(2)** Spacy (kaggle) \n","\n","* 캐글 노트북 환경에서 실습해보는 것을 권장드립니다!\n","\n","* [kaggle_spaCy](https://www.kaggle.com/code/nirant/hitchhiker-s-guide-to-nlp-in-spacy) 👉 필수\n","  * 도날드 트럼프 트위터 트윗 내용 데이터 분석\n","\n","\n","👀 **노트북 키포인트** \n","  1. spacy.display 메서드를 사용한 NER 시각화 \n","  2. Tagging 을 통한 트럼프 트윗 분석 : noun_chunks 는 dependency graph를 고려하여, noun phrase를 뽑아준다. \n","  3. [spacy Match](https://yujuwon.tistory.com/entry/spaCy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-Rule-based-Matching) : 직접 문장/단어 패턴을 등록하여 parsing\n","  4. Question and answering task using Dependency Parsing\n","    * spacy display :  ``style = 'dep'``\n","    * .dep_\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LuHGKITRbKYq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMArOUrxAJ8K"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1vb1qpw7zMmpoLHTDuCSbWbwRQGXjIA-E","timestamp":1665413924772},{"file_id":"1RyTVvavR5yzGtoim73Sw_LLFKhhbdDd-","timestamp":1664432950955}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}