{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## T5\n",
        "#### Overview\n",
        "The T5 model was presented in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n",
        "\n",
        "by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu."
      ],
      "metadata": {
        "id": "O0hYxYXuES13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "논문의 초록은 다음과 같습니다.\n",
        "\n",
        "모델이 다운스트림 작업에서 미세 조정되기 전에 데이터가 풍부한 작업에서 먼저 사전 훈련되는 전이 학습은 자연어 처리(NLP)에서 강력한 기술로 부상했습니다. \n",
        "\n",
        "전이 학습의 효과로 인해 접근 방식, 방법론 및 실습이 다양해졌습니다. 이 백서에서는 모든 언어 문제를 텍스트-텍스트 형식으로 변환하는 통합 프레임워크를 도입하여 NLP를 위한 전이 학습 기술의 환경을 탐색합니다. \n",
        "\n",
        "우리의 체계적인 연구는 수십 가지 언어 이해 작업에 대한 사전 교육 목표, 아키텍처, 레이블이 지정되지 않은 데이터 세트, 전달 접근 방식 및 기타 요인을 비교합니다. 탐색에서 얻은 통찰력을 규모와 새로운 \"Colossal Clean Crawled Corpus\"와 결합하여 요약, 질문 답변, 텍스트 분류 등. NLP의 전이 학습에 대한 향후 작업을 용이하게 하기 위해 데이터 세트, 사전 훈련된 모델 및 코드를 공개합니다."
      ],
      "metadata": {
        "id": "SOk0XH8IEgM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tips:\n",
        "\n",
        "T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. T5 works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task, e.g., for translation: translate English to German: …, for summarization: summarize: ….\n",
        "\n",
        "T5 uses relative scalar embeddings. Encoder input padding can be done on the left and on the right."
      ],
      "metadata": {
        "id": "tdQQ7WAUyK9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training\n",
        "\n",
        "T5 is an encoder-decoder model and converts all NLP problems into a text-to-text format. It is trained using teacher forcing. This means that for training, we always need an input sequence and a corresponding target sequence.\n",
        "\n",
        "The input sequence is fed to the model using input_ids. The target sequence is shifted to the right, i.e., prepended by a start-sequence token and fed to the decoder using the decoder_input_ids. \n",
        "\n",
        "In teacher-forcing style, the target sequence is then appended by the EOS token and corresponds to the labels. The PAD token is hereby used as the start-sequence token. T5 can be trained / fine-tuned both in a supervised and unsupervised fashion.\n",
        "\n",
        "<Br>\n",
        "\n",
        "\n",
        "- Unsupervised denoising training\n",
        "\n",
        "In this setup, spans of the input sequence are masked by so-called sentinel tokens (a.k.a unique mask tokens) and the output sequence is formed as a concatenation of the same sentinel tokens and the real masked tokens. \n",
        "\n",
        "Each sentinel token represents a unique mask token for this sentence and should start with <extra_id_0>, <extra_id_1>, … up to <extra_id_99>. As a default, 100 sentinel tokens are available in T5Tokenizer.\n",
        "\n",
        "For instance, the sentence “The cute dog walks in the park” with the masks put on “cute dog” and “the” should be processed as follows:"
      ],
      "metadata": {
        "id": "q3VSr2Eqyg2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Supervised training\n",
        "\n",
        "In this setup, the input sequence and output sequence are a standard sequence-to-sequence input-output mapping. Suppose that we want to fine-tune the model for translation for example, and we have a training example: \n",
        "\n",
        "the input sequence “The house is wonderful.” and output sequence “Das Haus ist wunderbar.”, then they should be prepared for the model as follows:"
      ],
      "metadata": {
        "id": "p_C5haL81l0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
        "labels = tokenizer(\"Das Haus ist wunderbar.\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "# the forward function automatically creates the correct decoder_input_ids\n",
        "loss = model(input_ids=input_ids, labels=labels).loss\n",
        "loss.item()\n",
        "0.2542\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "15n4sLi_1ZKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, only 2 inputs are required for the model in order to compute a loss: input_ids (which are the input_ids of the encoded input sequence) and labels (which are the input_ids of the encoded target sequence). The model will automatically create the decoder_input_ids based on the labels, by shifting them one position to the right and prepending the config decoder_start_token_id, which for T5 is equal to 0 (i.e. the id of the pad token). \n",
        "\n",
        "Also note the task prefix: we prepend the input sequence with ‘translate English to German: ’ before encoding it. This will help in improving the performance, as this task prefix was used during T5’s pre-training.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "However, the example above only shows a single training example. In practice, one trains deep learning models in batches. This entails that we must pad/truncate examples to the same length. For encoder-decoder models, one typically defines a max_source_length and max_target_length, which determine the maximum length of the input and output sequences respectively (otherwise they are truncated). These should be carefully set depending on the task.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "In addition, we must make sure that padding token id’s of the labels are not taken into account by the loss function. In PyTorch and Tensorflow, this can be done by replacing them with -100, which is the ignore_index of the CrossEntropyLoss. In Flax, one can use the decoder_attention_mask to ignore padded tokens from the loss (see the Flax summarization script for details). We also pass attention_mask as additional input to the model, which makes sure that padding tokens of the inputs are ignored. The code example below illustrates all of this."
      ],
      "metadata": {
        "id": "740YJ_Vg2wuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# the following 2 hyperparameters are task-specific\n",
        "max_source_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "# Suppose we have the following 2 training examples:\n",
        "input_sequence_1 = \"Welcome to NYC\"\n",
        "output_sequence_1 = \"Bienvenue à NYC\"\n",
        "\n",
        "input_sequence_2 = \"HuggingFace is a company\"\n",
        "output_sequence_2 = \"HuggingFace est une entreprise\"\n",
        "\n",
        "# encode the inputs\n",
        "task_prefix = \"translate English to French: \"\n",
        "input_sequences = [input_sequence_1, input_sequence_2]\n",
        "\n",
        "encoding = tokenizer(\n",
        "    [task_prefix + sequence for sequence in input_sequences],\n",
        "    padding=\"longest\",\n",
        "    max_length=max_source_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
        "\n",
        "# encode the targets\n",
        "target_encoding = tokenizer(\n",
        "    [output_sequence_1, output_sequence_2],\n",
        "    padding=\"longest\",\n",
        "    max_length=max_target_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "labels = target_encoding.input_ids\n",
        "\n",
        "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
        "labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "# forward pass\n",
        "loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
        "loss.item()\n",
        "0.188\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "tk72pbcY3NbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inference\n",
        "At inference time, it is recommended to use generate(). \n",
        "\n",
        "This method takes care of encoding the input and feeding the encoded hidden states via cross-attention layers to the decoder and auto-regressively generates the decoder output. \n",
        "\n",
        "```\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "Das Haus ist wunderbar.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "0RvJ_OqV4Uki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "task_prefix = \"translate English to German: \"\n",
        "# use different length sentences to test batching\n",
        "sentences = [\"The house is wonderful.\", \"I like to work in NYC.\"]\n",
        "\n",
        "inputs = tokenizer([task_prefix + sentence for sentence in sentences], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "output_sequences = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    do_sample=False,  # disable sampling to test if batching affects output\n",
        ")\n",
        "\n",
        "print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))\n",
        "['Das Haus ist wunderbar.', 'Ich arbeite gerne in NYC.']\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "3mZmyLBc4_7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because T5 has been trained with the span-mask denoising objective, it can be used to predict the sentinel (masked-out) tokens during inference. The predicted tokens will then be placed between the sentinel tokens.\n",
        "\n",
        "```\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "sequence_ids = model.generate(input_ids)\n",
        "sequences = tokenizer.batch_decode(sequence_ids)\n",
        "sequences\n",
        "['<pad><extra_id_0> park offers<extra_id_1> the<extra_id_2> park.</s>']\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Wd1cGdk46MkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ZZZvQ7v0RIK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}