{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNF1oLOiBQs9b+TGxjcnCdI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaeyunyeo/2022-2-Euron-Study-Assignment/blob/week14/Week14_%EC%98%88%EC%8A%B5%EA%B3%BC%EC%A0%9C_%EC%97%AC%EC%B1%84%EC%9C%A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6men1C6WsK_i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob ,os\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_colwidth', 700)\n",
        "\n",
        "path = r'C:\\Users\\q\\PerfectGuide\\data\\OpinosisDataset1.0\\OpinosisDataset1.0\\topics'\n",
        "\n",
        "all_files = glob.glob(os.path.join(path, \"*.data\"))    \n",
        "filename_list = []\n",
        "opinion_text = []\n",
        "for file_ in all_files:\n",
        "    df = pd.read_table(file_,index_col=None, header=0,encoding='latin1')\n",
        "\n",
        "    filename_ = file_.split('\\\\')[-1]\n",
        "    filename = filename_.split('.')[0]\n",
        "\n",
        "    filename_list.append(filename)\n",
        "    opinion_text.append(df.to_string())\n",
        "\n",
        "document_df = pd.DataFrame({'filename':filename_list, 'opinion_text':opinion_text})\n",
        "document_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "lemmar = WordNetLemmatizer()\n",
        "\n",
        "# 입력으로 들어온 token단어들에 대해서 lemmatization 어근 변환. \n",
        "def LemTokens(tokens):\n",
        "    return [lemmar.lemmatize(token) for token in tokens]\n",
        "\n",
        "# TfidfVectorizer 객체 생성 시 tokenizer인자로 해당 함수를 설정하여 lemmatization 적용\n",
        "# 입력으로 문장을 받아서 stop words 제거-> 소문자 변환 -> 단어 토큰화 -> lemmatization 어근 변환. \n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "7Pw6Fu21tVPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english' , \\\n",
        "                             ngram_range=(1,2), min_df=0.05, max_df=0.85 )\n",
        "\n",
        "#opinion_text 컬럼값으로 feature vectorization 수행\n",
        "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 5개 집합으로 군집화 수행. 예제를 위해 동일한 클러스터링 결과 도출용 random_state=0 \n",
        "km_cluster = KMeans(n_clusters=5, max_iter=10000, random_state=0)\n",
        "km_cluster.fit(feature_vect)\n",
        "cluster_label = km_cluster.labels_\n",
        "cluster_centers = km_cluster.cluster_centers_\n",
        "km_cluster.labels_"
      ],
      "metadata": {
        "id": "7OlM2DdTtVND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_df['cluster_label'] = cluster_label\n",
        "document_df.head()"
      ],
      "metadata": {
        "id": "M0fGFYo4tVJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_df[document_df['cluster_label']==0].sort_values(by='filename')"
      ],
      "metadata": {
        "id": "OX1BcuqstVHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_df[document_df['cluster_label']==1].sort_values(by='filename')"
      ],
      "metadata": {
        "id": "pPHV6OqhtVEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_df[document_df['cluster_label']==2].sort_values(by='filename')"
      ],
      "metadata": {
        "id": "CoIg-VjLtVBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_df[document_df['cluster_label']==3].sort_values(by='filename')"
      ],
      "metadata": {
        "id": "8RopyVpjtU_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_df[document_df['cluster_label']==4].sort_values(by='filename')"
      ],
      "metadata": {
        "id": "wi2yoMSitU8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 3개의 집합으로 군집화 \n",
        "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
        "km_cluster.fit(feature_vect)\n",
        "cluster_label = km_cluster.labels_\n",
        "\n",
        "\n",
        "# 소속 클러스터를 cluster_label 컬럼으로 할당하고 cluster_label 값으로 정렬\n",
        "document_df['cluster_label'] = cluster_label\n",
        "document_df.sort_values(by='cluster_label')"
      ],
      "metadata": {
        "id": "iXAbpG29tU56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 군집 별 핵심 단어 추출하기"
      ],
      "metadata": {
        "id": "Kpk3mOjKtjGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_vect.shape"
      ],
      "metadata": {
        "id": "ecdO5aJstU3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_centers = km_cluster.cluster_centers_\n",
        "print('cluster_centers shape :',cluster_centers.shape)\n",
        "print(cluster_centers)"
      ],
      "metadata": {
        "id": "qlwArY7_tU06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cluster_details(cluster_model, cluster_data, feature_names, clusters_num, top_n_features=10):\n",
        "    cluster_details = {}\n",
        "    \n",
        "    # cluster_centers array 의 값이 큰 순으로 정렬된 index 값을 반환\n",
        "    # 군집 중심점(centroid)별 할당된 word 피처들의 거리값이 큰 순으로 값을 구하기 위함.  \n",
        "    centroid_feature_ordered_ind = cluster_model.cluster_centers_.argsort()[:,::-1]\n",
        "    \n",
        "    #개별 군집별로 iteration하면서 핵심단어, 그 단어의 중심 위치 상대값, 대상 파일명 입력\n",
        "    for cluster_num in range(clusters_num):\n",
        "        # 개별 군집별 정보를 담을 데이터 초기화. \n",
        "        cluster_details[cluster_num] = {}\n",
        "        cluster_details[cluster_num]['cluster'] = cluster_num\n",
        "        \n",
        "        # cluster_centers_.argsort()[:,::-1] 로 구한 index 를 이용하여 top n 피처 단어를 구함. \n",
        "        top_feature_indexes = centroid_feature_ordered_ind[cluster_num, :top_n_features]\n",
        "        top_features = [ feature_names[ind] for ind in top_feature_indexes ]\n",
        "        \n",
        "        # top_feature_indexes를 이용해 해당 피처 단어의 중심 위치 상댓값 구함 \n",
        "        top_feature_values = cluster_model.cluster_centers_[cluster_num, top_feature_indexes].tolist()\n",
        "        \n",
        "        # cluster_details 딕셔너리 객체에 개별 군집별 핵심 단어와 중심위치 상대값, 그리고 해당 파일명 입력\n",
        "        cluster_details[cluster_num]['top_features'] = top_features\n",
        "        cluster_details[cluster_num]['top_features_value'] = top_feature_values\n",
        "        filenames = cluster_data[cluster_data['cluster_label'] == cluster_num]['filename']\n",
        "        filenames = filenames.values.tolist()\n",
        "        cluster_details[cluster_num]['filenames'] = filenames\n",
        "        \n",
        "    return cluster_details"
      ],
      "metadata": {
        "id": "PrH44NxatUya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_cluster_details(cluster_details):\n",
        "    for cluster_num, cluster_detail in cluster_details.items():\n",
        "        print('####### Cluster {0}'.format(cluster_num))\n",
        "        print('Top features:', cluster_detail['top_features'])\n",
        "        print('Reviews 파일명 :',cluster_detail['filenames'][:7])\n",
        "        print('==================================================')"
      ],
      "metadata": {
        "id": "uBO5I0DhtUv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = tfidf_vect.get_feature_names()\n",
        "\n",
        "cluster_details = get_cluster_details(cluster_model=km_cluster, cluster_data=document_df,\\\n",
        "                                  feature_names=feature_names, clusters_num=3, top_n_features=10 )\n",
        "print_cluster_details(cluster_details)"
      ],
      "metadata": {
        "id": "oLhaTH7GtUtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문서 유사도 측정 방법 - 코사인 유사도"
      ],
      "metadata": {
        "id": "IaOgjqsQtrfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "metadata": {
        "id": "FeG7V2r1tUqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cos_similarity(v1, v2):\n",
        "    dot_product = np.dot(v1, v2)\n",
        "    l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))\n",
        "    similarity = dot_product / l2_norm     \n",
        "    \n",
        "    return similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "doc_list = ['if you take the blue pill, the story ends' ,\n",
        "            'if you take the red pill, you stay in Wonderland',\n",
        "            'if you take the red pill, I show you how deep the rabbit hole goes']\n",
        "\n",
        "tfidf_vect_simple = TfidfVectorizer()\n",
        "feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list)\n",
        "print(feature_vect_simple.shape)"
      ],
      "metadata": {
        "id": "F-5gGrQMtUn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_vect_dense = feature_vect_simple.todense()\n",
        "feature_vect_dense"
      ],
      "metadata": {
        "id": "o3mNl-zjtUi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(feature_vect_dense[0]).reshape(-1,)"
      ],
      "metadata": {
        "id": "ePG-A078trBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFidfVectorizer로 transform()한 결과는 Sparse Matrix이므로 Dense Matrix로 변환. \n",
        "feature_vect_dense = feature_vect_simple.todense()\n",
        "\n",
        "#첫번째 문장과 두번째 문장의 feature vector  추출\n",
        "vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
        "vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
        "\n",
        "#첫번째 문장과 두번째 문장의 feature vector로 두개 문장의 Cosine 유사도 추출\n",
        "similarity_simple = cos_similarity(vect1, vect2 )\n",
        "print('문장 1, 문장 2 Cosine 유사도: {0:.3f}'.format(similarity_simple))"
      ],
      "metadata": {
        "id": "dqrLdaoTtq-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
        "vect3 = np.array(feature_vect_dense[2]).reshape(-1,)\n",
        "similarity_simple = cos_similarity(vect1, vect3 )\n",
        "print('문장 1, 문장 3 Cosine 유사도: {0:.3f}'.format(similarity_simple))\n",
        "\n",
        "vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
        "vect3 = np.array(feature_vect_dense[2]).reshape(-1,)\n",
        "similarity_simple = cos_similarity(vect2, vect3 )\n",
        "print('문장 2, 문장 3 Cosine 유사도: {0:.3f}'.format(similarity_simple))"
      ],
      "metadata": {
        "id": "QJsz6E5htq8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_simple_pair = cosine_similarity(feature_vect_simple[0] , feature_vect_simple)\n",
        "print(similarity_simple_pair)"
      ],
      "metadata": {
        "id": "p5HAX425tq5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_simple_pair = cosine_similarity(feature_vect_simple[0] , feature_vect_simple[1:])\n",
        "print(similarity_simple_pair)"
      ],
      "metadata": {
        "id": "j-EvGuB4tq3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_simple_pair = cosine_similarity(feature_vect_simple , feature_vect_simple)\n",
        "print(similarity_simple_pair)\n",
        "print('shape:',similarity_simple_pair.shape)"
      ],
      "metadata": {
        "id": "IXE18-kKtq0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "lemmar = WordNetLemmatizer()\n",
        "\n",
        "# 입력으로 들어온 token단어들에 대해서 lemmatization 어근 변환. \n",
        "def LemTokens(tokens):\n",
        "    return [lemmar.lemmatize(token) for token in tokens]\n",
        "\n",
        "# TfidfVectorizer 객체 생성 시 tokenizer인자로 해당 함수를 설정하여 lemmatization 적용\n",
        "# 입력으로 문장을 받아서 stop words 제거-> 소문자 변환 -> 단어 토큰화 -> lemmatization 어근 변환. \n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "ScALJzwQtqyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob ,os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#path = r'C:\\Users\\q\\Text\\OpinosisDataset1.0\\OpinosisDataset1.0\\topics'\n",
        "path = r'C:\\Users\\q\\PerfectGuide\\data\\OpinosisDataset1.0\\OpinosisDataset1.0\\topics'\n",
        "all_files = glob.glob(os.path.join(path, \"*.data\"))     \n",
        "filename_list = []\n",
        "opinion_text = []\n",
        "\n",
        "for file_ in all_files:\n",
        "    df = pd.read_table(file_,index_col=None, header=0,encoding='latin1')\n",
        "    filename_ = file_.split('\\\\')[-1]\n",
        "    filename = filename_.split('.')[0]\n",
        "    filename_list.append(filename)\n",
        "    opinion_text.append(df.to_string())\n",
        "\n",
        "document_df = pd.DataFrame({'filename':filename_list, 'opinion_text':opinion_text})\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english' , \\\n",
        "                             ngram_range=(1,2), min_df=0.05, max_df=0.85 )\n",
        "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])\n",
        "\n",
        "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
        "km_cluster.fit(feature_vect)\n",
        "cluster_label = km_cluster.labels_\n",
        "cluster_centers = km_cluster.cluster_centers_\n",
        "document_df['cluster_label'] = cluster_label"
      ],
      "metadata": {
        "id": "7gGu93iAtqwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_df.head(51)\n"
      ],
      "metadata": {
        "id": "pZGo0CWftqt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hotel_indexes = document_df[document_df['cluster_label']==2].index\n",
        "hotel_indexes"
      ],
      "metadata": {
        "id": "SzVGcAfTuZeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# cluster_label=2인 데이터는 호텔로 클러스터링된 데이터임. DataFrame에서 해당 Index를 추출\n",
        "hotel_indexes = document_df[document_df['cluster_label']==2].index\n",
        "print('호텔로 클러스터링 된 문서들의 DataFrame Index:', hotel_indexes)\n",
        "\n",
        "# 호텔로 클러스터링된 데이터 중 첫번째 문서를 추출하여 파일명 표시.  \n",
        "comparison_docname = document_df.iloc[hotel_indexes[0]]['filename']\n",
        "print('##### 비교 기준 문서명 ',comparison_docname,' 와 타 문서 유사도######')\n",
        "\n",
        "''' document_df에서 추출한 Index 객체를 feature_vect로 입력하여 호텔 클러스터링된 feature_vect 추출 \n",
        "이를 이용하여 호텔로 클러스터링된 문서 중 첫번째 문서와 다른 문서간의 코사인 유사도 측정.'''\n",
        "similarity_pair = cosine_similarity(feature_vect[hotel_indexes[0]] , feature_vect[hotel_indexes])\n",
        "print(similarity_pair)"
      ],
      "metadata": {
        "id": "bq6IDvI3uZbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# argsort()를 이용하여 앞예제의 첫번째 문서와 타 문서간 유사도가 큰 순으로 정렬한 인덱스 추출하되 자기 자신은 제외. \n",
        "sorted_index = similarity_pair.argsort()[:,::-1]\n",
        "sorted_index = sorted_index[:, 1:]\n",
        "\n",
        "# 유사도가 큰 순으로 hotel_indexes를 추출하여 재 정렬\n",
        "hotel_sorted_indexes = hotel_indexes[sorted_index.reshape(-1)]\n",
        "\n",
        "# 유사도가 큰 순으로 유사도 값을 재정렬하되 자기 자신은 제외\n",
        "hotel_1_sim_value = np.sort(similarity_pair.reshape(-1))[::-1]\n",
        "hotel_1_sim_value = hotel_1_sim_value[1:]\n",
        "\n",
        "# 유사도가 큰 순으로 정렬된 인덱스와 유사도 값을 이용해 파일명과 유사도값을 막대 그래프로 시각화\n",
        "hotel_1_sim_df = pd.DataFrame()\n",
        "hotel_1_sim_df['filename'] = document_df.iloc[hotel_sorted_indexes]['filename']\n",
        "hotel_1_sim_df['similarity'] = hotel_1_sim_value\n",
        "print('가장 유사도가 큰 파일명 및 유사도:\\n', hotel_1_sim_df.iloc[0, :])\n",
        "\n",
        "sns.barplot(x='similarity', y='filename',data=hotel_1_sim_df)\n",
        "plt.title(comparison_docname)"
      ],
      "metadata": {
        "id": "O6rHdymEuZYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.9 한글 텍스트 처리"
      ],
      "metadata": {
        "id": "uageuyF5uhJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Twitter\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 컬럼 분리 문자 \\t \n",
        "train_df = pd.read_csv('ratings_train.txt', sep='\\t') # 한글 encoding시 encoding='cp949' 적용.\n",
        "train_df.head(10)"
      ],
      "metadata": {
        "id": "crANOsHduZWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['label'].value_counts( )\n"
      ],
      "metadata": {
        "id": "LSSbUbZtuZTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()\n"
      ],
      "metadata": {
        "id": "OWbLjKb8uZLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "train_df = train_df.fillna(' ')\n",
        "# 정규 표현식을 이용하여 숫자를 공백으로 변경(정규 표현식으로 \\d 는 숫자를 의미함.) \n",
        "train_df['document'] = train_df['document'].apply( lambda x : re.sub(r\"\\d+\", \" \", x) )\n",
        "\n",
        "# 테스트 데이터 셋을 로딩하고 동일하게 Null 및 숫자를 공백으로 변환. \n",
        "test_df = pd.read_csv('ratings_test.txt', sep='\\t') # 한글 encoding시 encoding='cp949' 적용.\n",
        "test_df = test_df.fillna(' ')\n",
        "test_df['document'] = test_df['document'].apply( lambda x : re.sub(r\"\\d+\", \" \", x) )\n",
        "\n",
        "# id 컬럼 삭제 수행. \n",
        "train_df.drop('id', axis=1, inplace=True) \n",
        "test_df.drop('id', axis=1, inplace=True)\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "def tw_tokenizer(text):\n",
        "    # 입력 인자로 들어온 text 를 형태소 단어로 토큰화 하여 list 객체 반환\n",
        "    tokens_ko = okt.morphs(text)\n",
        "    return tokens_ko\n",
        "\n",
        "#tw_tokenizer('아버지가방에 들어가신다')\n",
        "okt.morphs('아버지가방에 들어가신다')"
      ],
      "metadata": {
        "id": "uciHtyBfuZJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Okt 객체의 morphs( ) 객체를 이용한 tokenizer를 사용. ngram_range는 (1,2) \n",
        "tfidf_vect = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.9)\n",
        "tfidf_vect.fit(train_df['document'])\n",
        "tfidf_matrix_train = tfidf_vect.transform(train_df['document'])\n",
        "print(tfidf_matrix_train.shape)"
      ],
      "metadata": {
        "id": "udyUNX6DuZFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression 을 이용하여 감성 분석 Classification 수행. \n",
        "lg_clf = LogisticRegression(random_state=0, solver='liblinear')\n",
        "\n",
        "# Parameter C 최적화를 위해 GridSearchCV 를 이용. \n",
        "params = { 'C': [1 ,3.5, 4.5, 5.5, 10 ] }\n",
        "grid_cv = GridSearchCV(lg_clf , param_grid=params , cv=3 ,scoring='accuracy', verbose=1 )\n",
        "grid_cv.fit(tfidf_matrix_train , train_df['label'] )\n",
        "print(grid_cv.best_params_ , round(grid_cv.best_score_,4))"
      ],
      "metadata": {
        "id": "PkZwPKzFuZAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()\n"
      ],
      "metadata": {
        "id": "R_kLl6clxHLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 학습 데이터를 적용한 TfidfVectorizer를 이용하여 테스트 데이터를 TF-IDF 값으로 Feature 변환함. \n",
        "tfidf_matrix_test = tfidf_vect.transform(test_df['document'])\n",
        "\n",
        "# classifier 는 GridSearchCV에서 최적 파라미터로 학습된 classifier를 그대로 이용\n",
        "best_estimator = grid_cv.best_estimator_\n",
        "preds = best_estimator.predict(tfidf_matrix_test)\n",
        "\n",
        "print('Logistic Regression 정확도: ',accuracy_score(test_df['label'],preds))"
      ],
      "metadata": {
        "id": "BPamnqQ9xHJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.10 Text analysis 실습"
      ],
      "metadata": {
        "id": "N_uhgWZvxNIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge , LogisticRegression\n",
        "from sklearn.model_selection import train_test_split , cross_val_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "mercari_df= pd.read_csv('mercari_train.tsv',sep='\\t')\n",
        "print(mercari_df.shape)\n",
        "mercari_df.head(3)"
      ],
      "metadata": {
        "id": "WlIP_FPoxHG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mercari_df.info())\n"
      ],
      "metadata": {
        "id": "b5y6z5ehxHEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "y_train_df = mercari_df['price']\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(y_train_df, bins=100)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ab2AvXcoxHCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "y_train_df = np.log1p(y_train_df)\n",
        "sns.histplot(y_train_df, bins=50)\n",
        "plt.show()\n",
        "mercari_df['price'] = np.log1p(mercari_df['price'])\n",
        "mercari_df['price'].head(3)"
      ],
      "metadata": {
        "id": "SHcOXAXJxG_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shipping 값 유형:\\n',mercari_df['shipping'].value_counts())\n",
        "print('item_condition_id 값 유형:\\n',mercari_df['item_condition_id'].value_counts())\n",
        "boolean_cond= mercari_df['item_description']=='No description yet'\n",
        "mercari_df[boolean_cond]['item_description'].count()"
      ],
      "metadata": {
        "id": "Wi2MjkBYxG9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'Men/Tops/T-shirts'.split('/')\n",
        "# apply lambda에서 호출되는 대,중,소 분할 함수 생성, 대,중,소 값을 리스트 반환\n",
        "def split_cat(category_name):\n",
        "    try:\n",
        "        return category_name.split('/')\n",
        "    except:\n",
        "        return ['Other_Null' , 'Other_Null' , 'Other_Null']\n",
        "\n",
        "# 위의 split_cat( )을 apply lambda에서 호출하여 대,중,소 컬럼을 mercari_df에 생성. \n",
        "mercari_df['cat_dae'], mercari_df['cat_jung'], mercari_df['cat_so'] = \\\n",
        "                        zip(*mercari_df['category_name'].apply(lambda x : split_cat(x)))\n",
        "\n",
        "# 대분류만 값의 유형과 건수를 살펴보고, 중분류, 소분류는 값의 유형이 많으므로 분류 갯수만 추출\n",
        "print('대분류 유형 :\\n', mercari_df['cat_dae'].value_counts())\n",
        "print('중분류 갯수 :', mercari_df['cat_jung'].nunique())\n",
        "print('소분류 갯수 :', mercari_df['cat_so'].nunique())\n",
        "'Men/Tops/T-shirts'.split('/')\n",
        "# apply lambda에서 호출되는 대,중,소 분할 함수 생성, 대,중,소 값을 리스트 반환\n",
        "def split_cat(category_name):\n",
        "    try:\n",
        "        return category_name.split('/')\n",
        "    except:\n",
        "        return ['Other_Null' , 'Other_Null' , 'Other_Null']\n",
        "\n",
        "# 위의 split_cat( )을 apply lambda에서 호출하여 대,중,소 컬럼을 mercari_df에 생성. \n",
        "mercari_df['category_list'] = mercari_df['category_name'].apply(lambda x : split_cat(x))\n",
        "mercari_df['category_list'].head()\n",
        "mercari_df['cat_dae'] = mercari_df['category_list'].apply(lambda x:x[0])\n",
        "mercari_df['cat_jung'] = mercari_df['category_list'].apply(lambda x:x[1])\n",
        "mercari_df['cat_so'] = mercari_df['category_list'].apply(lambda x:x[2])\n",
        "\n",
        "mercari_df.drop('category_list', axis=1, inplace=True) \n",
        "mercari_df[['cat_dae','cat_jung','cat_so']].head()"
      ],
      "metadata": {
        "id": "w0aFqvIoxG7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mercari_df['brand_name'] = mercari_df['brand_name'].fillna(value='Other_Null')\n",
        "mercari_df['category_name'] = mercari_df['category_name'].fillna(value='Other_Null')\n",
        "mercari_df['item_description'] = mercari_df['item_description'].fillna(value='Other_Null')\n",
        "\n",
        "# 각 컬럼별로 Null값 건수 확인. 모두 0가 나와야 합니다.\n",
        "mercari_df.isnull().sum()\n",
        "mercari_df.info()"
      ],
      "metadata": {
        "id": "snF4w7n8xG44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('brand name 의 유형 건수 :', mercari_df['brand_name'].nunique())\n",
        "print('brand name sample 5건 : \\n', mercari_df['brand_name'].value_counts()[:5])\n",
        "print('name 의 종류 갯수 :', mercari_df['name'].nunique())\n",
        "print('name sample 7건 : \\n', mercari_df['name'][:7])"
      ],
      "metadata": {
        "id": "3v3QcFDZxG2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mercari_df['item_description'].str.len().mean()\n",
        "pd.set_option('max_colwidth', 200)\n",
        "\n",
        "# item_description의 평균 문자열 개수\n",
        "print('item_description 평균 문자열 개수:',mercari_df['item_description'].str.len().mean())\n",
        "\n",
        "mercari_df['item_description'][:2]\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "5CG0E4inxbKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# name 속성에 대한 feature vectorization 변환\n",
        "cnt_vec = CountVectorizer()\n",
        "X_name = cnt_vec.fit_transform(mercari_df['name'])\n",
        "\n",
        "# item_description 에 대한 feature vectorization 변환 \n",
        "tfidf_descp = TfidfVectorizer(max_features = 50000, ngram_range= (1,3) , stop_words='english')\n",
        "X_descp = tfidf_descp.fit_transform(mercari_df['item_description'])\n",
        "\n",
        "print('name vectorization shape:',X_name.shape)\n",
        "print('item_description vectorization shape:',X_descp.shape)"
      ],
      "metadata": {
        "id": "cWjDj2d6xbHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "ohe_result = lb.fit_transform([1, 2, 6, 4, 2])\n",
        "print(type(ohe_result))\n",
        "print(ohe_result)\n",
        "\n",
        "lb_sparse = preprocessing.LabelBinarizer(sparse_output=True)\n",
        "ohe_result_sparse = lb_sparse.fit_transform([1, 2, 6, 4, 2])\n",
        "print(type(ohe_result_sparse))\n",
        "print(ohe_result_sparse)"
      ],
      "metadata": {
        "id": "u3Hc9xSqxbCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# brand_name, item_condition_id, shipping 각 피처들을 희소 행렬 원-핫 인코딩 변환\n",
        "lb_brand_name= LabelBinarizer(sparse_output=True)\n",
        "X_brand = lb_brand_name.fit_transform(mercari_df['brand_name'])\n",
        "\n",
        "lb_item_cond_id = LabelBinarizer(sparse_output=True)\n",
        "X_item_cond_id = lb_item_cond_id.fit_transform(mercari_df['item_condition_id'])\n",
        "\n",
        "lb_shipping= LabelBinarizer(sparse_output=True)\n",
        "X_shipping = lb_shipping.fit_transform(mercari_df['shipping'])\n",
        "\n",
        "# cat_dae, cat_jung, cat_so 각 피처들을 희소 행렬 원-핫 인코딩 변환\n",
        "lb_cat_dae = LabelBinarizer(sparse_output=True)\n",
        "X_cat_dae= lb_cat_dae.fit_transform(mercari_df['cat_dae'])\n",
        "\n",
        "lb_cat_jung = LabelBinarizer(sparse_output=True)\n",
        "X_cat_jung = lb_cat_jung.fit_transform(mercari_df['cat_jung'])\n",
        "\n",
        "lb_cat_so = LabelBinarizer(sparse_output=True)\n",
        "X_cat_so = lb_cat_so.fit_transform(mercari_df['cat_so'])"
      ],
      "metadata": {
        "id": "w_QuFtRgxbAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_brand), type(X_item_cond_id), type(X_shipping))\n",
        "print('X_brand_shape:{0}, X_item_cond_id shape:{1}'.format(X_brand.shape, X_item_cond_id.shape))\n",
        "print('X_shipping shape:{0}, X_cat_dae shape:{1}'.format(X_shipping.shape, X_cat_dae.shape))\n",
        "print('X_cat_jung shape:{0}, X_cat_so shape:{1}'.format(X_cat_jung.shape, X_cat_so.shape))\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "Q8JG8TG2xa97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# 원-핫 인코딩을 적용합니다. \n",
        "oh_encoder = OneHotEncoder()\n",
        "\n",
        "# brand_name, item_condition_id, shipping 각 피처들을 희소 행렬 원-핫 인코딩 변환\n",
        "X_brand = oh_encoder.fit_transform(mercari_df['brand_name'].values.reshape(-1, 1))\n",
        "X_item_cond_id = oh_encoder.fit_transform(mercari_df['item_condition_id'].values.reshape(-1, 1))\n",
        "X_shipping = oh_encoder.fit_transform(mercari_df['shipping'].values.reshape(-1, 1))\n",
        "X_cat_dae= oh_encoder.fit_transform(mercari_df['cat_dae'].values.reshape(-1, 1))\n",
        "X_cat_jung = oh_encoder.fit_transform(mercari_df['cat_jung'].values.reshape(-1, 1))\n",
        "X_cat_so = oh_encoder.fit_transform(mercari_df['cat_so'].values.reshape(-1, 1))\n",
        "print(type(X_brand), type(X_item_cond_id), type(X_shipping))\n",
        "print('X_brand_shape:{0}, X_item_cond_id shape:{1}'.format(X_brand.shape, X_item_cond_id.shape))\n",
        "print('X_shipping shape:{0}, X_cat_dae shape:{1}'.format(X_shipping.shape, X_cat_dae.shape))\n",
        "print('X_cat_jung shape:{0}, X_cat_so shape:{1}'.format(X_cat_jung.shape, X_cat_so.shape))"
      ],
      "metadata": {
        "id": "7GsWRgaJxa7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from  scipy.sparse import hstack\n",
        "import gc\n",
        "\n",
        "sparse_matrix_list = (X_name, X_descp, X_brand, X_item_cond_id,\n",
        "            X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
        "\n",
        "# 사이파이 sparse 모듈의 hstack 함수를 이용하여 앞에서 인코딩과 Vectorization을 수행한 데이터 셋을 모두 결합. \n",
        "X_features_sparse= hstack(sparse_matrix_list).tocsr()\n",
        "print(type(X_features_sparse), X_features_sparse.shape)\n",
        "\n",
        "# 데이터 셋이 메모리를 많이 차지하므로 사용 용도가 끝났으면 바로 메모리에서 삭제. \n",
        "del X_features_sparse\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "A1M7WxIrxa5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsle(y , y_pred):\n",
        "    # underflow, overflow를 막기 위해 log가 아닌 log1p로 rmsle 계산 \n",
        "    return np.sqrt(np.mean(np.power(np.log1p(y) - np.log1p(y_pred), 2)))\n",
        "\n",
        "def evaluate_org_price(y_test , preds): \n",
        "    \n",
        "    # 원본 데이터는 log1p로 변환되었으므로 expm1으로 원복 필요. \n",
        "    preds_exmpm = np.expm1(preds)\n",
        "    y_test_exmpm = np.expm1(y_test)\n",
        "    \n",
        "    # rmsle로 RMSLE 값 추출\n",
        "    rmsle_result = rmsle(y_test_exmpm, preds_exmpm)\n",
        "    return rmsle_result"
      ],
      "metadata": {
        "id": "q5Q_fLNHxa3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc \n",
        "from  scipy.sparse import hstack\n",
        "\n",
        "def model_train_predict(model,matrix_list):\n",
        "    # scipy.sparse 모듈의 hstack 을 이용하여 sparse matrix 결합\n",
        "    X= hstack(matrix_list).tocsr()     \n",
        "    \n",
        "    X_train, X_test, y_train, y_test=train_test_split(X, mercari_df['price'], \n",
        "                                                      test_size=0.2, random_state=156)\n",
        "    \n",
        "    # 모델 학습 및 예측\n",
        "    model.fit(X_train , y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    \n",
        "    del X , X_train , X_test , y_train \n",
        "    gc.collect()\n",
        "    \n",
        "    return preds , y_test"
      ],
      "metadata": {
        "id": "WIXzs9Pzxpck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model = Ridge(solver = \"lsqr\", fit_intercept=False)\n",
        "\n",
        "sparse_matrix_list = (X_name, X_brand, X_item_cond_id,\n",
        "                      X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
        "linear_preds , y_test = model_train_predict(model=linear_model ,matrix_list=sparse_matrix_list)\n",
        "print('Item Description을 제외했을 때 rmsle 값:', evaluate_org_price(y_test , linear_preds))\n",
        "\n",
        "sparse_matrix_list = (X_descp, X_name, X_brand, X_item_cond_id,\n",
        "                      X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
        "linear_preds , y_test = model_train_predict(model=linear_model , matrix_list=sparse_matrix_list)\n",
        "print('Item Description을 포함한 rmsle 값:',  evaluate_org_price(y_test ,linear_preds))\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "7sJiWmJexpaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "sparse_matrix_list = (X_descp, X_name, X_brand, X_item_cond_id,\n",
        "                      X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
        "\n",
        "lgbm_model = LGBMRegressor(n_estimators=200, learning_rate=0.5, num_leaves=125, random_state=156)\n",
        "lgbm_preds , y_test = model_train_predict(model = lgbm_model , matrix_list=sparse_matrix_list)\n",
        "print('LightGBM rmsle 값:',  evaluate_org_price(y_test , lgbm_preds))\n",
        "preds = lgbm_preds * 0.45 + linear_preds * 0.55\n",
        "print('LightGBM과 Ridge를 ensemble한 최종 rmsle 값:',  evaluate_org_price(y_test , preds))"
      ],
      "metadata": {
        "id": "zBBa-zDuxpX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aELh5JpIxpVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x8HJvSQQxpTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uMNnQe3XxpQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D5OeLmWrxpNv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}