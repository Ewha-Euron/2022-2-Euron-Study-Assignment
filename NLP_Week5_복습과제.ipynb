{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **2 Coding: Implementing word2vec (18 points)**\n",
        "In this part you will implement the word2vec model and train your own word vectors with stochastic gradient descent (SGD). Before you begin, first run the following commands within the assignment directory in order to create the appropriate conda virtual environment. This guarantees that you have all the necessary packages to complete the assignment. Also note that you probably want to finish the previous math section before writing the code since you will be asked to implement the math functions in Python. You want to implement and test the following subsections in order since they are accumulative.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "conda env create -f env.yml\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "conda activate a2\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Once you are done with the assignment you can deactivate this environment by running:"
      ],
      "metadata": {
        "id": "mIMvghVZPz7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/유런/a2/a2\")"
      ],
      "metadata": {
        "id": "18VDLf0OvoRc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ipynb-py-convert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAsR1nfvtFxx",
        "outputId": "ed4771a0-e978-4240-dac2-7cfe3f5cbbae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipynb-py-convert\n",
            "  Downloading ipynb-py-convert-0.4.6.tar.gz (3.9 kB)\n",
            "Building wheels for collected packages: ipynb-py-convert\n",
            "  Building wheel for ipynb-py-convert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipynb-py-convert: filename=ipynb_py_convert-0.4.6-py3-none-any.whl size=4640 sha256=d153b51c6666cf9e351b40787515556c44a1447018a47144047969fe47326307\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/31/a9/761b134adbbca3c92d491eff3bad785c0a0c0079695d6f0504\n",
            "Successfully built ipynb-py-convert\n",
            "Installing collected packages: ipynb-py-convert\n",
            "Successfully installed ipynb-py-convert-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ipynb-py-convert sgd.py sgd.ipynb"
      ],
      "metadata": {
        "id": "PIPEQxWNtPxi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ipynb-py-convert run.py run.ipynb"
      ],
      "metadata": {
        "id": "rrzPwdDu6OH7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lkzg9ug6iyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed2b8a04-9d57-4897-ed00-cdef9b6308e4"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "# Save parameters every a few SGD iterations as fail-safe\n",
        "SAVE_PARAMS_EVERY = 5000\n",
        "\n",
        "import pickle\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import os.path as op\n",
        "\n",
        "def load_saved_params():\n",
        "    \"\"\"\n",
        "    A helper function that loads previously saved parameters and resets\n",
        "    iteration start.\n",
        "    \"\"\"\n",
        "    st = 0\n",
        "    for f in glob.glob(\"saved_params_*.npy\"):\n",
        "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
        "        if (iter > st):\n",
        "            st = iter\n",
        "\n",
        "    if st > 0:\n",
        "        params_file = \"saved_params_%d.npy\" % st\n",
        "        state_file = \"saved_state_%d.pickle\" % st\n",
        "        params = np.load(params_file)\n",
        "        with open(state_file, \"rb\") as f:\n",
        "            state = pickle.load(f)\n",
        "        return st, params, state\n",
        "    else:\n",
        "        return st, None, None\n",
        "\n",
        "\n",
        "def save_params(iter, params):\n",
        "    params_file = \"saved_params_%d.npy\" % iter\n",
        "    np.save(params_file, params)\n",
        "    with open(\"saved_state_%d.pickle\" % iter, \"wb\") as f:\n",
        "        pickle.dump(random.getstate(), f)\n",
        "\n",
        "\n",
        "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n",
        "        PRINT_EVERY=10):\n",
        "    \"\"\" Stochastic Gradient Descent\n",
        "\n",
        "    Implement the stochastic gradient descent method in this function.\n",
        "\n",
        "    Arguments:\n",
        "    f -- the function to optimize, it should take a single\n",
        "         argument and yield two outputs, a loss and the gradient\n",
        "         with respect to the arguments\n",
        "    x0 -- the initial point to start SGD from\n",
        "    step -- the step size for SGD\n",
        "    iterations -- total iterations to run SGD for\n",
        "    postprocessing -- postprocessing function for the parameters\n",
        "                      if necessary. In the case of word2vec we will need to\n",
        "                      normalize the word vectors to have unit length.\n",
        "    PRINT_EVERY -- specifies how many iterations to output loss\n",
        "\n",
        "    Return:\n",
        "    x -- the parameter value after SGD finishes\n",
        "    \"\"\"\n",
        "\n",
        "    # Anneal learning rate every several iterations\n",
        "    ANNEAL_EVERY = 20000\n",
        "\n",
        "    if useSaved:\n",
        "        start_iter, oldx, state = load_saved_params()\n",
        "        if start_iter > 0:\n",
        "            x0 = oldx\n",
        "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
        "\n",
        "        if state:\n",
        "            random.setstate(state)\n",
        "    else:\n",
        "        start_iter = 0\n",
        "\n",
        "    x = x0\n",
        "\n",
        "    if not postprocessing:\n",
        "        postprocessing = lambda x: x\n",
        "\n",
        "    exploss = None\n",
        "\n",
        "    for iter in range(start_iter + 1, iterations + 1):\n",
        "        # You might want to print the progress every few iterations.\n",
        "\n",
        "        loss = None\n",
        "        ### YOUR CODE HERE (~2 lines)\n",
        "        loss, gradX = f(x)\n",
        "        \n",
        "        x -= step * gradX\n",
        "        if postprocessing:\n",
        "            x = postprocessing(x)\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        x = postprocessing(x)\n",
        "        if iter % PRINT_EVERY == 0:\n",
        "            if not exploss:\n",
        "                exploss = loss\n",
        "            else:\n",
        "                exploss = .95 * exploss + .05 * loss\n",
        "            print(\"iter %d: %f\" % (iter, exploss))\n",
        "\n",
        "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
        "            save_params(iter, x)\n",
        "\n",
        "        if iter % ANNEAL_EVERY == 0:\n",
        "            step *= 0.5\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def sanity_check():\n",
        "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
        "\n",
        "    print(\"Running sanity checks...\")\n",
        "    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n",
        "    print(\"test 1 result:\", t1)\n",
        "    assert abs(t1) <= 1e-6\n",
        "\n",
        "    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n",
        "    print(\"test 2 result:\", t2)\n",
        "    assert abs(t2) <= 1e-6\n",
        "\n",
        "    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n",
        "    print(\"test 3 result:\", t3)\n",
        "    assert abs(t3) <= 1e-6\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    print(\"ALL TESTS PASSED\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sanity_check()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running sanity checks...\n",
            "iter 100: 0.004578\n",
            "iter 200: 0.004353\n",
            "iter 300: 0.004136\n",
            "iter 400: 0.003929\n",
            "iter 500: 0.003733\n",
            "iter 600: 0.003546\n",
            "iter 700: 0.003369\n",
            "iter 800: 0.003200\n",
            "iter 900: 0.003040\n",
            "iter 1000: 0.002888\n",
            "test 1 result: 8.414836786079764e-10\n",
            "iter 100: 0.000000\n",
            "iter 200: 0.000000\n",
            "iter 300: 0.000000\n",
            "iter 400: 0.000000\n",
            "iter 500: 0.000000\n",
            "iter 600: 0.000000\n",
            "iter 700: 0.000000\n",
            "iter 800: 0.000000\n",
            "iter 900: 0.000000\n",
            "iter 1000: 0.000000\n",
            "test 2 result: 0.0\n",
            "iter 100: 0.041205\n",
            "iter 200: 0.039181\n",
            "iter 300: 0.037222\n",
            "iter 400: 0.035361\n",
            "iter 500: 0.033593\n",
            "iter 600: 0.031913\n",
            "iter 700: 0.030318\n",
            "iter 800: 0.028802\n",
            "iter 900: 0.027362\n",
            "iter 1000: 0.025994\n",
            "test 3 result: -2.524451035823933e-09\n",
            "----------------------------------------\n",
            "ALL TESTS PASSED\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuiyiSeT7Ns9"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from utils.treebank import StanfordSentiment\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from word2vec import *\n",
        "from sgd import *\n",
        "\n",
        "# Check Python Version\n",
        "import sys\n",
        "assert sys.version_info[0] == 3\n",
        "assert sys.version_info[1] >= 5\n",
        "\n",
        "# Reset the random seed to make sure that everyone gets the same results\n",
        "random.seed(314)\n",
        "dataset = StanfordSentiment()\n",
        "tokens = dataset.tokens()\n",
        "nWords = len(tokens)\n",
        "\n",
        "# We are going to train 10-dimensional vectors for this assignment\n",
        "dimVectors = 10\n",
        "\n",
        "# Context size\n",
        "C = 5\n",
        "\n",
        "# Reset the random seed to make sure that everyone gets the same results\n",
        "random.seed(31415)\n",
        "np.random.seed(9265)\n",
        "\n",
        "startTime=time.time()\n",
        "wordVectors = np.concatenate(\n",
        "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
        "       dimVectors, np.zeros((nWords, dimVectors))),\n",
        "    axis=0)\n",
        "wordVectors = sgd(\n",
        "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n",
        "        negSamplingLossAndGradient),\n",
        "    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
        "# Note that normalization is not called here. This is not a bug,\n",
        "# normalizing during training loses the notion of length.\n",
        "\n",
        "print(\"sanity check: cost at convergence should be around or below 10\")\n",
        "print(\"training took %d seconds\" % (time.time() - startTime))\n",
        "\n",
        "# concatenate the input and output word vectors\n",
        "wordVectors = np.concatenate(\n",
        "    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
        "    axis=0)\n",
        "\n",
        "visualizeWords = [\n",
        "    \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
        "    \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"dumb\",\n",
        "    \"annoying\", \"female\", \"male\", \"queen\", \"king\", \"man\", \"woman\", \"rain\", \"snow\",\n",
        "    \"hail\", \"coffee\", \"tea\"]\n",
        "\n",
        "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
        "visualizeVecs = wordVectors[visualizeIdx, :]\n",
        "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
        "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
        "U,S,V = np.linalg.svd(covariance)\n",
        "coord = temp.dot(U[:,0:2])\n",
        "\n",
        "for i in range(len(visualizeWords)):\n",
        "    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n",
        "        bbox=dict(facecolor='green', alpha=0.1))\n",
        "\n",
        "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
        "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
        "\n",
        "plt.savefig('word_vectors.png')\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}