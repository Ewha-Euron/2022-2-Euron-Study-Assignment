{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaesunkkk/2022-2-Euron-Study-Assignment/blob/week6/NLP_Week6_%EB%B3%B5%EC%8A%B5%EA%B3%BC%EC%A0%9C___week5_2022_nlp_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhUHfXkPAORh"
      },
      "source": [
        "ğŸ“Œ week5 ë‚´ìš© ì£¼ì°¨ì— í•´ë‹¹ë˜ëŠ” ê³¼ì œëŠ” Glove ëª¨ë¸ ì‹¤ìŠµ, NER task ì‹¤ìŠµ, Dependency Parsing task ì‹¤ìŠµìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (**ì°¸ê³ ** : **ì œì¶œì€ week6 branch ë³µìŠµê³¼ì œë¡œ!**)\n",
        "\n",
        "ğŸ“Œ ìœ„í‚¤ë…ìŠ¤ì˜ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ êµì¬ ì‹¤ìŠµ, ìºê¸€ ë…¸íŠ¸ë¶ ë“±ì˜ ìë£Œë¡œ êµ¬ì„±ë˜ì–´ìˆëŠ” ê³¼ì œì…ë‹ˆë‹¤. \n",
        "\n",
        "ğŸ“Œ ì•ˆë‚´ëœ ë§í¬ì— ë§ì¶”ì–´ **ì§ì ‘ ì½”ë“œë¥¼ ë”°ë¼ ì¹˜ë©´ì„œ (í•„ì‚¬)** í•´ë‹¹ nlp task ì˜ ê¸°ë³¸ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë©”ì„œë“œë¥¼ ìˆ™ì§€í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ğŸ˜Š í•„ìˆ˜ë¼ê³  ì²´í¬í•œ ë¶€ë¶„ì€ ê³¼ì œì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì£¼ì‹œê³ , ì„ íƒìœ¼ë¡œ ì²´í¬í•œ ë¶€ë¶„ì€ ììœ¨ì ìœ¼ë¡œ ìŠ¤í„°ë”” í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ê¶ê¸ˆí•œ ì‚¬í•­ì€ ê¹ƒí—ˆë¸Œ ì´ìŠˆë‚˜, ì¹´í†¡ë°©, ì„¸ì…˜ ë°œí‘œ ì‹œì‘ ì´ì „ ì‹œê°„ ë“±ì„ í™œìš©í•˜ì—¬ ììœ ë¡­ê²Œ ê³µìœ í•´ì£¼ì„¸ìš”!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XjTSbcxBB6o",
        "outputId": "0d6ac6ef-7b8f-4b9b-efeb-7bf9fc8937cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "# nltk colab í™˜ê²½ì—ì„œ ì‹¤í–‰ì‹œ í•„ìš”í•œ ì½”ë“œì…ë‹ˆë‹¤. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vPZn15zBHIv"
      },
      "source": [
        "### 1ï¸âƒ£ **Glove**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P11biHcUuBaH"
      },
      "source": [
        "ğŸ‘€ **ë‚´ìš© ë³µìŠµ** \n",
        "* ìŠ¤íƒ í¬ë“œ ëŒ€í•™ì—ì„œ ê°œë°œí•œ ì¹´ìš´íŠ¸ ê¸°ë°˜ê³¼ ì˜ˆì¸¡ ê¸°ë°˜ì„ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ ì„ë² ë”© ë°©ë²•ë¡  \n",
        "* word2vec ì˜ ë‹¨ì ì„ ë³´ì™„í•´ì„œ ë‚˜ì˜¨ ëª¨ë¸ \n",
        "* glove model ì˜ **input ì€ ë°˜ë“œì‹œ ë™ì‹œë“±ì¥í–‰ë ¬ í˜•íƒœ**ì—¬ì•¼ í•œë‹¤ â­\n",
        "\n",
        "![1](https://www.dropbox.com/s/nz0ji4yzre56ifv/word_presentation.png?raw=1) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ğŸ¤” í•œêµ­ì–´ ì˜ˆì œëŠ” ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” k-Glove ë¡œ ì†Œê°œë˜ëŠ” ì—°êµ¬ê°€ ìˆê¸´ í•œë°, ì¢€ ë” ì•Œì•„ë´ì•¼ í•  ê²ƒ ê°™ì•„ìš”!\n",
        "\n",
        "â• [ë…¼ë¬¸1](https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=NPAP13255003&dbt=NPAP)\n",
        "\n",
        "\n",
        "â•[ë…¼ë¬¸2](https://scienceon.kisti.re.kr/commons/util/originalView.do?cn=CFKO201832073078664&oCn=NPAP13255064&dbt=CFKO&journal=NPRO00383361&keyword=%ED%95%9C%EA%B5%AD%EC%96%B4%20%EB%8C%80%ED%99%94%20%EC%97%94%EC%A7%84%EC%97%90%EC%84%9C%EC%9D%98%20%EB%AC%B8%EC%9E%A5%EB%B6%84%EB%A5%98)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asGcGy6fBM1E"
      },
      "source": [
        "ğŸ”¹ **1-(1)** glove python\n",
        "\n",
        "* [ì‹¤ìŠµ : basic code](https://wikidocs.net/22885) ğŸ‘‰ í•„ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V31NoJdu5t3p",
        "outputId": "c41e83ec-85d7-4eb6-e733-dffe9ae46805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting glove_python_binary\n",
            "  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 948 kB 9.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.21.6)\n",
            "Installing collected packages: glove-python-binary\n",
            "Successfully installed glove-python-binary-0.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install glove_python_binary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta6QgoKO5uXJ",
        "outputId": "a67f6d0f-cb14-4179-f0c8-e42bed8c2afd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing 20 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n"
          ]
        }
      ],
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "result=[]\n",
        "corpus = Corpus() \n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„°ë¡œë¶€í„° GloVeì—ì„œ ì‚¬ìš©í•  ë™ì‹œ ë“±ì¥ í–‰ë ¬ ìƒì„±\n",
        "corpus.fit(result, window=5)\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "\n",
        "# í•™ìŠµì— ì´ìš©í•  ì“°ë ˆë“œì˜ ê°œìˆ˜ëŠ” 4ë¡œ ì„¤ì •, ì—í¬í¬ëŠ” 20.\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ADfVM9lO9NE"
      },
      "source": [
        "ğŸ”¹ **1-(2)** pre-trained glove \n",
        "\n",
        "* **ì‚¬ì „í•™ìŠµëª¨ë¸** : ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë˜ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë“¤ì„ ë‹¤ë¥¸ ë¬¸ì œì— í•™ìŠµì‹œí‚¨ ê°€ì¤‘ì¹˜ë“¤ë¡œ ì´ˆê¸°í™”í•˜ëŠ” ë°©ë²•ì´ë‹¤.ì‚¬ì „ í•™ìŠµí•œ ê°€ì¤‘ì¹˜ë¥¼ í™œìš©í•´ í•™ìŠµí•˜ê³ ì í•˜ëŠ” ë³¸ë˜ ë¬¸ì œë¥¼ í•˜ìœ„ë¬¸ì œë¼ê³  í•œë‹¤. \n",
        "\n",
        "* [ì‹¤ìŠµ : ë¬¸ì¥ì˜ ê¸ë¶€ì •ì„ íŒë‹¨í•˜ëŠ” ê°ì„± ë¶„ë¥˜ ëª¨ë¸ ë§Œë“¤ê¸°](https://wikidocs.net/33793) ğŸ‘‰ í•„ìˆ˜\n",
        "  * [ì„¤ëª…ì°¸ê³ ](https://omicro03.medium.com/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-16%EC%9D%BC%EC%B0%A8-pre-trained-word-embedding-bb30db424a35)\n",
        "* pre-trained data ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦¼\n",
        "* kaggle ëŒ€íšŒì—ì„œ ì£¼ë¡œ ì´ ë°©ì‹ì„ ë§ì´ ì‚¬ìš©í•¨\n",
        "  * [ì°¸ê³ ](https://lsjsj92.tistory.com/455)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding"
      ],
      "metadata": {
        "id": "TirrO57QSO_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlngY35O53sk"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "output_dim = 128\n",
        "input_length = 500\n",
        "\n",
        "v = Embedding(vocab_size, output_dim, input_length=input_length)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
        "y_train = [1, 0, 0, 1, 1, 0, 1]"
      ],
      "metadata": {
        "id": "mQ2V_UBLRhSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1 \n",
        "print('ë‹¨ì–´ ì§‘í•© :',vocab_size)"
      ],
      "metadata": {
        "id": "jQsqP8flRm-J",
        "outputId": "a770174d-3867-4c08-b26e-dfdefa1097b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¨ì–´ ì§‘í•© : 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print('ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼ :',X_encoded)"
      ],
      "metadata": {
        "id": "CJEesr2rRpRV",
        "outputId": "feca5674-8442-44c5-94ce-76fba2a59f00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼ : [[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(l) for l in X_encoded)\n",
        "print('ìµœëŒ€ ê¸¸ì´ :',max_len)"
      ],
      "metadata": {
        "id": "U3KCh-B-RtI8",
        "outputId": "b5b056d2-3206-4e34-854b-b9cff4ac9854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìµœëŒ€ ê¸¸ì´ : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
        "y_train = np.array(y_train)\n",
        "print('íŒ¨ë”© ê²°ê³¼ :')\n",
        "print(X_train)"
      ],
      "metadata": {
        "id": "zI1O5BcwRu92",
        "outputId": "579f075d-4145-4048-c866-c7b7e2f8a353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "íŒ¨ë”© ê²°ê³¼ :\n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  0  0]\n",
            " [ 7  8  0  0]\n",
            " [ 9 10  0  0]\n",
            " [11 12  0  0]\n",
            " [13  0  0  0]\n",
            " [14 15  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "embedding_dim = 4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "id": "IVqVpyT7RxPs",
        "outputId": "d3c1112c-dc4c-4774-82fb-385900cf6c2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 1s - loss: 0.6885 - acc: 0.7143 - 835ms/epoch - 835ms/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.6874 - acc: 0.7143 - 9ms/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.6863 - acc: 0.7143 - 9ms/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.6852 - acc: 0.7143 - 9ms/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.6841 - acc: 0.7143 - 5ms/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.6830 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.6819 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.6808 - acc: 0.8571 - 9ms/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.6797 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.6786 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.6774 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.6763 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.6751 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.6740 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.6728 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.6716 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.6705 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.6693 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.6681 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.6669 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.6657 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.6644 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.6632 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.6620 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.6607 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.6594 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.6582 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.6569 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.6556 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.6543 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.6529 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.6516 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.6503 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.6489 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.6475 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.6462 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.6448 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.6434 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.6420 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.6405 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.6391 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.6376 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.6362 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.6347 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.6332 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.6317 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.6302 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.6287 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.6272 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.6256 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.6241 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.6225 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.6209 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.6193 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.6177 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.6161 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.6145 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.6129 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.6112 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.6096 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.6079 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.6062 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.6045 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.6028 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.6011 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.5994 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.5977 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.5959 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.5942 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.5924 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.5906 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.5888 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.5871 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.5853 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.5834 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.5816 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.5798 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.5780 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.5761 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.5742 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.5724 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.5705 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.5686 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.5667 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.5648 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.5629 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.5610 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.5591 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.5572 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.5552 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.5533 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.5513 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.5494 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.5474 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.5454 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.5434 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.5415 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.5395 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.5375 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.5355 - acc: 1.0000 - 6ms/epoch - 6ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8636bbc190>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "id": "0nD-LA61Sb_H",
        "outputId": "a89dcad6-5f0f-4f9a-d70f-99fa703c5a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  2  3  4]\n",
            " [ 5  6  0  0]\n",
            " [ 7  8  0  0]\n",
            " [ 9 10  0  0]\n",
            " [11 12  0  0]\n",
            " [13  0  0  0]\n",
            " [14 15  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "id": "Z-I6-eMhR0Nx",
        "outputId": "7cfba94c-bfb7-40e0-88f4-04ed5219e5e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve, urlopen\n",
        "import gzip\n",
        "import zipfile\n",
        "\n",
        "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
        "zf = zipfile.ZipFile('glove.6B.zip')\n",
        "zf.extractall() \n",
        "zf.close()"
      ],
      "metadata": {
        "id": "Ha8Yvz9aSeUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict = dict()\n",
        "\n",
        "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "\n",
        "    # 100ê°œì˜ ê°’ì„ ê°€ì§€ëŠ” arrayë¡œ ë³€í™˜\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "\n",
        "print('%sê°œì˜ Embedding vectorê°€ ìˆìŠµë‹ˆë‹¤.' % len(embedding_dict))"
      ],
      "metadata": {
        "id": "_TkkWkhtSgC_",
        "outputId": "fed907c8-c0b9-4bf2-abfd-46e2dda95029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000ê°œì˜ Embedding vectorê°€ ìˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_dict['respectable'])\n",
        "print('ë²¡í„°ì˜ ì°¨ì› ìˆ˜ :',len(embedding_dict['respectable']))"
      ],
      "metadata": {
        "id": "xedylqydSiKp",
        "outputId": "d82ef320-e2f6-41e0-ad5f-d08ee6437d69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.049773   0.19903    0.10585    0.1391    -0.32395    0.44053\n",
            "  0.3947    -0.22805   -0.25793    0.49768    0.15384   -0.08831\n",
            "  0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\n",
            "  0.74756    0.98256    0.81872    0.28507    0.16178   -0.48626\n",
            " -0.006265  -0.92469   -0.30625   -0.067318  -0.046762  -0.76291\n",
            " -0.0025264 -0.018795   0.12882   -0.52457    0.3586     0.43119\n",
            " -0.89477   -0.057421  -0.53724    0.25587    0.55195    0.44698\n",
            " -0.24252    0.29946    0.25776   -0.8717     0.68426   -0.05688\n",
            " -0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488\n",
            " -0.32507   -0.90171    0.17672    0.075601   0.54896   -0.21488\n",
            " -0.54018   -0.45882   -0.79536    0.26331    0.18879   -0.16363\n",
            "  0.3975     0.1099     0.1164    -0.083499   0.50159    0.35802\n",
            "  0.25677    0.088546   0.42108    0.28674   -0.71285   -0.82915\n",
            "  0.15297   -0.82712    0.022112   1.067     -0.31776    0.1211\n",
            " -0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694\n",
            " -0.0090944  0.1109     0.62543   -0.23682   -0.44928   -0.3667\n",
            " -0.21616   -0.19187   -0.032502   0.38025  ]\n",
            "ë²¡í„°ì˜ ì°¨ì› ìˆ˜ : 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "print('ì„ë² ë”© í–‰ë ¬ì˜ í¬ê¸°(shape) :',np.shape(embedding_matrix))"
      ],
      "metadata": {
        "id": "pbl0t_s1SmQH",
        "outputId": "8fdd44a3-c9fe-4463-c569-bf0bcd4f42dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì„ë² ë”© í–‰ë ¬ì˜ í¬ê¸°(shape) : (16, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index.items())"
      ],
      "metadata": {
        "id": "E6DGF_vxSn4y",
        "outputId": "59650bb3-e811-4d06-98d1-7de035547f46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('highly', 14), ('respectable', 15)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('ë‹¨ì–´ greatì˜ ë§µí•‘ëœ ì •ìˆ˜ :',tokenizer.word_index['great'])"
      ],
      "metadata": {
        "id": "saclILELSpXV",
        "outputId": "6178f7d0-e497-4bd4-f1c3-2386a881da77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¨ì–´ greatì˜ ë§µí•‘ëœ ì •ìˆ˜ : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_dict['great'])"
      ],
      "metadata": {
        "id": "b4gVMfcISr9v",
        "outputId": "1f4294ef-6389-4957-df44-716029d4acb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.013786   0.38216    0.53236    0.15261   -0.29694   -0.20558\n",
            " -0.41846   -0.58437   -0.77355   -0.87866   -0.37858   -0.18516\n",
            " -0.128     -0.20584   -0.22925   -0.42599    0.3725     0.26077\n",
            " -1.0702     0.62916   -0.091469   0.70348   -0.4973    -0.77691\n",
            "  0.66045    0.09465   -0.44893    0.018917   0.33146   -0.35022\n",
            " -0.35789    0.030313   0.22253   -0.23236   -0.19719   -0.0053125\n",
            " -0.25848    0.58081   -0.10705   -0.17845   -0.16206    0.087086\n",
            "  0.63029   -0.76649    0.51619    0.14073    1.019     -0.43136\n",
            "  0.46138   -0.43585   -0.47568    0.19226    0.36065    0.78987\n",
            "  0.088945  -2.7814    -0.15366    0.01015    1.1798     0.15168\n",
            " -0.050112   1.2626    -0.77527    0.36031    0.95761   -0.11385\n",
            "  0.28035   -0.02591    0.31246   -0.15424    0.3778    -0.13599\n",
            "  0.2946    -0.31579    0.42943    0.086969   0.019169  -0.27242\n",
            " -0.31696    0.37327    0.61997    0.13889    0.17188    0.30363\n",
            " -1.2776     0.044423  -0.52736   -0.88536   -0.19428   -0.61947\n",
            " -0.10146   -0.26301   -0.061707   0.36627   -0.95223   -0.39346\n",
            " -0.69183   -1.0426     0.28855    0.63056  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in tokenizer.word_index.items():\n",
        "    # ë‹¨ì–´ì™€ ë§µí•‘ë˜ëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì„ë² ë”© ë²¡í„°ê°’\n",
        "    vector_value = embedding_dict.get(word)\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value"
      ],
      "metadata": {
        "id": "7IHJHkE1SuNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix[2]"
      ],
      "metadata": {
        "id": "XQmzT5LBSvsj",
        "outputId": "0e1e2573-1e22-4282-dfa9-00c73b4661a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.013786  ,  0.38216001,  0.53236002,  0.15261   , -0.29694   ,\n",
              "       -0.20558   , -0.41846001, -0.58437002, -0.77354997, -0.87866002,\n",
              "       -0.37858   , -0.18516   , -0.12800001, -0.20584001, -0.22925   ,\n",
              "       -0.42598999,  0.3725    ,  0.26076999, -1.07019997,  0.62915999,\n",
              "       -0.091469  ,  0.70348001, -0.4973    , -0.77691001,  0.66044998,\n",
              "        0.09465   , -0.44893   ,  0.018917  ,  0.33146   , -0.35021999,\n",
              "       -0.35789001,  0.030313  ,  0.22253001, -0.23236001, -0.19719   ,\n",
              "       -0.0053125 , -0.25848001,  0.58081001, -0.10705   , -0.17845   ,\n",
              "       -0.16205999,  0.087086  ,  0.63028997, -0.76648998,  0.51618999,\n",
              "        0.14072999,  1.01900005, -0.43136001,  0.46138   , -0.43584999,\n",
              "       -0.47567999,  0.19226   ,  0.36065   ,  0.78987002,  0.088945  ,\n",
              "       -2.78139997, -0.15366   ,  0.01015   ,  1.17980003,  0.15167999,\n",
              "       -0.050112  ,  1.26259995, -0.77526999,  0.36030999,  0.95761001,\n",
              "       -0.11385   ,  0.28035   , -0.02591   ,  0.31246001, -0.15424   ,\n",
              "        0.37779999, -0.13598999,  0.29460001, -0.31579   ,  0.42943001,\n",
              "        0.086969  ,  0.019169  , -0.27241999, -0.31696001,  0.37327   ,\n",
              "        0.61997002,  0.13889   ,  0.17188001,  0.30362999, -1.27760005,\n",
              "        0.044423  , -0.52736002, -0.88536   , -0.19428   , -0.61947   ,\n",
              "       -0.10146   , -0.26301   , -0.061707  ,  0.36627001, -0.95222998,\n",
              "       -0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "output_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, output_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "id": "eLaYvx9_Sxoy",
        "outputId": "576e9508-4f5e-42b0-fcd9-61bda9e81212",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 1s - loss: 0.8028 - acc: 0.1429 - 518ms/epoch - 518ms/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.7808 - acc: 0.4286 - 7ms/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.7595 - acc: 0.4286 - 8ms/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.7390 - acc: 0.4286 - 7ms/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.7192 - acc: 0.4286 - 8ms/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.7001 - acc: 0.4286 - 5ms/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.6816 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.6638 - acc: 0.5714 - 7ms/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.6467 - acc: 0.5714 - 7ms/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.6302 - acc: 0.5714 - 8ms/epoch - 8ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.6142 - acc: 0.7143 - 9ms/epoch - 9ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.5988 - acc: 0.7143 - 10ms/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.5839 - acc: 0.7143 - 10ms/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.5696 - acc: 0.7143 - 6ms/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.5557 - acc: 0.7143 - 5ms/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.5422 - acc: 0.7143 - 7ms/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.5292 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.5166 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.5044 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.4926 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.4811 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.4700 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.4592 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.4487 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.4385 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.4286 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.4190 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.4096 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.4005 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.3917 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.3831 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.3748 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.3667 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.3588 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.3511 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.3437 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.3364 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.3294 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.3225 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.3158 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.3093 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.3030 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.2969 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.2909 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.2851 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.2795 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.2739 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.2686 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.2634 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.2583 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.2534 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.2486 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.2439 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.2394 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.2350 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.2307 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.2265 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.2224 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.2184 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.2145 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.2108 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.2071 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.2035 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.2000 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.1966 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.1933 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.1901 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.1869 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.1839 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.1809 - acc: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.1780 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.1751 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.1723 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.1696 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.1670 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.1644 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.1619 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.1594 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.1570 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.1546 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.1523 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.1501 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.1479 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.1458 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.1437 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.1416 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.1396 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.1377 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.1358 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.1339 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.1321 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.1303 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.1285 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.1268 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.1251 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.1235 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.1218 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.1203 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.1187 - acc: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.1172 - acc: 1.0000 - 9ms/epoch - 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f86302772d0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_wcrE5PtLMI"
      },
      "source": [
        "ğŸ”¹ **1-(3)** fine tuning glove\n",
        "* ë¯¸ì„¸ì¡°ì • : ì‚¬ì „ í•™ìŠµí•œ ëª¨ë“  ê°€ì¤‘ì¹˜ì™€ ë”ë¶ˆì–´ í•˜ìœ„ ë¬¸ì œë¥¼ ìœ„í•œ ìµœì†Œí•œì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¶”ê°€í•´ ëª¨ë¸ì„ ì¶”ê°€ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ë‹¤. \n",
        "\n",
        "* fine tuning ì´ í•„ìš”í•œ ê²½ìš° \n",
        "  * pretrained model ì— ë°ì´í„°ì…‹ì— ìˆëŠ” ë‹¨ì–´ê°€ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš° \n",
        "  * ë°ì´í„° ì§‘í•©ì´ ë„ˆë¬´ ì‘ì•„ì„œ ì „ì²´ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê¸° ì–´ë ¤ìš´ ê²½ìš° \n",
        "\n",
        "* [Mittens ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ fine tuning](https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39) ğŸ‘‰ í•„ìˆ˜\n",
        "  *  GloVe ì„ë² ë”©ì„ fine-tuning í•˜ê¸° ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "  * [github](https://github.com/roamanalytics/mittens)\n",
        "\n",
        "* [í•œêµ­ì–´ ì†Œì„¤ í…ìŠ¤íŠ¸ ë°ì´í„° ë¯¸ì„¸ì¡°ì • ëª¨ë¸ í•™ìŠµ - GPT2](https://m.blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=horajjan&logNo=222104684132&categoryNo=120&proxyReferer=) ğŸ‘‰ ì„ íƒ (glove ëª¨ë¸ ì˜ˆì œëŠ” ì•„ë‹™ë‹ˆë‹¤. fine-tuning ì— ì´ˆì ì„ ë‘ì–´ì„œ ì°¸ê³ í•´ì£¼ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDBDn64S58U5",
        "outputId": "24337c87-8e96-4be5-960f-f8b03d26335d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mittens\n",
            "  Downloading mittens-0.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mittens) (1.21.6)\n",
            "Installing collected packages: mittens\n",
            "Successfully installed mittens-0.2\n"
          ]
        }
      ],
      "source": [
        "pip install -U mittens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHiR5mN4577l"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "from nltk.corpus import brown\n",
        "from mittens import GloVe, Mittens\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def glove2dict(glove_filename):\n",
        "    with open(glove_filename, encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
        "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
        "                for line in reader}\n",
        "    return embed"
      ],
      "metadata": {
        "id": "_iZLRm0pS9BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_-OB9Siga3G"
      },
      "source": [
        "* (ì°¸ê³ ) word2vec pretrained example\n",
        "\n",
        "â• [word2vec ì‚¬ì „í•™ìŠµ ëª¨ë¸ -í•œêµ­ì–´1](http://doc.mindscale.kr/km/unstructured/11.html)\n",
        "\n",
        "â• [word2vec ì‚¬ì „í•™ìŠµ - í•œêµ­ì–´2](https://monetd.github.io/python/nlp/Word-Embedding-Word2Vec-%EC%8B%A4%EC%8A%B5/#%ED%95%9C%EA%B5%AD%EC%96%B4-word2vec-%EB%A7%8C%EB%93%A4%EA%B8%B0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUWWDwdiPLS9"
      },
      "source": [
        "### **2ï¸âƒ£ NER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N0B4VknPkTk"
      },
      "source": [
        "ğŸ‘€ **ë‚´ìš© ë³µìŠµ** \n",
        "* ê°œì²´ëª… ì¸ì‹ì„ ì‚¬ìš©í•˜ë©´ ì½”í¼ìŠ¤ë¡œë¶€í„° ì–´ë–¤ ë‹¨ì–´ê°€ ì‚¬ëŒ, ì¥ì†Œ, ì¡°ì§ ë“±ì„ ì˜ë¯¸í•˜ëŠ” ë‹¨ì–´ì¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWgla1BuPRqJ"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "ğŸ”¹ **2-(1)** NER task by nltk library\n",
        "\n",
        "\n",
        "* nltk ì—ì„œëŠ” ê°œì²´ëª… ì¸ì‹ê¸° (NER chunker) ë¥¼ ì§€ì›í•˜ê³  ìˆë‹¤. \n",
        "* ne_chunk ëŠ” ê°œì²´ëª…ì„ íƒœê¹…í•˜ê¸° ìœ„í•´ì„œ ì•ì„œ í’ˆì‚¬ íƒœê¹… pos_tag ê°€ ìˆ˜í–‰ë˜ì–´ì•¼ í•œë‹¤. \n",
        "\n",
        "\n",
        "ğŸ“Œ [basic code](https://wikidocs.net/30682) ğŸ‘‰ í•„ìˆ˜ \n",
        "\n",
        "ğŸ“Œ [BIO í‘œí˜„, LSTMì„ í™œìš©í•œ NER ì‹¤ìŠµ](https://wikidocs.net/24682) ğŸ‘‰ ì„ íƒ\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diaZweMyAxJz",
        "outputId": "d4cf3256-f14c-4124-a551-bc5d0e8fe364",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1k09tKha3Lgi",
        "outputId": "0bb2827c-4519-433e-de9c-df217352b02e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "sentence = \"James is working at Disney in London\"\n",
        "\n",
        "#POS Tagging\n",
        "\n",
        "tokenized_sentence = pos_tag(word_tokenize(sentence))\n",
        "print(tokenized_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NER\n",
        "\n",
        "ner_sentence = ne_chunk(tokenized_sentence)\n",
        "print(ner_sentence)"
      ],
      "metadata": {
        "id": "7n5Ir17GTHI0",
        "outputId": "49d36bd6-3a13-421b-f44d-97f55fa3032e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON James/NNP)\n",
            "  is/VBZ\n",
            "  working/VBG\n",
            "  at/IN\n",
            "  (ORGANIZATION Disney/NNP)\n",
            "  in/IN\n",
            "  (GPE London/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPX-WtSvPmm6"
      },
      "source": [
        "ğŸ”¹ **2-(2)** NER task by spacy library\n",
        "\n",
        "\n",
        "* spaCy ëŠ” ìì—°ì–´ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒŒì´ì¬ ê¸°ë°˜ì˜ ì˜¤í”ˆ ì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤. \n",
        "  * Tokenization \n",
        "  * POS tagging \n",
        "  * Lemmatization \n",
        "  * Sentence Boundary Detection (SBD)\n",
        "  * Named Entity Recognition (NER)\n",
        "  * Similarity\n",
        "  * Text Classification\n",
        "  * Rule-based Matching\n",
        "  * Training\n",
        "  * Serialization\n",
        "\n",
        "* spaCy ì™€ NER\n",
        "  * .ents â†’ .label_\n",
        "\n",
        "\n",
        "ğŸ“Œ [basic code](https://frhyme.github.io/python-lib/nlp_spacy_1/) ğŸ‘‰ í•„ìˆ˜ (NER ë¶€ë¶„ë§Œ)\n",
        "\n",
        "ğŸ“Œ [kaggle_Custom NER using SpaCy](https://www.kaggle.com/code/amarsharma768/custom-ner-using-spacy/notebook) ğŸ‘‰ ì„ íƒ\n",
        "\n",
        "  * í›ˆë ¨ë˜ì§€ ì•Šì€ ë°ì´í„° ì„¸íŠ¸ì— ëª…ëª…ëœ ì—”í‹°í‹°ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²• : ì´ë ¥ì„œ pdf ë°ì´í„° í™œìš© \n",
        "  * manually labelled \n",
        "\n",
        "ğŸ“Œ [í•œêµ­ì–´ NER](https://github.com/monologg/KoBERT-NER) ğŸ‘‰ ì°¸ê³ í•˜ë©´ ì¢‹ì„ ìë£Œ\n",
        "\n",
        "â• [ì°¸ê³ ](http://aispiration.com/nlp2/nlp-ner-python.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXjRfz-qP0Xx",
        "outputId": "9cf518bd-ec1a-4467-d304-c686e68b88de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "U.K. GPE\n",
            "$1 billion MONEY\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp('Apple is looking at buying U.K. startup for $1 billion')\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"\"\"But Google is starting from behind. The company made a late push\n",
        "into hardware, and Appleâ€™s Siri, available on iPhones, and Amazonâ€™s Alexa\n",
        "software, which runs on its Echo and Dot devices, have clear leads in\n",
        "consumer adoption.\"\"\".replace(\"\\n\", \" \").strip())\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "8ytrVwVUTRBF",
        "outputId": "147448f4-0726-41f0-a5fb-e62e1c1b37d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "Appleâ€™s Siri ORG\n",
            "iPhones ORG\n",
            "Amazon ORG\n",
            "Alexa ORG\n",
            "Echo GPE\n",
            "Dot ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "008-V5QsQG25"
      },
      "source": [
        "###**3ï¸âƒ£ Dependency Parsing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQfcodHQQPlt"
      },
      "source": [
        "ğŸ‘€ **ë‚´ìš© ë³µìŠµ** \n",
        "* ë¬¸ì¥ì˜ ì „ì²´ì ì¸ êµ¬ì„±/êµ¬ì¡° ë³´ë‹¤ëŠ” ê° ê°œë³„ë‹¨ì–´ ê°„ì˜ 'ì˜ì¡´ê´€ê³„' ë˜ëŠ” 'ìˆ˜ì‹ê´€ê³„' ì™€ ê°™ì€ ë‹¨ì–´ê°„ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì´ ëª©ì ì¸ NLP Task\n",
        "* ë¬¸ì¥ í•´ì„ì˜ ëª¨í˜¸ì„±ì„ ì—†ì• ê¸° ìœ„í•´ Parsing ì„ í•œë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJLAzZnbRNlL"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "ğŸ”¹ **3-(1)** Dependency Parsing by spacy library\n",
        "\n",
        "\n",
        "* [basic](https://frhyme.github.io/python-lib/nlp_spacy_1/#navigating-parse-tree) ğŸ‘‰ dependecy parsing ë¶€ë¶„ë§Œ í•„ìˆ˜\n",
        "* .dep_ ë©”ì„œë“œ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbQEYt76bJXz",
        "outputId": "a7cead5a-d4e3-48dc-94dc-3020bd8e7544",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'generator'>\n",
            "<class 'spacy.tokens.span.Span'>\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "============================================================\n",
            "Text: The original noun chunk text.\n",
            "Root text: The original text of the word connecting the noun chunk to the rest of the parse.\n",
            "Root dep: Dependency relation connecting the root to its head.\n",
            "Root head text: The text of the root token's head.\n",
            "============================================================\n",
            "          Autonomous cars                     cars                    nsubj                    shift\n",
            "      insurance liability                liability                     dobj                    shift\n",
            "            manufacturers            manufacturers                     pobj                   toward\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "\n",
        "noun_chunks = doc.noun_chunks\n",
        "print(type(noun_chunks))\n",
        "noun_chunk = list(noun_chunks)[0]\n",
        "print(type(noun_chunk))\n",
        "token = noun_chunk[0]\n",
        "print(type(token))\n",
        "\n",
        "print(\"==\"*30)\n",
        "print(\"\"\"\n",
        "Text: The original noun chunk text.\n",
        "Root text: The original text of the word connecting the noun chunk to the rest of the parse.\n",
        "Root dep: Dependency relation connecting the root to its head.\n",
        "Root head text: The text of the root token's head.\n",
        "\"\"\".strip())\n",
        "print(\"==\"*30)\n",
        "str_format = \"{:>25}\"*4\n",
        "for chunk in doc.noun_chunks:\n",
        "  print(str_format.format(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9QAEsrLAxHP",
        "outputId": "d25084b1-4d4b-49a6-cd92-2f1685c723cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autonomous\n",
            "childre: [] head: cars\n",
            "================================\n",
            "cars\n",
            "childre: [Autonomous] head: shift\n",
            "================================\n",
            "shift\n",
            "childre: [cars, liability] head: !this is root node\n",
            "================================\n",
            "insurance\n",
            "childre: [] head: liability\n",
            "================================\n",
            "liability\n",
            "childre: [insurance, toward] head: shift\n",
            "================================\n",
            "toward\n",
            "childre: [manufacurers] head: liability\n",
            "================================\n",
            "manufacurers\n",
            "childre: [] head: toward\n",
            "================================\n"
          ]
        }
      ],
      "source": [
        "#nevigating parse tree\n",
        "\n",
        "doc=nlp(\"Autonomous cars shift insurance liability toward manufacurers\")\n",
        "for tok in doc:\n",
        "  print(tok.text)\n",
        "  children = list(tok.children)\n",
        "  print('childre:', children, 'head:', tok.head if tok.head != tok else \"!this is root node\")\n",
        "  print(\"==\"*16)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ì´ë¥¼ ê°„ë‹¨í•˜ê²Œ ë„¤íŠ¸ì›Œí¬ë¡œ í‘œí˜„\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nG = nx.Graph()\n",
        "doc[2] ##root node\n",
        "\n",
        "def add_n_to_g(inputG, tok):\n",
        "  inputG.add_node(tok)\n",
        "  children = list(tok.children)\n",
        "  if children != []:\n",
        "    inputG.add_nodes_from(children)\n",
        "    for c in children:\n",
        "      inputG.add_edges_from([(tok, c, {'dependency':c.dep_})])\n",
        "      add_n_to_g(inputG, c)\n",
        "\n",
        "add_n_to_g(nG, doc[2])\n",
        "print(nG.nodes(data=True))\n",
        "print(\"==\"*20)\n",
        "for e in nG.edges(data=True):\n",
        "  print(f\"{e[0]}, {e[1]}, ### dependency: {e[2]['dependency']}\")"
      ],
      "metadata": {
        "id": "_hnlJSLoTYyd",
        "outputId": "b74998b1-ec41-453f-f9c3-38db1073d6da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(shift, {}), (cars, {}), (liability, {}), (Autonomous, {}), (insurance, {}), (toward, {}), (manufacurers, {})]\n",
            "========================================\n",
            "shift, cars, ### dependency: nsubj\n",
            "shift, liability, ### dependency: dobj\n",
            "cars, Autonomous, ### dependency: amod\n",
            "liability, insurance, ### dependency: compound\n",
            "liability, toward, ### dependency: prep\n",
            "toward, manufacurers, ### dependency: pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQD5oiGgRfHe"
      },
      "source": [
        "ğŸ”¹ **3-(2)** Spacy (kaggle) \n",
        "\n",
        "* ìºê¸€ ë…¸íŠ¸ë¶ í™˜ê²½ì—ì„œ ì‹¤ìŠµí•´ë³´ëŠ” ê²ƒì„ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤!\n",
        "\n",
        "* [kaggle_spaCy](https://www.kaggle.com/code/nirant/hitchhiker-s-guide-to-nlp-in-spacy) ğŸ‘‰ í•„ìˆ˜\n",
        "  * ë„ë‚ ë“œ íŠ¸ëŸ¼í”„ íŠ¸ìœ„í„° íŠ¸ìœ— ë‚´ìš© ë°ì´í„° ë¶„ì„\n",
        "\n",
        "\n",
        "ğŸ‘€ **ë…¸íŠ¸ë¶ í‚¤í¬ì¸íŠ¸** \n",
        "  1. spacy.display ë©”ì„œë“œë¥¼ ì‚¬ìš©í•œ NER ì‹œê°í™” \n",
        "  2. Tagging ì„ í†µí•œ íŠ¸ëŸ¼í”„ íŠ¸ìœ— ë¶„ì„ : noun_chunks ëŠ” dependency graphë¥¼ ê³ ë ¤í•˜ì—¬, noun phraseë¥¼ ë½‘ì•„ì¤€ë‹¤. \n",
        "  3. [spacy Match](https://yujuwon.tistory.com/entry/spaCy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-Rule-based-Matching) : ì§ì ‘ ë¬¸ì¥/ë‹¨ì–´ íŒ¨í„´ì„ ë“±ë¡í•˜ì—¬ parsing\n",
        "  4. Question and answering task using Dependency Parsing\n",
        "    * spacy display :  ``style = 'dep'``\n",
        "    * .dep_\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ggGrR76lTvyG",
        "outputId": "0dfbd9c5-2ca9-4709-fc67-1a755bd8047e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = pd.read_csv(\"/content/drive/MyDrive/Euron_NLP/á„‡á…©á†¨á„‰á…³á†¸á„€á…ªá„Œá…¦/data/all_djt_tweets.csv\")"
      ],
      "metadata": {
        "id": "kkLrrGMKTzCa",
        "outputId": "a78abcfb-ef39-47a4-8c11-9db21b5a988b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (0,1,2,3,4,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuHGKITRbKYq",
        "outputId": "2f080c9f-4390-4d5f-ae3c-7f14de73a142",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-10 14:36:18.486733: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.0/en_core_web_lg-3.4.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 587.7 MB 7.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (21.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (8.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.0.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.4.0\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "2022-10-10 14:37:19.177001: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\n",
            "\u001b[38;5;1mâœ˜ No compatible package found for 'en_vectors_web_lg' (spaCy\n",
            "v3.4.1)\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "!python -m spacy download en_vectors_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMArOUrxAJ8K"
      },
      "outputs": [],
      "source": [
        "def explain_text_entities(text):\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        print(f'Entity: {ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "explain_text_entities(tweets['text'][9])"
      ],
      "metadata": {
        "id": "XVXsWr-kT2Zh",
        "outputId": "198011f4-9603-4282-96d7-07bf4524fc00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Friday, Label: DATE, Absolute or relative dates or periods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_sentence = tweets['text'][0]\n",
        "doc = nlp(one_sentence)\n",
        "spacy.displacy.render(doc, style='ent',jupyter=True)"
      ],
      "metadata": {
        "id": "M5C5yiQVT4Hr",
        "outputId": "f375555a-fe16-4a54-a914-5f44286d9721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Over 90%\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
              "</mark>\n",
              " approval rating for your all time favorite (I hope) President within \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Republican Party\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    52%\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
              "</mark>\n",
              " overall. This despite all of the made up stories by \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Fake News Media\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " trying endlessly to make me look as bad and evil as possible. Look at the real villains please!</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def redact_names(text):\n",
        "    doc = nlp(text)\n",
        "    redacted_sentence = []\n",
        "    for ent in doc.ents:\n",
        "        ent.merge()\n",
        "    for token in doc:\n",
        "        if token.ent_type_ == \"PERSON\":\n",
        "            redacted_sentence.append(\"[REDACTED]\")\n",
        "        else:\n",
        "            redacted_sentence.append(token.string)\n",
        "    return \"\".join(redacted_sentence)"
      ],
      "metadata": {
        "id": "ZlE0zRW9T5zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = tweets['text'][9]\n",
        "doc = nlp(example_text)\n",
        "spacy.displacy.render(doc, style='ent', jupyter=True)\n",
        "\n",
        "for idx, sentence in enumerate(doc.sents):\n",
        "    for noun in sentence.noun_chunks:\n",
        "        print(f\"sentence {idx+1} has noun chunk '{noun}'\")"
      ],
      "metadata": {
        "id": "zEe-c1JZT7lD",
        "outputId": "4a44d2b7-b448-42d1-edf5-f878ba1ca223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Stock Market hit all time high on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Friday\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". Congratulations U.S.A.!</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence 1 has noun chunk 'Stock Market'\n",
            "sentence 1 has noun chunk 'all time'\n",
            "sentence 1 has noun chunk 'Friday'\n",
            "sentence 2 has noun chunk 'Congratulations U.S.A.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_sentence = tweets['text'][300]\n",
        "doc = nlp(one_sentence)\n",
        "spacy.displacy.render(doc, style='ent', jupyter=True)\n",
        "\n",
        "for token in doc:\n",
        "    print(token, token.pos_)"
      ],
      "metadata": {
        "id": "DcNWILEiT-cn",
        "outputId": "d857c89f-251d-4ebd-e25b-40ba8a94770f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Incredibly beautiful ceremony as \n",
              "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.S. Korean War\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
              "</mark>\n",
              " remains are returned to \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    American\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " soil. Thank you to \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Honolulu\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " and all of our great Military participants on a job well done. A special thanks to Vice President \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mike Pence\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " on delivering a truly magnificent tribute!</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Incredibly ADV\n",
            "beautiful ADJ\n",
            "ceremony NOUN\n",
            "as SCONJ\n",
            "U.S. PROPN\n",
            "Korean PROPN\n",
            "War PROPN\n",
            "remains VERB\n",
            "are AUX\n",
            "returned VERB\n",
            "to ADP\n",
            "American ADJ\n",
            "soil NOUN\n",
            ". PUNCT\n",
            "Thank VERB\n",
            "you PRON\n",
            "to ADP\n",
            "Honolulu PROPN\n",
            "and CCONJ\n",
            "all PRON\n",
            "of ADP\n",
            "our PRON\n",
            "great ADJ\n",
            "Military ADJ\n",
            "participants NOUN\n",
            "on ADP\n",
            "a DET\n",
            "job NOUN\n",
            "well ADV\n",
            "done VERB\n",
            ". PUNCT\n",
            "A DET\n",
            "special ADJ\n",
            "thanks NOUN\n",
            "to ADP\n",
            "Vice PROPN\n",
            "President PROPN\n",
            "Mike PROPN\n",
            "Pence PROPN\n",
            "on ADP\n",
            "delivering VERB\n",
            "a DET\n",
            "truly ADV\n",
            "magnificent ADJ\n",
            "tribute NOUN\n",
            "! PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = tweets['text'].str.cat(sep=' ')\n",
        "# spaCy enforces a max limit of 1000000 characters for NER and similar use cases.\n",
        "# Since `text` might be longer than that, we will slice it off here\n",
        "max_length = 1000000-1\n",
        "text = text[:max_length]\n",
        "\n",
        "# removing URLs and '&amp' substrings using regex\n",
        "import re\n",
        "url_reg  = r'[a-z]*[:.]+\\S+'\n",
        "text   = re.sub(url_reg, '', text)\n",
        "noise_reg = r'\\&amp'\n",
        "text   = re.sub(noise_reg, '', text)"
      ],
      "metadata": {
        "id": "y7DEXPHeT_vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "eUOBB2i7UClv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "items_of_interest = list(doc.noun_chunks)\n",
        "items_of_interest = [str(x) for x in items_of_interest]"
      ],
      "metadata": {
        "id": "3FloYR-_UD9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trump_topics = []\n",
        "for token in doc:\n",
        "    if (not token.is_stop) and (token.pos_ == \"NOUN\") and (len(str(token))>2):\n",
        "        trump_topics.append(token)\n",
        "        \n",
        "trump_topics = [str(x) for x in trump_topics]"
      ],
      "metadata": {
        "id": "KOpdsE_uUFU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trump_topics = []\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ not in [\"PERCENT\", \"CARDINAL\", \"DATE\"]:\n",
        "#         print(ent.text,ent.label_)\n",
        "        trump_topics.append(ent.text.strip())"
      ],
      "metadata": {
        "id": "4sw7aem4UGi6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}