{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moonyaeji/2022-2-Euron-Study-Assignment/blob/master/week13_nlp_%EB%B3%B5%EC%8A%B5%EA%B3%BC%EC%A0%9C\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“Œ  **ìƒë‹¨ì˜ 'íŒŒì¼-ë“œë¼ì´ë¸Œì— ì‚¬ë³¸ ì €ì¥' í•´ì„œ ë³¸ì¸ ë“œë¼ì´ë¸Œì— ì €ì¥ëœ ì‚¬ë³¸ ì´ìš©í•´ì„œ ì‹¤ìŠµ í•´ì£¼ì„¸ìš”!!**\n",
        "\n",
        "ğŸ“Œ week13 ë³µìŠµìŠµê³¼ì œëŠ” **NLG ì‹¤ìŠµ**ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ìœ„í‚¤ë…ìŠ¤ì˜ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ êµì¬ ì‹¤ìŠµ, ê´€ë ¨ ë¸”ë¡œê·¸ ë“±ì˜ ë¬¸ì„œ ìë£Œë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” ê³¼ì œì…ë‹ˆë‹¤. \n",
        "\n",
        "ğŸ“Œ ì•ˆë‚´ëœ ë§í¬ì— ë§ì¶”ì–´ **ì§ì ‘ ì½”ë“œë¥¼ ë”°ë¼ ì¹˜ë©´ì„œ (í•„ì‚¬)** í•´ë‹¹ nlp task ì˜ ê¸°ë³¸ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë©”ì„œë“œë¥¼ ìˆ™ì§€í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ğŸ˜Š í•„ìˆ˜ë¼ê³  ì²´í¬í•œ ë¶€ë¶„ì€ ê³¼ì œì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì£¼ì‹œê³ , ì„ íƒìœ¼ë¡œ ì²´í¬í•œ ë¶€ë¶„ì€ ììœ¨ì ìœ¼ë¡œ ìŠ¤í„°ë”” í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ê¶ê¸ˆí•œ ì‚¬í•­ì€ ê¹ƒí—ˆë¸Œ ì´ìŠˆë‚˜, ì¹´í†¡ë°©, ì„¸ì…˜ ë°œí‘œ ì‹œì‘ ì´ì „ ì‹œê°„ ë“±ì„ í™œìš©í•˜ì—¬ ììœ ë¡­ê²Œ ê³µìœ í•´ì£¼ì„¸ìš”!"
      ],
      "metadata": {
        "id": "BX3ac8Ag1RPC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppuTdZVWSo0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk colab í™˜ê²½ì—ì„œ ì‹¤í–‰ì‹œ í•„ìš”í•œ ì½”ë“œì…ë‹ˆë‹¤. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "4JEquLR91VBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ¥° **ì´í•˜ ì˜ˆì œë¥¼ ì‹¤ìŠµí•˜ì‹œë©´ ë©ë‹ˆë‹¤.**\n",
        "\n",
        "**1-(1)~(2)ëŠ” í•„ìˆ˜ê³¼ì œ, 2ëŠ” ì„ íƒê³¼ì œì…ë‹ˆë‹¤.**\n"
      ],
      "metadata": {
        "id": "Kq8aMYKGPQR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1ï¸âƒ£ NLG task ì‹¤ìŠµ**"
      ],
      "metadata": {
        "id": "SHTPAk95iNtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‘€ ë‚´ìš© ë³µìŠµ\n",
        "\n",
        "NLG ëŠ” ìƒˆë¡œìš´ text ë¥¼ ë§Œë“¤ì–´ ë‚´ëŠ” ëª¨ë“  task ë¥¼ ì˜ë¯¸í•˜ë©° ê¸°ê³„ë²ˆì—­, í…ìŠ¤íŠ¸ ìš”ì•½, ì±„íŒ…, ìŠ¤í† ë¦¬í…”ë§, QA ë“±ì´ ìˆë‹¤. "
      ],
      "metadata": {
        "id": "j5msd7Igjz9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 1-(1) RNN ì„ ì´ìš©í•œ text generation \n",
        "\n",
        "ğŸ“Œ [Text generation with RNN](https://wikidocs.net/45101) \n",
        "\n",
        "* Simple RNN ì„ ì´ìš©í•œ ê°„ë‹¨í•œ í•œêµ­ì–´ text generation ì˜ˆì œì™€ LSTM ì„ ì´ìš©í•œ ë‰´ìš• íƒ€ì„ì¦ˆ ê¸°ì‚¬ í—¤ë“œë¼ì¸ ìƒì„± ì˜ˆì œë¥¼ í•„ì‚¬í•´ì£¼ì„¸ìš”."
      ],
      "metadata": {
        "id": "9L-jAHPkiBV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° : %d' % vocab_size)\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "sequences = list()\n",
        "for line in text.split('\\n'): # ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ í† í°í™”\n",
        "    encoded = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "print('í•™ìŠµì— ì‚¬ìš©í•  ìƒ˜í”Œì˜ ê°œìˆ˜: %d' % len(sequences))\n",
        "\n",
        "print(sequences)\n",
        "\n",
        "max_len = max(len(l) for l in sequences) # ëª¨ë“  ìƒ˜í”Œì—ì„œ ê¸¸ì´ê°€ ê°€ì¥ ê¸´ ìƒ˜í”Œì˜ ê¸¸ì´ ì¶œë ¥\n",
        "print('ìƒ˜í”Œì˜ ìµœëŒ€ ê¸¸ì´ : {}'.format(max_len))\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "print(sequences)\n",
        "sequences = np.array(sequences)\n",
        "X = sequences[:,:-1]\n",
        "y = sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
        "embedding_dim = 10\n",
        "hidden_units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=200, verbose=2)\n",
        "\n",
        "def sentence_generation(model, tokenizer, current_word, n): # ëª¨ë¸, í† í¬ë‚˜ì´ì €, í˜„ì¬ ë‹¨ì–´, ë°˜ë³µí•  íšŸìˆ˜\n",
        "    init_word = current_word\n",
        "    sentence = ''\n",
        "\n",
        "    # në²ˆ ë°˜ë³µ\n",
        "    for _ in range(n):\n",
        "        # í˜„ì¬ ë‹¨ì–´ì— ëŒ€í•œ ì •ìˆ˜ ì¸ì½”ë”©ê³¼ íŒ¨ë”©\n",
        "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n",
        "        # ì…ë ¥í•œ X(í˜„ì¬ ë‹¨ì–´)ì— ëŒ€í•´ì„œ Yë¥¼ ì˜ˆì¸¡í•˜ê³  Y(ì˜ˆì¸¡í•œ ë‹¨ì–´)ë¥¼ resultì— ì €ì¥.\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "\n",
        "        for word, index in tokenizer.word_index.items(): \n",
        "            # ë§Œì•½ ì˜ˆì¸¡í•œ ë‹¨ì–´ì™€ ì¸ë±ìŠ¤ì™€ ë™ì¼í•œ ë‹¨ì–´ê°€ ìˆë‹¤ë©´ break\n",
        "            if index == result:\n",
        "                break\n",
        "\n",
        "        # í˜„ì¬ ë‹¨ì–´ + ' ' + ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ í˜„ì¬ ë‹¨ì–´ë¡œ ë³€ê²½\n",
        "        current_word = current_word + ' '  + word\n",
        "\n",
        "        # ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ë¬¸ì¥ì— ì €ì¥\n",
        "        sentence = sentence + ' ' + word\n",
        "\n",
        "    sentence = init_word + sentence\n",
        "    return sentence\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "Klj6gjETZdgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "df = pd.read_csv('ArticlesApril2018.csv')\n",
        "df.head()\n",
        "\n",
        "headline = []\n",
        "# í—¤ë“œë¼ì¸ì˜ ê°’ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
        "headline.extend(list(df.headline.values)) \n",
        "headline[:5]\n",
        "\n",
        "headline = [word for word in headline if word != \"Unknown\"]\n",
        "print('ë…¸ì´ì¦ˆê°’ ì œê±° í›„ ìƒ˜í”Œì˜ ê°œìˆ˜ : {}'.format(len(headline)))\n",
        "\n",
        "headline[:5]\n",
        "\n",
        "def repreprocessing(raw_sentence):\n",
        "    preproceseed_sentence = raw_sentence.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    # êµ¬ë‘ì  ì œê±°ì™€ ë™ì‹œì— ì†Œë¬¸ìí™”\n",
        "    return ''.join(word for word in preproceseed_sentence if word not in punctuation).lower()\n",
        "\n",
        "preprocessed_headline = [repreprocessing(x) for x in headline]\n",
        "preprocessed_headline[:5]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_headline)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° : %d' % vocab_size)\n",
        "sequences = list()\n",
        "\n",
        "for sentence in preprocessed_headline:\n",
        "\n",
        "    # ê° ìƒ˜í”Œì— ëŒ€í•œ ì •ìˆ˜ ì¸ì½”ë”©\n",
        "    encoded = tokenizer.texts_to_sequences([sentence])[0] \n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "sequences[:11]\n",
        "index_to_word = {}\n",
        "for key, value in tokenizer.word_index.items(): # ì¸ë±ìŠ¤ë¥¼ ë‹¨ì–´ë¡œ ë°”ê¾¸ê¸° ìœ„í•´ index_to_wordë¥¼ ìƒì„±\n",
        "    index_to_word[value] = key\n",
        "\n",
        "print('ë¹ˆë„ìˆ˜ ìƒìœ„ 582ë²ˆ ë‹¨ì–´ : {}'.format(index_to_word[582]))\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "print(sequences[:3])"
      ],
      "metadata": {
        "id": "MsJZ9ldFiARK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 1-(2) Text summarization with attention\n",
        "\n",
        "ğŸ“Œ [ì•„ë§ˆì¡´ ë¦¬ë·° ìš”ì•½](https://wikidocs.net/72820) \n",
        "\n",
        "* seq2seq + attention ì„ ì´ìš©í•œ ì•„ë§ˆì¡´ ë¦¬ë·° ê¸€ text summarization ì˜ˆì œë¥¼ í•„ì‚¬í•´ì£¼ì„¸ìš”."
      ],
      "metadata": {
        "id": "ktAuag_Nk5Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import urllib.request\n",
        "np.random.seed(seed=0)\n",
        "\n",
        "# Reviews.csv íŒŒì¼ì„ dataë¼ëŠ” ì´ë¦„ì˜ ë°ì´í„°í”„ë ˆì„ì— ì €ì¥. ë‹¨, 10ë§Œê°œì˜ í–‰(rows)ìœ¼ë¡œ ì œí•œ.\n",
        "data = pd.read_csv(\"Reviews.csv íŒŒì¼ì˜ ê²½ë¡œ\", nrows = 100000)\n",
        "print('ì „ì²´ ë¦¬ë·° ê°œìˆ˜ :',(len(data)))\n",
        "\n",
        "data = data[['Text','Summary']]\n",
        "data.head()\n",
        "data.sample(10)\n",
        "data.drop_duplicates(subset=['Text'], inplace=True)\n",
        "print(\"ì „ì²´ ìƒ˜í”Œìˆ˜ :\", len(data))\n",
        "data.dropna(axis=0, inplace=True)\n",
        "print('ì „ì²´ ìƒ˜í”Œìˆ˜ :',(len(data)))\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print('ë¶ˆìš©ì–´ ê°œìˆ˜ :', len(stop_words))\n",
        "print(stop_words)\n",
        "\n",
        "def preprocess_sentence(sentence, remove_stopwords = True):\n",
        "    sentence = sentence.lower() # í…ìŠ¤íŠ¸ ì†Œë¬¸ìí™”\n",
        "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> ë“±ì˜ html íƒœê·¸ ì œê±°\n",
        "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # ê´„í˜¸ë¡œ ë‹«íŒ ë¬¸ìì—´  ì œê±° Ex) my husband (and myself) for => my husband for\n",
        "    sentence = re.sub('\"','', sentence) # ìŒë”°ì˜´í‘œ \" ì œê±°\n",
        "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # ì•½ì–´ ì •ê·œí™”\n",
        "    sentence = re.sub(r\"'s\\b\",\"\",sentence) # ì†Œìœ ê²© ì œê±°. Ex) roland's -> roland\n",
        "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # ì˜ì–´ ì™¸ ë¬¸ì(ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ë“±) ê³µë°±ìœ¼ë¡œ ë³€í™˜\n",
        "    sentence = re.sub('[m]{2,}', 'mm', sentence) # mì´ 3ê°œ ì´ìƒì´ë©´ 2ê°œë¡œ ë³€ê²½. Ex) ummmmmmm yeah -> umm yeah\n",
        "\n",
        "    # ë¶ˆìš©ì–´ ì œê±° (Text)\n",
        "    if remove_stopwords:\n",
        "        tokens = ' '.join(word for word in sentence.split() if not word in stop_words if len(word) > 1)\n",
        "    # ë¶ˆìš©ì–´ ë¯¸ì œê±° (Summary)\n",
        "    else:\n",
        "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
        "    return tokens\n",
        "\n",
        "    temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
        "temp_summary = 'Great way to start (or finish) the day!!!'\n",
        "print(preprocess_sentence(temp_text))\n",
        "print(preprocess_sentence(temp_summary, 0))"
      ],
      "metadata": {
        "id": "mLmPLEYdZlzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_size = 256\n",
        "\n",
        "# ì¸ì½”ë”\n",
        "encoder_inputs = Input(shape=(text_max_len,))\n",
        "\n",
        "# ì¸ì½”ë”ì˜ ì„ë² ë”© ì¸µ\n",
        "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
        "\n",
        "# ì¸ì½”ë”ì˜ LSTM 1\n",
        "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "# ì¸ì½”ë”ì˜ LSTM 2\n",
        "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# ì¸ì½”ë”ì˜ LSTM 3\n",
        "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# ë””ì½”ë”\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# ë””ì½”ë”ì˜ ì„ë² ë”© ì¸µ\n",
        "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# ë””ì½”ë”ì˜ LSTM\n",
        "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])\n",
        "\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation = 'softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
        "\n",
        "# ëª¨ë¸ ì •ì˜\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/20.%20Text%20Summarization/attention.py\", filename=\"attention.py\")\n",
        "from attention import AttentionLayer\n",
        "\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# ì–´í…ì…˜ì˜ ê²°ê³¼ì™€ ë””ì½”ë”ì˜ hidden stateë“¤ì„ ì—°ê²°\n",
        "decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# ë””ì½”ë”ì˜ ì¶œë ¥ì¸µ\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
        "\n",
        "# ëª¨ë¸ ì •ì˜\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
        "history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
        "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size = 256, callbacks=[es], epochs = 50)"
      ],
      "metadata": {
        "id": "xcgptSxJ3zhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2ï¸âƒ£ Text summarization task**"
      ],
      "metadata": {
        "id": "HfTr_BPwGc8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 2-(1) Pororo - text summarization \n",
        "\n",
        "ğŸ“Œ [ê³µì‹ë¬¸ì„œ](https://kakaobrain.github.io/pororo/seq2seq/summary.html) \n",
        "\n",
        "ğŸ“Œ [ì˜ˆì œ ì‹¤ìŠµ](https://teddylee777.github.io/machine-learning/nlp-korean-pororo) \n",
        "\n",
        "* PORORO : ì¹´ì¹´ì˜¤ ë¸Œë ˆì¸ì—ì„œ ì œê³µí•œ ìì—°ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬"
      ],
      "metadata": {
        "id": "eu3adOV2bDs_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e2WbVbbsatD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NmVEEfAqdSa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 2-(2) BERT ë¥¼ ì´ìš©í•œ text summarization \n",
        "\n",
        "ğŸ“Œ [ë…¼ë¬¸ ë¦¬ë·°](https://medium.com/@eyfydsyd97/bert%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%9A%94%EC%95%BD-text-summary-b582b5cc7d) \n",
        "\n",
        "ğŸ“Œ [BERT Extractive summarizer Library](https://github.com/dmmiller612/bert-extractive-summarizer) \n",
        "\n",
        "\n",
        "ğŸ“Œ [Text summarization Github Repo](https://github.com/uoneway/Text-Summarization-Repo) \n",
        "\n",
        "\n",
        "\n",
        "â• [BERT ë¥¼ ì´ìš©í•œ ë‰´ìŠ¤ ìš”ì•½ ìë™í™” App êµ¬í˜„ Repo](https://github.com/huydang90/News-Summarization-with-BERT) ğŸ‘‰ í”„ë¡œì íŠ¸ ì˜ˆì‹œ ì°¸ê³  ìë£Œ\n",
        "\n"
      ],
      "metadata": {
        "id": "xsx6OgzwbnIt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "svtikbdZatBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mnfL6KIZdSuo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
