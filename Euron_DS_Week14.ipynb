{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2500e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, os\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', 700)\n",
    "\n",
    "path = r'C:\\Users\\yerim\\Jupyter\\OpinosisDataset1.0\\topics'\n",
    "# path로 지정한 디렉터리 밑에 있는 모든 .data 파일들의 파일명을 리스트로 취합\n",
    "all_files = glob.glob(os.path.join(path, \"*.data\"))    \n",
    "filename_list = []\n",
    "opinion_text = []\n",
    "\n",
    "# 개별 파일들의 파일명은 filename_list 리스트로 취합, \n",
    "# 개별 파일들의 파일 내용은 DataFrame 로딩 후 다시 string으로 변환하여 opinion_text 리스트로 취합 \n",
    "for file_ in all_files:\n",
    "    # 개별 파일을 읽어서 DataFrame으로 생성 \n",
    "    df = pd.read_table(file_,index_col=None, header=0,encoding='latin1')\n",
    "    \n",
    "    # 절대경로로 주어진 file 명을 가공. 만일 Linux에서 수행시에는 아래 \\\\를 / 변경. \n",
    "    # 맨 마지막 .data 확장자도 제거\n",
    "    filename_ = file_.split('\\\\')[-1]\n",
    "    filename = filename_.split('.')[0]\n",
    "\n",
    "    # 파일명 리스트와 파일 내용 리스트에 파일명과 파일 내용을 추가. \n",
    "    filename_list.append(filename)\n",
    "    opinion_text.append(df.to_string())\n",
    "\n",
    "# 파일명 리스트와 파일 내용 리스트를  DataFrame으로 생성\n",
    "document_df = pd.DataFrame({'filename':filename_list, 'opinion_text':opinion_text})\n",
    "document_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dade057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "lemmar = WordNetLemmatizer()\n",
    "\n",
    "# 입력으로 들어온 token단어들에 대해서 lemmatization 어근 변환. \n",
    "def LemTokens(tokens):\n",
    "    return [lemmar.lemmatize(token) for token in tokens]\n",
    "\n",
    "# TfidfVectorizer 객체 생성 시 tokenizer인자로 해당 함수를 설정하여 lemmatization 적용\n",
    "# 입력으로 문장을 받아서 stop words 제거-> 소문자 변환 -> 단어 토큰화 -> lemmatization 어근 변환. \n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2902db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english' , \\\n",
    "                             ngram_range=(1,2), min_df=0.05, max_df=0.85 )\n",
    "\n",
    "#opinion_text 컬럼값으로 feature vectorization 수행\n",
    "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df['cluster_label'] = cluster_label\n",
    "document_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df[document_df['cluster_label']==0].sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df[document_df['cluster_label']==1].sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df[document_df['cluster_label']==2].sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3771098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df[document_df['cluster_label']==3].sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24352b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df[document_df['cluster_label']==4].sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c690a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 3개의 집합으로 군집화 \n",
    "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
    "km_cluster.fit(feature_vect)\n",
    "cluster_label = km_cluster.labels_\n",
    "\n",
    "\n",
    "# 소속 클러스터를 cluster_label 컬럼으로 할당하고 cluster_label 값으로 정렬\n",
    "document_df['cluster_label'] = cluster_label\n",
    "document_df.sort_values(by='cluster_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f77ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = km_cluster.cluster_centers_\n",
    "print('cluster_centers shape :',cluster_centers.shape)\n",
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 군집별 top n 핵심단어, 그 단어의 중심 위치 상대값, 대상 파일명들을 반환함. \n",
    "def get_cluster_details(cluster_model, cluster_data, feature_names, clusters_num, top_n_features=10):\n",
    "    cluster_details = {}\n",
    "    \n",
    "    # cluster_centers array 의 값이 큰 순으로 정렬된 index 값을 반환\n",
    "    # 군집 중심점(centroid)별 할당된 word 피처들의 거리값이 큰 순으로 값을 구하기 위함.  \n",
    "    centroid_feature_ordered_ind = cluster_model.cluster_centers_.argsort()[:,::-1]\n",
    "    \n",
    "    #개별 군집별로 iteration하면서 핵심단어, 그 단어의 중심 위치 상대값, 대상 파일명 입력\n",
    "    for cluster_num in range(clusters_num):\n",
    "        # 개별 군집별 정보를 담을 데이터 초기화. \n",
    "        cluster_details[cluster_num] = {}\n",
    "        cluster_details[cluster_num]['cluster'] = cluster_num\n",
    "        \n",
    "        # cluster_centers_.argsort()[:,::-1] 로 구한 index 를 이용하여 top n 피처 단어를 구함. \n",
    "        top_feature_indexes = centroid_feature_ordered_ind[cluster_num, :top_n_features]\n",
    "        top_features = [ feature_names[ind] for ind in top_feature_indexes ]\n",
    "        \n",
    "        # top_feature_indexes를 이용해 해당 피처 단어의 중심 위치 상댓값 구함 \n",
    "        top_feature_values = cluster_model.cluster_centers_[cluster_num, top_feature_indexes].tolist()\n",
    "        \n",
    "        # cluster_details 딕셔너리 객체에 개별 군집별 핵심 단어와 중심위치 상대값, 그리고 해당 파일명 입력\n",
    "        cluster_details[cluster_num]['top_features'] = top_features\n",
    "        cluster_details[cluster_num]['top_features_value'] = top_feature_values\n",
    "        filenames = cluster_data[cluster_data['cluster_label'] == cluster_num]['filename']\n",
    "        filenames = filenames.values.tolist()\n",
    "        cluster_details[cluster_num]['filenames'] = filenames\n",
    "        \n",
    "    return cluster_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_details(cluster_details):\n",
    "    for cluster_num, cluster_detail in cluster_details.items():\n",
    "        print('####### Cluster {0}'.format(cluster_num))\n",
    "        print('Top features:', cluster_detail['top_features'])\n",
    "        print('Reviews 파일명 :',cluster_detail['filenames'][:7])\n",
    "        print('==================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0827ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vect.get_feature_names()\n",
    "\n",
    "cluster_details = get_cluster_details(cluster_model=km_cluster, cluster_data=document_df,\\\n",
    "                                  feature_names=feature_names, clusters_num=3, top_n_features=10 )\n",
    "print_cluster_details(cluster_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170afefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cos_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))\n",
    "    similarity = dot_product / l2_norm     \n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58729ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "doc_list = ['if you take the blue pill, the story ends' ,\n",
    "            'if you take the red pill, you stay in Wonderland',\n",
    "            'if you take the red pill, I show you how deep the rabbit hole goes']\n",
    "\n",
    "tfidf_vect_simple = TfidfVectorizer()\n",
    "feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list)\n",
    "print(feature_vect_simple.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c95ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFidfVectorizer로 transform()한 결과는 Sparse Matrix이므로 Dense Matrix로 변환. \n",
    "feature_vect_dense = feature_vect_simple.todense()\n",
    "\n",
    "#첫번째 문장과 두번째 문장의 feature vector  추출\n",
    "vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
    "vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
    "\n",
    "#첫번째 문장과 두번째 문장의 feature vector로 두개 문장의 Cosine 유사도 추출\n",
    "similarity_simple = cos_similarity(vect1, vect2 )\n",
    "print('문장 1, 문장 2 Cosine 유사도: {0:.3f}'.format(similarity_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7b2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
    "vect3 = np.array(feature_vect_dense[2]).reshape(-1,)\n",
    "similarity_simple = cos_similarity(vect1, vect3 )\n",
    "print('문장 1, 문장 3 Cosine 유사도: {0:.3f}'.format(similarity_simple))\n",
    "\n",
    "vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
    "vect3 = np.array(feature_vect_dense[2]).reshape(-1,)\n",
    "similarity_simple = cos_similarity(vect2, vect3 )\n",
    "print('문장 2, 문장 3 Cosine 유사도: {0:.3f}'.format(similarity_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e55cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_simple_pair = cosine_similarity(feature_vect_simple[0] , feature_vect_simple)\n",
    "print(similarity_simple_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_simple_pair = cosine_similarity(feature_vect_simple[0] , feature_vect_simple[1:])\n",
    "print(similarity_simple_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7370d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_simple_pair = cosine_similarity(feature_vect_simple , feature_vect_simple)\n",
    "print(similarity_simple_pair)\n",
    "print('shape:',similarity_simple_pair.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d483048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "lemmar = WordNetLemmatizer()\n",
    "\n",
    "# 입력으로 들어온 token단어들에 대해서 lemmatization 어근 변환. \n",
    "def LemTokens(tokens):\n",
    "    return [lemmar.lemmatize(token) for token in tokens]\n",
    "\n",
    "# TfidfVectorizer 객체 생성 시 tokenizer인자로 해당 함수를 설정하여 lemmatization 적용\n",
    "# 입력으로 문장을 받아서 stop words 제거-> 소문자 변환 -> 단어 토큰화 -> lemmatization 어근 변환. \n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7857dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "path = r'OpinosisDataset1.0\\topics'\n",
    "all_files = glob.glob(os.path.join(path, \"*.data\"))     \n",
    "filename_list = []\n",
    "opinion_text = []\n",
    "\n",
    "for file_ in all_files:\n",
    "    df = pd.read_table(file_,index_col=None, header=0,encoding='latin1')\n",
    "    filename_ = file_.split('\\\\')[-1]\n",
    "    filename = filename_.split('.')[0]\n",
    "    filename_list.append(filename)\n",
    "    opinion_text.append(df.to_string())\n",
    "\n",
    "document_df = pd.DataFrame({'filename':filename_list, 'opinion_text':opinion_text})\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english' , \\\n",
    "                             ngram_range=(1,2), min_df=0.05, max_df=0.85 )\n",
    "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])\n",
    "\n",
    "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
    "km_cluster.fit(feature_vect)\n",
    "cluster_label = km_cluster.labels_\n",
    "cluster_centers = km_cluster.cluster_centers_\n",
    "document_df['cluster_label'] = cluster_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ac182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# cluster_label=2인 데이터는 호텔로 클러스터링된 데이터임. DataFrame에서 해당 Index를 추출\n",
    "hotel_indexes = document_df[document_df['cluster_label']==2].index\n",
    "print('호텔로 클러스터링 된 문서들의 DataFrame Index:', hotel_indexes)\n",
    "\n",
    "# 호텔로 클러스터링된 데이터 중 첫번째 문서를 추출하여 파일명 표시.  \n",
    "comparison_docname = document_df.iloc[hotel_indexes[0]]['filename']\n",
    "print('##### 비교 기준 문서명 ',comparison_docname,' 와 타 문서 유사도######')\n",
    "\n",
    "''' document_df에서 추출한 Index 객체를 feature_vect로 입력하여 호텔 클러스터링된 feature_vect 추출 \n",
    "이를 이용하여 호텔로 클러스터링된 문서 중 첫번째 문서와 다른 문서간의 코사인 유사도 측정.'''\n",
    "similarity_pair = cosine_similarity(feature_vect[hotel_indexes[0]] , feature_vect[hotel_indexes])\n",
    "print(similarity_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebab62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# argsort()를 이용하여 앞예제의 첫번째 문서와 타 문서간 유사도가 큰 순으로 정렬한 인덱스 반환하되 자기 자신은 제외. \n",
    "sorted_index = similarity_pair.argsort()[:,::-1]\n",
    "sorted_index = sorted_index[:, 1:]\n",
    "\n",
    "# 유사도가 큰 순으로 hotel_indexes를 추출하여 재 정렬. \n",
    "hotel_sorted_indexes = hotel_indexes[sorted_index.reshape(-1)]\n",
    "\n",
    "# 유사도가 큰 순으로 유사도 값을 재정렬하되 자기 자신은 제외\n",
    "hotel_1_sim_value = np.sort(similarity_pair.reshape(-1))[::-1]\n",
    "hotel_1_sim_value = hotel_1_sim_value[1:]\n",
    "\n",
    "# 유사도가 큰 순으로 정렬된 Index와 유사도값을 이용하여 파일명과 유사도값을 Seaborn 막대 그래프로 시각화\n",
    "hotel_1_sim_df = pd.DataFrame()\n",
    "hotel_1_sim_df['filename'] = document_df.iloc[hotel_sorted_indexes]['filename']\n",
    "hotel_1_sim_df['similarity'] = hotel_1_sim_value\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "sns.barplot(x='similarity', y='filename',data=hotel_1_sim_df)\n",
    "plt.title(comparison_docname)\n",
    "fig1.savefig('p553_hotel.tif', format='tif', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('ratings_train.txt', sep='\\t')\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af708971",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'].value_counts( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "train_df = train_df.fillna(' ')\n",
    "# 정규 표현식을 이용하여 숫자를 공백으로 변경(정규 표현식으로 \\d 는 숫자를 의미함.) \n",
    "train_df['document'] = train_df['document'].apply( lambda x : re.sub(r\"\\d+\", \" \", x) )\n",
    "\n",
    "# 테스트 데이터 셋을 로딩하고 동일하게 Null 및 숫자를 공백으로 변환\n",
    "test_df = pd.read_csv('ratings_test.txt', sep='\\t')\n",
    "test_df = test_df.fillna(' ')\n",
    "test_df['document'] = test_df['document'].apply( lambda x : re.sub(r\"\\d+\", \" \", x) )\n",
    "\n",
    "# id 칼럼 삭제 수행\n",
    "train_df.drop('id', axis=1, inplace=True) \n",
    "test_df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d66088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "def tw_tokenizer(text):\n",
    "    # 입력 인자로 들어온 text 를 형태소 단어로 토큰화 하여 list 객체 반환\n",
    "    tokens_ko = twitter.morphs(text)\n",
    "    return tokens_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Twitter 객체의 morphs( ) 객체를 이용한 tokenizer를 사용. ngram_range는 (1,2) \n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.9)\n",
    "tfidf_vect.fit(train_df['document'])\n",
    "tfidf_matrix_train = tfidf_vect.transform(train_df['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression 을 이용하여 감성 분석 Classification 수행. \n",
    "lg_clf = LogisticRegression(random_state=0, solver='liblinear')\n",
    "\n",
    "# Parameter C 최적화를 위해 GridSearchCV 를 이용. \n",
    "params = { 'C': [1 ,3.5, 4.5, 5.5, 10 ] }\n",
    "grid_cv = GridSearchCV(lg_clf , param_grid=params , cv=3 ,scoring='accuracy', verbose=1 )\n",
    "grid_cv.fit(tfidf_matrix_train , train_df['label'] )\n",
    "print(grid_cv.best_params_ , round(grid_cv.best_score_,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 학습 데이터를 적용한 TfidfVectorizer를 이용하여 테스트 데이터를 TF-IDF 값으로 Feature 변환함. \n",
    "tfidf_matrix_test = tfidf_vect.transform(test_df['document'])\n",
    "\n",
    "# classifier 는 GridSearchCV에서 최적 파라미터로 학습된 classifier를 그대로 이용\n",
    "best_estimator = grid_cv.best_estimator_\n",
    "preds = best_estimator.predict(tfidf_matrix_test)\n",
    "\n",
    "print('Logistic Regression 정확도: ',accuracy_score(test_df['label'],preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge , LogisticRegression\n",
    "from sklearn.model_selection import train_test_split , cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "mercari_df= pd.read_csv('mercari_train.tsv',sep='\\t')\n",
    "print(mercari_df.shape)\n",
    "mercari_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc35e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mercari_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed282f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_train_df = mercari_df['price']\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(y_train_df, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36fb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_train_df = np.log1p(y_train_df)\n",
    "sns.histplot(y_train_df, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a5231",
   "metadata": {},
   "outputs": [],
   "source": [
    "mercari_df['price'] = np.log1p(mercari_df['price'])\n",
    "mercari_df['price'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a59355",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shipping 값 유형:\\n',mercari_df['shipping'].value_counts())\n",
    "print('item_condition_id 값 유형:\\n',mercari_df['item_condition_id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_cond= mercari_df['item_description']=='No description yet'\n",
    "mercari_df[boolean_cond]['item_description'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply lambda에서 호출되는 대,중,소 분할 함수 생성, 대,중,소 값을 리스트 반환\n",
    "def split_cat(category_name):\n",
    "    try:\n",
    "        return category_name.split('/')\n",
    "    except:\n",
    "        return ['Other_Null' , 'Other_Null' , 'Other_Null']\n",
    "\n",
    "# 위의 split_cat( )을 apply lambda에서 호출하여 대,중,소 컬럼을 mercari_df에 생성. \n",
    "mercari_df['cat_dae'], mercari_df['cat_jung'], mercari_df['cat_so'] = \\\n",
    "                        zip(*mercari_df['category_name'].apply(lambda x : split_cat(x)))\n",
    "\n",
    "# 대분류만 값의 유형과 건수를 살펴보고, 중분류, 소분류는 값의 유형이 많으므로 분류 갯수만 추출\n",
    "print('대분류 유형 :\\n', mercari_df['cat_dae'].value_counts())\n",
    "print('중분류 갯수 :', mercari_df['cat_jung'].nunique())\n",
    "print('소분류 갯수 :', mercari_df['cat_so'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a93dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mercari_df['brand_name'] = mercari_df['brand_name'].fillna(value='Other_Null')\n",
    "mercari_df['category_name'] = mercari_df['category_name'].fillna(value='Other_Null')\n",
    "mercari_df['item_description'] = mercari_df['item_description'].fillna(value='Other_Null')\n",
    "\n",
    "# 각 컬럼별로 Null값 건수 확인. 모두 0가 나와야 합니다.\n",
    "mercari_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9106c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('brand name 의 유형 건수 :', mercari_df['brand_name'].nunique())\n",
    "print('brand name sample 5건 : \\n', mercari_df['brand_name'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79175cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('name 의 종류 갯수 :', mercari_df['name'].nunique())\n",
    "print('name sample 7건 : \\n', mercari_df['name'][:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567c25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 200)\n",
    "\n",
    "# item_description의 평균 문자열 개수\n",
    "print('item_description 평균 문자열 개수:',mercari_df['item_description'].str.len().mean())\n",
    "\n",
    "mercari_df['item_description'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c1532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name 속성에 대한 feature vectorization 변환\n",
    "cnt_vec = CountVectorizer()\n",
    "X_name = cnt_vec.fit_transform(mercari_df.name)\n",
    "\n",
    "# item_description 에 대한 feature vectorization 변환 \n",
    "tfidf_descp = TfidfVectorizer(max_features = 50000, ngram_range= (1,3) , stop_words='english')\n",
    "X_descp = tfidf_descp.fit_transform(mercari_df['item_description'])\n",
    "\n",
    "print('name vectorization shape:',X_name.shape)\n",
    "print('item_description vectorization shape:',X_descp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5588a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# brand_name, item_condition_id, shipping 각 피처들을 희소 행렬 원-핫 인코딩 변환\n",
    "lb_brand_name= LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb_brand_name.fit_transform(mercari_df['brand_name'])\n",
    "\n",
    "lb_item_cond_id = LabelBinarizer(sparse_output=True)\n",
    "X_item_cond_id = lb_item_cond_id.fit_transform(mercari_df['item_condition_id'])\n",
    "\n",
    "lb_shipping= LabelBinarizer(sparse_output=True)\n",
    "X_shipping = lb_shipping.fit_transform(mercari_df['shipping'])\n",
    "\n",
    "# cat_dae, cat_jung, cat_so 각 피처들을 희소 행렬 원-핫 인코딩 변환\n",
    "lb_cat_dae = LabelBinarizer(sparse_output=True)\n",
    "X_cat_dae= lb_cat_dae.fit_transform(mercari_df['cat_dae'])\n",
    "\n",
    "lb_cat_jung = LabelBinarizer(sparse_output=True)\n",
    "X_cat_jung = lb_cat_jung.fit_transform(mercari_df['cat_jung'])\n",
    "\n",
    "lb_cat_so = LabelBinarizer(sparse_output=True)\n",
    "X_cat_so = lb_cat_so.fit_transform(mercari_df['cat_so'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23616616",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_brand), type(X_item_cond_id), type(X_shipping))\n",
    "print('X_brand_shape:{0}, X_item_cond_id shape:{1}'.format(X_brand.shape, X_item_cond_id.shape))\n",
    "print('X_shipping shape:{0}, X_cat_dae shape:{1}'.format(X_shipping.shape, X_cat_dae.shape))\n",
    "print('X_cat_jung shape:{0}, X_cat_so shape:{1}'.format(X_cat_jung.shape, X_cat_so.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c5603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.sparse import hstack\n",
    "import gc\n",
    "\n",
    "sparse_matrix_list = (X_name, X_descp, X_brand, X_item_cond_id,\n",
    "            X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
    "\n",
    "# 사이파이 sparse 모듈의 hstack 함수를 이용하여 앞에서 인코딩과 Vectorization을 수행한 데이터 셋을 모두 결합. \n",
    "X_features_sparse= hstack(sparse_matrix_list).tocsr()\n",
    "print(type(X_features_sparse), X_features_sparse.shape)\n",
    "\n",
    "# 데이터 셋이 메모리를 많이 차지하므로 사용 용도가 끝났으면 바로 메모리에서 삭제. \n",
    "del X_features_sparse\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8458f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y , y_pred):\n",
    "    # underflow, overflow를 막기 위해 log가 아닌 log1p로 rmsle 계산 \n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y) - np.log1p(y_pred), 2)))\n",
    "\n",
    "def evaluate_org_price(y_test , preds): \n",
    "    \n",
    "    # 원본 데이터는 log1p로 변환되었으므로 exmpm1으로 원복 필요. \n",
    "    preds_exmpm = np.expm1(preds)\n",
    "    y_test_exmpm = np.expm1(y_test)\n",
    "    \n",
    "    # rmsle로 RMSLE 값 추출\n",
    "    rmsle_result = rmsle(y_test_exmpm, preds_exmpm)\n",
    "    return rmsle_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535978a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "from  scipy.sparse import hstack\n",
    "\n",
    "def model_train_predict(model,matrix_list):\n",
    "    # scipy.sparse 모듈의 hstack 을 이용하여 sparse matrix 결합\n",
    "    X= hstack(matrix_list).tocsr()     \n",
    "    \n",
    "    X_train, X_test, y_train, y_test=train_test_split(X, mercari_df['price'], \n",
    "                                                      test_size=0.2, random_state=156)\n",
    "    \n",
    "    # 모델 학습 및 예측\n",
    "    model.fit(X_train , y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    del X , X_train , X_test , y_train \n",
    "    gc.collect()\n",
    "    \n",
    "    return preds , y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36b744",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = Ridge(solver = \"lsqr\", fit_intercept=False)\n",
    "\n",
    "sparse_matrix_list = (X_name, X_brand, X_item_cond_id,\n",
    "                      X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
    "linear_preds , y_test = model_train_predict(model=linear_model ,matrix_list=sparse_matrix_list)\n",
    "print('Item Description을 제외했을 때 rmsle 값:', evaluate_org_price(y_test , linear_preds))\n",
    "\n",
    "sparse_matrix_list = (X_descp, X_name, X_brand, X_item_cond_id,\n",
    "                      X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
    "linear_preds , y_test = model_train_predict(model=linear_model , matrix_list=sparse_matrix_list)\n",
    "print('Item Description을 포함한 rmsle 값:',  evaluate_org_price(y_test ,linear_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fda7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "sparse_matrix_list = (X_descp, X_name, X_brand, X_item_cond_id,\n",
    "                      X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
    "\n",
    "lgbm_model = LGBMRegressor(n_estimators=200, learning_rate=0.5, num_leaves=125, random_state=156)\n",
    "lgbm_preds , y_test = model_train_predict(model = lgbm_model , matrix_list=sparse_matrix_list)\n",
    "print('LightGBM rmsle 값:',  evaluate_org_price(y_test , lgbm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lgbm_preds * 0.45 + linear_preds * 0.55\n",
    "print('LightGBM과 Ridge를 ensemble한 최종 rmsle 값:',  evaluate_org_price(y_test , preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
