# **1. K-Means 알고리즘 이해**

### **1-1. K-Means**
- 군집화에서 가장 일반적으로 사용되는 알고리즘
- 군집 중심점(centroid)을 선택 -> 해당 중심에 가장 가까운 포인트들을 선택
- 군집 중심점(centroid) 선택 방법
  - 선택된 포인트의 평균 지점으로 이동
  - 이동된 중심점에서 다시 가까운 포인트를 선택
  - 다시 중심점을 평균 지점으로 이동
  - 반복 수행 -> 더이상 중심점의 이동이 없을 경우에 반복을 멈추고 해당 중심점에 속하는 데이터 포인트들을 군집화
  
    <img src="https://user-images.githubusercontent.com/98953721/199372431-de195e3b-d339-498d-b26f-5ad427ff2798.png" width="600" height="250"/>
  
  
  1) 군집화의 기준이 되는 중심을 구성하려는 군집화 개수만큼 임의의 위치에 가져다놓기
  2) 각 데이터는 가장 가까운 곳에 위치한 중심점에 소속됨
  3) 소속 결정 -> 군집 중심점을 소속된 데이터의 평균 중심으로 이동
  4) 중심점이 이동됨 => 기존에 속한 중심점보다 더 가까운 중심점이 있다면 해당 중심점으로 다시 소속 변경
  5) 다시 중심을 소속된 데이터의 평균 중심으로 이동
  6) 데이터의 중심점 소속 변경이 더 이상 없으면 군집화 종료
  
**- K-Means 알고리즘의 장점**
  1) 일반적인 군집화에서 가장 많이 활용되는 알고리즘
  2) 쉽고 간결함
  3) 개별 군집 내의 데이터가 원형으로 흩어져 있는 경우에 매우 
  
**- K-Means 알고리즘의 단점**
  1) 거리 기반 알고리즘 -> 속성의 개수가 매우 많을 경우 정확도가 떨어짐(개선: PCA를 이용한 차원 축소)
  2) 반복 수행 횟수가 많아지면 수행 시간 증가
  3) 몇 개의 군집을 선택해야 할 지 결정하기 hard


### **1-2. 사이킷런 KMeans 클래스**
**- 초기화 파라미터**
```Python
class sklearn.cluster.KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, 
                             precompute_distances='auto',verbose=0, random_state=None, 
                             copy_x=True, n_jobs=1, algorithm='auto')
```
- n_clusters: 군집화 할 개수, 군집 중심점의 개수
- init: 초기에 군집 중심점의 좌표를 설정할 방식, 일반적으로 k-means++ 방식으로 최초 설정
- max_iter: 최대 반복 횟수, 해당 횟수 이전에 모든 데이터의 중심점 이동이 없으면 종료

**- 학습**
- fit(데이터 세트)
- fit_transform(데이터 세트)

**- 주요 속성 정보 확인**
- labels_: 각 데이터 포인트가 속한 군집 중심점 레이블
- cluster_centers_: 각 군집 중심점 좌표, Shape = (군집 개수, 피처 개수)  
                    => 군집 중심점 좌표가 어딘지 시각화 가능
                    
### **1-3. K-Means를 이용한 붓꽃 데이터 세트 군집화**

### **1-4. 군집화 알고리즘 테스트를 위한 데이터 생성**
- 사이킷런에서 다양한 유형의 군집화 알고리즘을 테스트해 보기 위해 간단한 데이터를 생성해 볼 수 있음

**- 간단한 데이터 생성기**
- 하나의 클래스에 여러 개의 군집이 분포될 수 있게 데이터를 생성할 수 있음
- make_blobs() API
  - 개별 군집의 중심점과 표준 편차 제어 기능
  - feature 데이터 셋과 target 데이터 셋이 튜플(tuple)로 반환됨
  - Parameters
    - n_samples: 생성할 총 데이터의 개수(default = 100)
    - n_features: 데이터의 feature 개수
    - centers: int 형태 => 군집의 개수, ndarray 형태 => 개별 군집 중심점의 좌표
    - cluster_std: 생성될 군집 데이터의 표준 편차, 군집별로 서로 다른 표준 편차를 가진 데이터 세트 생성 가능
                   작을수록 군집 중심에 데이터가 모여 있으며, 클수록 데이터가 퍼져 있음
                   
    <img src="https://user-images.githubusercontent.com/98953721/199440432-25227a48-cbf4-45b7-b91d-58484451bc49.png" width="550" height="200"/>
                   
- make_classification() API: 노이즈를 포함한 데이터를 만드는 데 유용
- make_circle(), make_moon() API: 중심 기반의 군집화로 해결하기 어려운 데이터 세트를 만드는 데 사용됨


# **2. 군집 평가(Cluster Estimation)**

### **2-1. 실루엣 분석의 개요**
**실루엣 분석(silhouette analysis)**
- 각 군집 간의 거리가 얼마나 효율적으로 분리돼 있는지(= 다른 군집과는 먼 거리/ 동일 군집끼리는 가까움)를 나타냄
- 군집화가 잘 될수록 개별 군집은 비슷한 정도의 여유공간을 가지고 떨어져 있을 것임
**실루엣 계수(silhouette coefficient)**
- 개별 데이터가 가지는 군집화 지표
- 해당 데이터가 같은 군집 내의 데이터와 얼마나 가깝게 군집화돼 있고, 다른 군집에 있는 데이터와는 얼마나 멀리 분리돼 있는지를 나타내는 지표
- i번째 데이터 포인트의 실루엣 계수: s(i) = {b(i) - a(i)} / {max(a(i),b(i))}      --- 정규화   
  ( a(i): 해당 데이터 포인트와 '같은' 군집 내에 있는 다른 데이터 포인트와 평균 거리 /  
    b(i): 해당 데이터 포인트가 속하지 않은 군집 중 가장 '가까운' 군집과의 평균 거리)  
<img src="https://user-images.githubusercontent.com/98953721/200035443-088be4e7-70ba-4016-b3f6-8eb8beb44ce9.png" width="650" height="300"/>

- -1 <= s(i) <= 1
- 0(근처의 군집과 가깝다.) <--------> 1(근처의 군집과 멀다.) / (-) 값: 아예 다른 군집에 할당됨  
- methods
  ```
  sklearn.metrics.silhouette_samples(X,labels,metric='euclidean',**kwds)
  ```
  - 각 데이터 포인트의 실루엣 계수 반환
  
  ```
  sklearn.metrics.silhouette_score(X,labels,metric='euclidean',sample_size=None,**kwds)  
  ```
  - 전체 데이터의 실루엣 계수 값의 평균 ---> np.mean(silhouette_samples())
  - 일반적으로 높을수록 군집화가 어느 정도 잘 됐다고 판단할 수 있지만, 높다고 무조건 좋다고 볼 수는 없음  

- 직관적으로 이해하기 쉽지만, 각 데이터별로 다른 데이터와의 거리를 반복적으로 계산해야 하므로 계산량이 많음
- 데이터 개수 증가 시 메모리 부족 등의 에러가 발생할 수 있음 -> 군집별로 임의의 데이터를 샘플링해 평가하는 방안을 고민해야 함

**좋은 군집화의 조건**
- silhouette_score() 값은 0과 1 사이, 1에 가까울수록 good
- 전체 실루엣 계수의 평균값과 더불어 개별 군집의 평균값의 편차가 크지 않아야 함    
  => 개별 군집의 실루엣 계수 평균값이 전체 실루엣 계수의 평균값에서 크게 벗어나지 않는 것이 중요

### **2-2. 붗꽃 데이터 세트를 이용한 군집 평가**

### **2-3. 군집별 평균 실루엣 계수의 시각화를 통한 군집 개수 최적화 방법**
- 개별 군집별로 적당히 분리된 거리를 유지하면서도 군집 내의 데이터가 서로 뭉쳐 있는 경우에 K-Means의 적절한 군집 개수가 설정되었다고 판단


# **3. 평균 이동(Mean Shift)**
- 중심을 데이터가 모여 있는 밀도가 가장 높은 곳으로 지속적으로 이동시키면서 군집화 수행
- 데이터의 분포도를 이용해 군집 중심점을 찾음
- 군집 중심점
  - 데이터 포인트가 모여있는 곳이라는 생각에서 착안
  - 확률 밀도 함수 이용 -> 가장 집중적으로 데이터가 모여있어 확률 밀도 함수가 피크인 점을 군집 중심점으로 선정
  - 일반적으로 주어진 모델의 확률 밀도 함수를 찾기 위해서 KDE(Kernel Density Estimation)을 이용
- 특정 데이터를 반경 내의 데이터 분포 확률 밀도가 가장 높은 곳으로 이동하기 위해 주변 데이터와의 거리 값을 KDE 함수 값으로 입력한 뒤 그 반환 값을 현재 위치에서 업데이트하면서 이동하는 방식을 취함
<img src="https://user-images.githubusercontent.com/98953721/200036258-b8098c3b-84c8-42c1-88f8-cc7ed0df601f.png" width="700" height="250"/>

- KDE(Kernel Density Function)
  - 커널(Kernel) 함수를 통해 어떤 변수의 확률 밀도 함수를 추정하는 대표적인 방법
  - 관측된 데이터 각각에 커널 함수를 적용한 값을 모두 더한 뒤 데이터 건수로 나눠 확률 밀도 함수 추정
  - 대표적인 커널 함수: 가우시안 분포 함수
  - 다음과 같은 커널 함수식으로 표현됨  
  
  $$KDE = \frac{1}{n}\sum_{i=1}^nK_{h}{(x-x_{i})} = \frac{1}{nh}\sum_{i=1}^nK{(\frac{x-x_{i}}{h})}$$
  
    - K: 커널 함수 / x: 확률 변수 값 / xi: 관측값 / h: 대역폭(bandwidth)
    - 대역폭(h)은 KDE 형태를 평활화(Smoothing)하는 데 적용됨  
      - 작은 h값: 좁고 뾰족한 KDE -> 변동성이 큰 방식으로 확률밀도함수 추정 => 과적합이 되기 쉬움
      - 큰 h값: 과도하게 평활화된 KDE -> 지나치게 단순화된 방식으로 확률밀도함수 추정 => 과소적합이 되기 쉬움
      - 일반적으로 대역폭 ∝ (1/군집중심점)
- 확률 밀도 함수(PDF,Probability Density Function)
  - 확률 변수의 분포를 나타내는 함수
  - 정규분포, 감마분포, t분포 등이 있음
  - 확률 밀도 함수를 통해 특정 변수가 어떤 값을 갖게 될지에 대한 확률을 추정할 수 있음 -> 변수의 특성 파악
 - 군집의 개수 지정 x -> 오직 대역폭의 크기에 따라 군집화 수행  
   => 유연한 군집화가 가능 but 대역폭에 따른 군집화 영향도가 매우 크다.
 - sklearn의 MeanShift 클래스 이용
  - Parameters
    - bandwidth: KDE의 대역폭(h)
  - 최적의 대역폭 계산을 위해 estimate_bandwidth() 함수 제공 
  - cluster_centers_ 속성: 시각화 시 군집의 중심 좌표 표시
- 이상치에 대한 영향을 적게 받음
- 알고리즘의 수행 시간이 오래 걸린다는 단점이 존재
### **3-2. 평균 이동 군집화 알고리즘 적용 예제**


# **4. GMM(Gaussian Mixture Model)**

### **4-1. GMM 소개**
- 군집화를 적용하고자 하는 데이터가 여러 개의 가우시안 분포(정규 분포)를 가진 데이터 집합들이 섞여서 생성된 것이라는 가정 하에 군집화를 수행하는 방식
<img src="https://user-images.githubusercontent.com/98953721/200118558-50e47388-becd-4d62-826c-da6021e5ba05.png" width="500" height="200"/>
- 전체 데이터 세트는 서로 다른 정규 분포 형태를 가진 여러 가지 확률 분포 곡선으로 구성될 수 있고, 이러한 서로 다른 정규 분포에 기반해 군집화를 수행하는 방법
<img src="https://user-images.githubusercontent.com/98953721/200118959-6b16f24e-2604-4169-83e2-a819253603c6.png" width="400" height="600"/>
- 여러 개의 정규 분포 곡선 추출 -> 개별 데이터가 어느 정규 분포에 속할 지를 결정  
  => 확률 기반 군집화
- EM(Expectation and Maximization) 방법을 적용
- 사이킷런의 GaussianMixture 클래스 이용

### **4-2. GMM을 이용한 붓꽃 데이터 세트 군집화**
```Python
from sklearn.mixture import GaussianMixture
```
- fit(feature data), predict(feature data)을 통해 군집 rufwjd
- Parameters
  - n_components: gaussian mixture의 모델의 총 개수 -> 군집의 개수를 정하는 데 중요한 역할

### **4-3. GMM vs K-Means**
- K-Means: 개별 군집 내의 데이터가 원형으로 흩어져 있는 경우에 효과적 -> 길쭉한 방향으로 데이터가 밀접해 있을 경우 최적의 군집화가 어려움
- GMM:
  - K-Means보다 유연하게 다양한 데이터 세트에 잘 적용될 수 있음
  - 군집화를 위한 수행 시간이 오래 걸린다는 단점이 존재


# **5. DBSCAN(Density Based Spatisl Clustering of Applications with Noise)**

### **5-1. DBSCAN 개요**
- 특정 공간 내의 데이터 밀도 차이 기반 알고리즘
- 복잡한 기하학적 분포도를 가진 데이터 세트에 대한 군집화 가능
- Parameters
  - eps
    - 입실론 주변 영역
    - 개별 데이터를 중심으로 입실론 반경을 가지는 원형의 영역의 반경 결정
    - 일반적으로 1 이하의 값 설정
    - 값 증가: 반경 커짐 -> 포함하는 데이터의 수 증가 -> 노이즈 데이터 개수 감소
  - min_samples
    - 핵심 포인트가 되기 위해 입실론 주변 영역 내에 포함돼야 할 데이터의 최소 개수
    - 값 증가: 주어진 반경 내에서 더 많은 데이터를 포함시켜야 함 -> 노이즈 데이터 개수 증가
    
- 데이터 포인트 정의
  - 핵심 포인트(Core point): 주변 영역 내에 최소 데이터 개수 이상의 타 데이터를 가지고 있을 경우
  - 이웃 포인트(Neighbor Point): 주변 영역 내에 위치한 타 데이터
  - 경계 포인트(Border Point): 주변 영역 내에 최소 데이터 개수 이상의 이웃 포인트를 가지고 있지는 않지만 핵심 포인트를 이웃 포인트로 가지고 있는 경우
  - 잡음 포인트(Noise Point): 최소 데이터 개수 이상의 이웃 포인트를 가지고 있지 않으며, 핵심 포인트도 이웃 포인트로 가지고 있지 않는 경우/ 군집 레이블 상에서 -1로 표시

- 입실론 주변 영역의 최소 데이터 개수를 포함하는 밀도 기준을 충족시키는 데이터인 핵심 포인트를 연결하면서 군집화를 구성하는 방식
- DBSCAN 적용 시 특정 군집 개수로 군집을 강제하지 않는 것이 좋음 -> eps와 min_samples 파라미터 조정을 통해 최적의 군집을 찾는 게 중요
- 
### **5-2. DBSCAN 적용하기 - 붓꽃 데이터 세트**

### **5-3. DBSCAN 적용하기 - make_circles() 데이터 세트**











