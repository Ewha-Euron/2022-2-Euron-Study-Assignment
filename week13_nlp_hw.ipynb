{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“Œ  **ìƒë‹¨ì˜ 'íŒŒì¼-ë“œë¼ì´ë¸Œì— ì‚¬ë³¸ ì €ì¥' í•´ì„œ ë³¸ì¸ ë“œë¼ì´ë¸Œì— ì €ì¥ëœ ì‚¬ë³¸ ì´ìš©í•´ì„œ ì‹¤ìŠµ í•´ì£¼ì„¸ìš”!!**\n",
        "\n",
        "ğŸ“Œ week13 ë³µìŠµìŠµê³¼ì œëŠ” **NLG ì‹¤ìŠµ**ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ìœ„í‚¤ë…ìŠ¤ì˜ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ êµì¬ ì‹¤ìŠµ, ê´€ë ¨ ë¸”ë¡œê·¸ ë“±ì˜ ë¬¸ì„œ ìë£Œë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” ê³¼ì œì…ë‹ˆë‹¤. \n",
        "\n",
        "ğŸ“Œ ì•ˆë‚´ëœ ë§í¬ì— ë§ì¶”ì–´ **ì§ì ‘ ì½”ë“œë¥¼ ë”°ë¼ ì¹˜ë©´ì„œ (í•„ì‚¬)** í•´ë‹¹ nlp task ì˜ ê¸°ë³¸ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë©”ì„œë“œë¥¼ ìˆ™ì§€í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ğŸ˜Š í•„ìˆ˜ë¼ê³  ì²´í¬í•œ ë¶€ë¶„ì€ ê³¼ì œì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì£¼ì‹œê³ , ì„ íƒìœ¼ë¡œ ì²´í¬í•œ ë¶€ë¶„ì€ ììœ¨ì ìœ¼ë¡œ ìŠ¤í„°ë”” í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ê¶ê¸ˆí•œ ì‚¬í•­ì€ ê¹ƒí—ˆë¸Œ ì´ìŠˆë‚˜, ì¹´í†¡ë°©, ì„¸ì…˜ ë°œí‘œ ì‹œì‘ ì´ì „ ì‹œê°„ ë“±ì„ í™œìš©í•˜ì—¬ ììœ ë¡­ê²Œ ê³µìœ í•´ì£¼ì„¸ìš”!"
      ],
      "metadata": {
        "id": "BX3ac8Ag1RPC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppuTdZVWSo0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk colab í™˜ê²½ì—ì„œ ì‹¤í–‰ì‹œ í•„ìš”í•œ ì½”ë“œì…ë‹ˆë‹¤. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "4JEquLR91VBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ¥° **ì´í•˜ ì˜ˆì œë¥¼ ì‹¤ìŠµí•˜ì‹œë©´ ë©ë‹ˆë‹¤.**\n",
        "\n",
        "**1-(1)~(2)ëŠ” í•„ìˆ˜ê³¼ì œ, 2ëŠ” ì„ íƒê³¼ì œì…ë‹ˆë‹¤.**\n"
      ],
      "metadata": {
        "id": "Kq8aMYKGPQR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1ï¸âƒ£ NLG task ì‹¤ìŠµ**"
      ],
      "metadata": {
        "id": "SHTPAk95iNtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‘€ ë‚´ìš© ë³µìŠµ\n",
        "\n",
        "NLG ëŠ” ìƒˆë¡œìš´ text ë¥¼ ë§Œë“¤ì–´ ë‚´ëŠ” ëª¨ë“  task ë¥¼ ì˜ë¯¸í•˜ë©° ê¸°ê³„ë²ˆì—­, í…ìŠ¤íŠ¸ ìš”ì•½, ì±„íŒ…, ìŠ¤í† ë¦¬í…”ë§, QA ë“±ì´ ìˆë‹¤. "
      ],
      "metadata": {
        "id": "j5msd7Igjz9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 1-(1) RNN ì„ ì´ìš©í•œ text generation \n",
        "\n",
        "ğŸ“Œ [Text generation with RNN](https://wikidocs.net/45101) \n",
        "\n",
        "* Simple RNN ì„ ì´ìš©í•œ ê°„ë‹¨í•œ í•œêµ­ì–´ text generation ì˜ˆì œì™€ LSTM ì„ ì´ìš©í•œ ë‰´ìš• íƒ€ì„ì¦ˆ ê¸°ì‚¬ í—¤ë“œë¼ì¸ ìƒì„± ì˜ˆì œë¥¼ í•„ì‚¬í•´ì£¼ì„¸ìš”."
      ],
      "metadata": {
        "id": "9L-jAHPkiBV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "Klj6gjETZdgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"ê²½ë§ˆì¥ì— ìˆëŠ” ë§ì´ ë›°ê³  ìˆë‹¤\\n\n",
        "ê·¸ì˜ ë§ì´ ë²•ì´ë‹¤\\n\n",
        "ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ê³±ë‹¤\\n\"\"\""
      ],
      "metadata": {
        "id": "MsJZ9ldFiARK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° : %d' % vocab_size)"
      ],
      "metadata": {
        "id": "jQO_iIvfPC6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "wxP893NmPGDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = list()\n",
        "for line in text.split('\\n'): # ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ í† í°í™”\n",
        "    encoded = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "print('í•™ìŠµì— ì‚¬ìš©í•  ìƒ˜í”Œì˜ ê°œìˆ˜: %d' % len(sequences))"
      ],
      "metadata": {
        "id": "7q1_Hc_EPHUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences)"
      ],
      "metadata": {
        "id": "PpplzYsaPJjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(l) for l in sequences) # ëª¨ë“  ìƒ˜í”Œì—ì„œ ê¸¸ì´ê°€ ê°€ì¥ ê¸´ ìƒ˜í”Œì˜ ê¸¸ì´ ì¶œë ¥\n",
        "print('ìƒ˜í”Œì˜ ìµœëŒ€ ê¸¸ì´ : {}'.format(max_len))"
      ],
      "metadata": {
        "id": "ueIB5cWzPK0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')"
      ],
      "metadata": {
        "id": "po2a-JnHPMDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences)"
      ],
      "metadata": {
        "id": "68hyHqZwPNTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = np.array(sequences)\n",
        "X = sequences[:,:-1]\n",
        "y = sequences[:,-1]"
      ],
      "metadata": {
        "id": "G40HlANBPOrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "k6YQ7o-tPP2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "id": "eNZCbNNqPRBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "metadata": {
        "id": "Od88khI4PSLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "id": "eBC3GEVcPTd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN"
      ],
      "metadata": {
        "id": "Jj5oaNCkPkN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 10\n",
        "hidden_units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=200, verbose=2)"
      ],
      "metadata": {
        "id": "yxx3PLWOPuX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(model, tokenizer, current_word, n): # ëª¨ë¸, í† í¬ë‚˜ì´ì €, í˜„ì¬ ë‹¨ì–´, ë°˜ë³µí•  íšŸìˆ˜\n",
        "    init_word = current_word\n",
        "    sentence = ''\n",
        "\n",
        "    # në²ˆ ë°˜ë³µ\n",
        "    for _ in range(n):\n",
        "        # í˜„ì¬ ë‹¨ì–´ì— ëŒ€í•œ ì •ìˆ˜ ì¸ì½”ë”©ê³¼ íŒ¨ë”©\n",
        "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n",
        "        # ì…ë ¥í•œ X(í˜„ì¬ ë‹¨ì–´)ì— ëŒ€í•´ì„œ Yë¥¼ ì˜ˆì¸¡í•˜ê³  Y(ì˜ˆì¸¡í•œ ë‹¨ì–´)ë¥¼ resultì— ì €ì¥.\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "\n",
        "        for word, index in tokenizer.word_index.items(): \n",
        "            # ë§Œì•½ ì˜ˆì¸¡í•œ ë‹¨ì–´ì™€ ì¸ë±ìŠ¤ì™€ ë™ì¼í•œ ë‹¨ì–´ê°€ ìˆë‹¤ë©´ break\n",
        "            if index == result:\n",
        "                break\n",
        "\n",
        "        # í˜„ì¬ ë‹¨ì–´ + ' ' + ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ í˜„ì¬ ë‹¨ì–´ë¡œ ë³€ê²½\n",
        "        current_word = current_word + ' '  + word\n",
        "\n",
        "        # ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ë¬¸ì¥ì— ì €ì¥\n",
        "        sentence = sentence + ' ' + word\n",
        "\n",
        "    sentence = init_word + sentence\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "kq4dGDYlPwQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model, tokenizer, 'ê²½ë§ˆì¥ì—', 4))"
      ],
      "metadata": {
        "id": "WaSnaDDNPyYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "qbWsIBHlP4FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('ArticlesApril2018.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "IOmeQQRuP412"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ì—´ì˜ ê°œìˆ˜: ',len(df.columns))\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "RpOviwfPP641"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['headline'].isnull().values.any())"
      ],
      "metadata": {
        "id": "216kGFyTP8ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headline = []\n",
        "# í—¤ë“œë¼ì¸ì˜ ê°’ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
        "headline.extend(list(df.headline.values)) \n",
        "headline[:5]"
      ],
      "metadata": {
        "id": "picD2qN-P9nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ì´ ìƒ˜í”Œì˜ ê°œìˆ˜ : {}'.format(len(headline)))"
      ],
      "metadata": {
        "id": "MJ7_NbgoP-1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headline = [word for word in headline if word != \"Unknown\"]\n",
        "print('ë…¸ì´ì¦ˆê°’ ì œê±° í›„ ìƒ˜í”Œì˜ ê°œìˆ˜ : {}'.format(len(headline)))"
      ],
      "metadata": {
        "id": "UGSQdF2CP_zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headline[:5]"
      ],
      "metadata": {
        "id": "iuU7Oc2QQBVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def repreprocessing(raw_sentence):\n",
        "    preproceseed_sentence = raw_sentence.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    # êµ¬ë‘ì  ì œê±°ì™€ ë™ì‹œì— ì†Œë¬¸ìí™”\n",
        "    return ''.join(word for word in preproceseed_sentence if word not in punctuation).lower()\n",
        "\n",
        "preprocessed_headline = [repreprocessing(x) for x in headline]\n",
        "preprocessed_headline[:5]"
      ],
      "metadata": {
        "id": "mnykztB5QEej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_headline)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° : %d' % vocab_size)"
      ],
      "metadata": {
        "id": "wl8GxtoFQF4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = list()\n",
        "\n",
        "for sentence in preprocessed_headline:\n",
        "\n",
        "    # ê° ìƒ˜í”Œì— ëŒ€í•œ ì •ìˆ˜ ì¸ì½”ë”©\n",
        "    encoded = tokenizer.texts_to_sequences([sentence])[0] \n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "sequences[:11]"
      ],
      "metadata": {
        "id": "JMlEG078QHOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = {}\n",
        "for key, value in tokenizer.word_index.items(): # ì¸ë±ìŠ¤ë¥¼ ë‹¨ì–´ë¡œ ë°”ê¾¸ê¸° ìœ„í•´ index_to_wordë¥¼ ìƒì„±\n",
        "    index_to_word[value] = key\n",
        "\n",
        "print('ë¹ˆë„ìˆ˜ ìƒìœ„ 582ë²ˆ ë‹¨ì–´ : {}'.format(index_to_word[582]))"
      ],
      "metadata": {
        "id": "WgFLeO_7QIlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(l) for l in sequences)\n",
        "print('ìƒ˜í”Œì˜ ìµœëŒ€ ê¸¸ì´ : {}'.format(max_len))"
      ],
      "metadata": {
        "id": "QlyvV96jQJ88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "print(sequences[:3])"
      ],
      "metadata": {
        "id": "wVRK_88bQLFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = np.array(sequences)\n",
        "X = sequences[:,:-1]\n",
        "y = sequences[:,-1]\n",
        "print(X[:3])"
      ],
      "metadata": {
        "id": "cOszKyKdQNlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y[:3])"
      ],
      "metadata": {
        "id": "CG_wRh3MQPWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "metadata": {
        "id": "zyOSjzdiQaHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM"
      ],
      "metadata": {
        "id": "-Ex4Qn_1QdEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 10\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=200, verbose=2)"
      ],
      "metadata": {
        "id": "Q-HmleaZQeGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(model, tokenizer, current_word, n): # ëª¨ë¸, í† í¬ë‚˜ì´ì €, í˜„ì¬ ë‹¨ì–´, ë°˜ë³µí•  íšŸìˆ˜\n",
        "    init_word = current_word\n",
        "    sentence = ''\n",
        "\n",
        "    # në²ˆ ë°˜ë³µ\n",
        "    for _ in range(n):\n",
        "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=max_len-1, padding='pre')\n",
        "\n",
        "        # ì…ë ¥í•œ X(í˜„ì¬ ë‹¨ì–´)ì— ëŒ€í•´ì„œ yë¥¼ ì˜ˆì¸¡í•˜ê³  y(ì˜ˆì¸¡í•œ ë‹¨ì–´)ë¥¼ resultì— ì €ì¥.\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "\n",
        "        for word, index in tokenizer.word_index.items(): \n",
        "            # ë§Œì•½ ì˜ˆì¸¡í•œ ë‹¨ì–´ì™€ ì¸ë±ìŠ¤ì™€ ë™ì¼í•œ ë‹¨ì–´ê°€ ìˆë‹¤ë©´\n",
        "            if index == result:\n",
        "                break\n",
        "\n",
        "        # í˜„ì¬ ë‹¨ì–´ + ' ' + ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ í˜„ì¬ ë‹¨ì–´ë¡œ ë³€ê²½\n",
        "        current_word = current_word + ' '  + word\n",
        "\n",
        "        # ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ë¬¸ì¥ì— ì €ì¥\n",
        "        sentence = sentence + ' ' + word\n",
        "\n",
        "    sentence = init_word + sentence\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "rklzt3iGQfd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model, tokenizer, 'i', 10))"
      ],
      "metadata": {
        "id": "6EiXAy6-QgnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model, tokenizer, 'how', 10))"
      ],
      "metadata": {
        "id": "892wzDYoQh0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 1-(2) Text summarization with attention\n",
        "\n",
        "ğŸ“Œ [ì•„ë§ˆì¡´ ë¦¬ë·° ìš”ì•½](https://wikidocs.net/72820) \n",
        "\n",
        "* seq2seq + attention ì„ ì´ìš©í•œ ì•„ë§ˆì¡´ ë¦¬ë·° ê¸€ text summarization ì˜ˆì œë¥¼ í•„ì‚¬í•´ì£¼ì„¸ìš”."
      ],
      "metadata": {
        "id": "ktAuag_Nk5Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import urllib.request\n",
        "np.random.seed(seed=0)"
      ],
      "metadata": {
        "id": "mLmPLEYdZlzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reviews.csv íŒŒì¼ì„ dataë¼ëŠ” ì´ë¦„ì˜ ë°ì´í„°í”„ë ˆì„ì— ì €ì¥. ë‹¨, 10ë§Œê°œì˜ í–‰(rows)ìœ¼ë¡œ ì œí•œ.\n",
        "data = pd.read_csv(\"Reviews.csv íŒŒì¼ì˜ ê²½ë¡œ\", nrows = 100000)\n",
        "print('ì „ì²´ ë¦¬ë·° ê°œìˆ˜ :',(len(data)))"
      ],
      "metadata": {
        "id": "xcgptSxJ3zhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "wR3x8tRRQxy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[['Text','Summary']]\n",
        "data.head()"
      ],
      "metadata": {
        "id": "MycRc4IBve2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëœë¤ìœ¼ë¡œ 10ê°œì˜ ìƒ˜í”Œ ì¶œë ¥\n",
        "data.sample(10)"
      ],
      "metadata": {
        "id": "I7o1jgBEvgyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Text ì—´ì—ì„œ ì¤‘ë³µì„ ë°°ì œí•œ ìœ ì¼í•œ ìƒ˜í”Œì˜ ìˆ˜ :', data['Text'].nunique())\n",
        "print('Summary ì—´ì—ì„œ ì¤‘ë³µì„ ë°°ì œí•œ ìœ ì¼í•œ ìƒ˜í”Œì˜ ìˆ˜ :', data['Summary'].nunique())"
      ],
      "metadata": {
        "id": "ePe3wrrwvhlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text ì—´ì—ì„œ ì¤‘ë³µì¸ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì¤‘ë³µ ì œê±°\n",
        "data.drop_duplicates(subset=['Text'], inplace=True)\n",
        "print(\"ì „ì²´ ìƒ˜í”Œìˆ˜ :\", len(data))"
      ],
      "metadata": {
        "id": "7jHfkfAvviwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "QpTcoNtQvkGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Null ê°’ì„ ê°€ì§„ ìƒ˜í”Œ ì œê±°\n",
        "data.dropna(axis=0, inplace=True)\n",
        "print('ì „ì²´ ìƒ˜í”Œìˆ˜ :',(len(data)))"
      ],
      "metadata": {
        "id": "AsW2cArWvla9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì „ì²˜ë¦¬ í•¨ìˆ˜ ë‚´ ì‚¬ìš©\n",
        "contractions = {\"'cause\": 'because',\n",
        " \"I'd\": 'I would',\n",
        " \"I'd've\": 'I would have',\n",
        " \"I'll\": 'I will',\n",
        " \"I'll've\": 'I will have',\n",
        " \"I'm\": 'I am',\n",
        " \"I've\": 'I have',\n",
        " \"ain't\": 'is not',\n",
        " \"aren't\": 'are not',\n",
        " \"can't\": 'cannot',\n",
        " \"could've\": 'could have',\n",
        " \"couldn't\": 'could not',\n",
        " \"didn't\": 'did not',\n",
        " \"doesn't\": 'does not',\n",
        " \"don't\": 'do not',\n",
        " \"hadn't\": 'had not',\n",
        " \"hasn't\": 'has not',\n",
        " \"haven't\": 'have not',\n",
        " \"he'd\": 'he would',\n",
        " \"he'll\": 'he will',\n",
        " \"he's\": 'he is',\n",
        " \"here's\": 'here is',\n",
        " \"how'd\": 'how did',\n",
        " \"how'd'y\": 'how do you',\n",
        " \"how'll\": 'how will',\n",
        " \"how's\": 'how is',\n",
        " \"i'd\": 'i would',\n",
        " \"i'd've\": 'i would have',\n",
        " \"i'll\": 'i will',\n",
        " \"i'll've\": 'i will have',\n",
        " \"i'm\": 'i am',\n",
        " \"i've\": 'i have',\n",
        " \"isn't\": 'is not',\n",
        " \"it'd\": 'it would',\n",
        " \"it'd've\": 'it would have',\n",
        " \"it'll\": 'it will',\n",
        " \"it'll've\": 'it will have',\n",
        " \"it's\": 'it is',\n",
        " \"let's\": 'let us',\n",
        " \"ma'am\": 'madam',\n",
        " \"mayn't\": 'may not',\n",
        " \"might've\": 'might have',\n",
        " \"mightn't\": 'might not',\n",
        " \"mightn't've\": 'might not have',\n",
        " \"must've\": 'must have',\n",
        " \"mustn't\": 'must not',\n",
        " \"mustn't've\": 'must not have',\n",
        " \"needn't\": 'need not',\n",
        " \"needn't've\": 'need not have',\n",
        " \"o'clock\": 'of the clock',\n",
        " \"oughtn't\": 'ought not',\n",
        " \"oughtn't've\": 'ought not have',\n",
        " \"sha'n't\": 'shall not',\n",
        " \"shan't\": 'shall not',\n",
        " \"shan't've\": 'shall not have',\n",
        " \"she'd\": 'she would',\n",
        " \"she'd've\": 'she would have',\n",
        " \"she'll\": 'she will',\n",
        " \"she'll've\": 'she will have',\n",
        " \"she's\": 'she is',\n",
        " \"should've\": 'should have',\n",
        " \"shouldn't\": 'should not',\n",
        " \"shouldn't've\": 'should not have',\n",
        " \"so's\": 'so as',\n",
        " \"so've\": 'so have',\n",
        " \"that'd\": 'that would',\n",
        " \"that'd've\": 'that would have',\n",
        " \"that's\": 'that is',\n",
        " \"there'd\": 'there would',\n",
        " \"there'd've\": 'there would have',\n",
        " \"there's\": 'there is',\n",
        " \"they'd\": 'they would',\n",
        " \"they'd've\": 'they would have',\n",
        " \"they'll\": 'they will',\n",
        " \"they'll've\": 'they will have',\n",
        " \"they're\": 'they are',\n",
        " \"they've\": 'they have',\n",
        " \"this's\": 'this is',\n",
        " \"to've\": 'to have',\n",
        " \"wasn't\": 'was not',\n",
        " \"we'd\": 'we would',\n",
        " \"we'd've\": 'we would have',\n",
        " \"we'll\": 'we will',\n",
        " \"we'll've\": 'we will have',\n",
        " \"we're\": 'we are',\n",
        " \"we've\": 'we have',\n",
        " \"weren't\": 'were not',\n",
        " \"what'll\": 'what will',\n",
        " \"what'll've\": 'what will have',\n",
        " \"what're\": 'what are',\n",
        " \"what's\": 'what is',\n",
        " \"what've\": 'what have',\n",
        " \"when's\": 'when is',\n",
        " \"when've\": 'when have',\n",
        " \"where'd\": 'where did',\n",
        " \"where's\": 'where is',\n",
        " \"where've\": 'where have',\n",
        " \"who'll\": 'who will',\n",
        " \"who'll've\": 'who will have',\n",
        " \"who's\": 'who is',\n",
        " \"who've\": 'who have',\n",
        " \"why's\": 'why is',\n",
        " \"why've\": 'why have',\n",
        " \"will've\": 'will have',\n",
        " \"won't\": 'will not',\n",
        " \"won't've\": 'will not have',\n",
        " \"would've\": 'would have',\n",
        " \"wouldn't\": 'would not',\n",
        " \"wouldn't've\": 'would not have',\n",
        " \"y'all\": 'you all',\n",
        " \"y'all'd\": 'you all would',\n",
        " \"y'all'd've\": 'you all would have',\n",
        " \"y'all're\": 'you all are',\n",
        " \"y'all've\": 'you all have',\n",
        " \"you'd\": 'you would',\n",
        " \"you'd've\": 'you would have',\n",
        " \"you'll\": 'you will',\n",
        " \"you'll've\": 'you will have',\n",
        " \"you're\": 'you are',\n",
        " \"you've\": 'you have'}"
      ],
      "metadata": {
        "id": "TItmtndUvpeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTKì˜ ë¶ˆìš©ì–´\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print('ë¶ˆìš©ì–´ ê°œìˆ˜ :', len(stop_words))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "qpkzRXDMvsFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ë¶ˆìš©ì–´ ê°œìˆ˜ : 179\n",
        "{'this', \"doesn't\", 'until', 'as', ... ì¤‘ëµ ... ,'whom', 'here', 'ma', \"it's\", 'am', 'your'}"
      ],
      "metadata": {
        "id": "tf2-fSusvtUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "def preprocess_sentence(sentence, remove_stopwords = True):\n",
        "    sentence = sentence.lower() # í…ìŠ¤íŠ¸ ì†Œë¬¸ìí™”\n",
        "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> ë“±ì˜ html íƒœê·¸ ì œê±°\n",
        "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # ê´„í˜¸ë¡œ ë‹«íŒ ë¬¸ìì—´  ì œê±° Ex) my husband (and myself) for => my husband for\n",
        "    sentence = re.sub('\"','', sentence) # ìŒë”°ì˜´í‘œ \" ì œê±°\n",
        "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # ì•½ì–´ ì •ê·œí™”\n",
        "    sentence = re.sub(r\"'s\\b\",\"\",sentence) # ì†Œìœ ê²© ì œê±°. Ex) roland's -> roland\n",
        "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # ì˜ì–´ ì™¸ ë¬¸ì(ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ë“±) ê³µë°±ìœ¼ë¡œ ë³€í™˜\n",
        "    sentence = re.sub('[m]{2,}', 'mm', sentence) # mì´ 3ê°œ ì´ìƒì´ë©´ 2ê°œë¡œ ë³€ê²½. Ex) ummmmmmm yeah -> umm yeah\n",
        "\n",
        "    # ë¶ˆìš©ì–´ ì œê±° (Text)\n",
        "    if remove_stopwords:\n",
        "        tokens = ' '.join(word for word in sentence.split() if not word in stop_words if len(word) > 1)\n",
        "    # ë¶ˆìš©ì–´ ë¯¸ì œê±° (Summary)\n",
        "    else:\n",
        "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "M3QdpmX6vueu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
        "temp_summary = 'Great way to start (or finish) the day!!!'\n",
        "print(preprocess_sentence(temp_text))\n",
        "print(preprocess_sentence(temp_summary, 0))"
      ],
      "metadata": {
        "id": "l8tVlpQRvwem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text ì—´ ì „ì²˜ë¦¬\n",
        "clean_text = []\n",
        "for s in data['Text']:\n",
        "    clean_text.append(preprocess_sentence(s))\n",
        "clean_text[:5]"
      ],
      "metadata": {
        "id": "MWrx60aHvyDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary ì—´ ì „ì²˜ë¦¬\n",
        "clean_summary = []\n",
        "for s in data['Summary']:\n",
        "    clean_summary.append(preprocess_sentence(s, 0))\n",
        "clean_summary[:5]"
      ],
      "metadata": {
        "id": "uILlIf6evzjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Text'] = clean_text\n",
        "data['Summary'] = clean_summary"
      ],
      "metadata": {
        "id": "Yu4JN1X7v0tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¸¸ì´ê°€ ê³µë°±ì¸ ìƒ˜í”Œì€ NULL ê°’ìœ¼ë¡œ ë³€í™˜\n",
        "data.replace('', np.nan, inplace=True)\n",
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "81r0sW8Mv1wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(axis = 0, inplace = True)\n",
        "print('ì „ì²´ ìƒ˜í”Œìˆ˜ :',(len(data)))"
      ],
      "metadata": {
        "id": "sGPAP_znv2x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¸¸ì´ ë¶„í¬ ì¶œë ¥\n",
        "text_len = [len(s.split()) for s in data['Text']]\n",
        "summary_len = [len(s.split()) for s in data['Summary']]\n",
        "\n",
        "print('í…ìŠ¤íŠ¸ì˜ ìµœì†Œ ê¸¸ì´ : {}'.format(np.min(text_len)))\n",
        "print('í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´ : {}'.format(np.max(text_len)))\n",
        "print('í…ìŠ¤íŠ¸ì˜ í‰ê·  ê¸¸ì´ : {}'.format(np.mean(text_len)))\n",
        "print('ìš”ì•½ì˜ ìµœì†Œ ê¸¸ì´ : {}'.format(np.min(summary_len)))\n",
        "print('ìš”ì•½ì˜ ìµœëŒ€ ê¸¸ì´ : {}'.format(np.max(summary_len)))\n",
        "print('ìš”ì•½ì˜ í‰ê·  ê¸¸ì´ : {}'.format(np.mean(summary_len)))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.boxplot(summary_len)\n",
        "plt.title('Summary')\n",
        "plt.subplot(1,2,2)\n",
        "plt.boxplot(text_len)\n",
        "plt.title('Text')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.title('Summary')\n",
        "plt.hist(summary_len, bins=40)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()\n",
        "\n",
        "plt.title('Text')\n",
        "plt.hist(text_len, bins=40)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BOhLrX7Nv302"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_max_len = 50\n",
        "summary_max_len = 8"
      ],
      "metadata": {
        "id": "ypwxauF_v5b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if(len(s.split()) <= max_len):\n",
        "        cnt = cnt + 1\n",
        "  print('ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ %s ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: %s'%(max_len, (cnt / len(nested_list))))"
      ],
      "metadata": {
        "id": "DpGO5qNBv86b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "below_threshold_len(text_max_len, data['Text'])"
      ],
      "metadata": {
        "id": "oLDvjJNmv92S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
        "data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
        "print('ì „ì²´ ìƒ˜í”Œìˆ˜ :',(len(data)))"
      ],
      "metadata": {
        "id": "43y1L3XGGUtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "saz_V9eAGVwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ìš”ì•½ ë°ì´í„°ì—ëŠ” ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€í•œë‹¤.\n",
        "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
        "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "OVP_OfVsGWuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = np.array(data['Text'])\n",
        "decoder_input = np.array(data['decoder_input'])\n",
        "decoder_target = np.array(data['decoder_target'])"
      ],
      "metadata": {
        "id": "l713cvJLGX5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print(indices)"
      ],
      "metadata": {
        "id": "15C0PqN9Gx8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]"
      ],
      "metadata": {
        "id": "2awz9PbQGy7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_of_val = int(len(encoder_input)*0.2)\n",
        "print('í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ìˆ˜ :',n_of_val)"
      ],
      "metadata": {
        "id": "6MFzRrHsG0AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ],
      "metadata": {
        "id": "7MleqYZZG1t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('í›ˆë ¨ ë°ì´í„°ì˜ ê°œìˆ˜ :', len(encoder_input_train))\n",
        "print('í›ˆë ¨ ë ˆì´ë¸”ì˜ ê°œìˆ˜ :',len(decoder_input_train))\n",
        "print('í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ê°œìˆ˜ :',len(encoder_input_test))\n",
        "print('í…ŒìŠ¤íŠ¸ ë ˆì´ë¸”ì˜ ê°œìˆ˜ :',len(decoder_input_test))"
      ],
      "metadata": {
        "id": "sMSzvIgRG3XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_tokenizer = Tokenizer()\n",
        "src_tokenizer.fit_on_texts(encoder_input_train)"
      ],
      "metadata": {
        "id": "rjPblcDIG4tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 7\n",
        "total_cnt = len(src_tokenizer.word_index) # ë‹¨ì–´ì˜ ìˆ˜\n",
        "rare_cnt = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸\n",
        "total_freq = 0 # í›ˆë ¨ ë°ì´í„°ì˜ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì´ í•©\n",
        "rare_freq = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ì˜ ì´ í•©\n",
        "\n",
        "# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ì˜ ìŒ(pair)ì„ keyì™€ valueë¡œ ë°›ëŠ”ë‹¤.\n",
        "for key, value in src_tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ìœ¼ë©´\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° :',total_cnt)\n",
        "print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))\n",
        "print('ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ë¥¼ ì œì™¸ì‹œí‚¬ ê²½ìš°ì˜ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° %s'%(total_cnt - rare_cnt))\n",
        "print(\"ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:\", (rare_freq / total_freq)*100)"
      ],
      "metadata": {
        "id": "4cPUttsIG6DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = 8000\n",
        "src_tokenizer = Tokenizer(num_words = src_vocab) \n",
        "src_tokenizer.fit_on_texts(encoder_input_train)\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
        "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)"
      ],
      "metadata": {
        "id": "YQxZKgb2G7kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder_input_train[:3])"
      ],
      "metadata": {
        "id": "MRdx2MNmG8Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tar_tokenizer = Tokenizer()\n",
        "tar_tokenizer.fit_on_texts(decoder_input_train)"
      ],
      "metadata": {
        "id": "0YddkHl4G9PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 6\n",
        "total_cnt = len(tar_tokenizer.word_index) # ë‹¨ì–´ì˜ ìˆ˜\n",
        "rare_cnt = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸\n",
        "total_freq = 0 # í›ˆë ¨ ë°ì´í„°ì˜ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì´ í•©\n",
        "rare_freq = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ì˜ ì´ í•©\n",
        "\n",
        "# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ì˜ ìŒ(pair)ì„ keyì™€ valueë¡œ ë°›ëŠ”ë‹¤.\n",
        "for key, value in tar_tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ìœ¼ë©´\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° :',total_cnt)\n",
        "print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))\n",
        "print('ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ë¥¼ ì œì™¸ì‹œí‚¬ ê²½ìš°ì˜ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° %s'%(total_cnt - rare_cnt))\n",
        "print(\"ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:\", (rare_freq / total_freq)*100)"
      ],
      "metadata": {
        "id": "gukcy7EYG-if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tar_vocab = 2000\n",
        "tar_tokenizer = Tokenizer(num_words = tar_vocab) \n",
        "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
        "tar_tokenizer.fit_on_texts(decoder_target_train)"
      ],
      "metadata": {
        "id": "6IZZ2DvXHALj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoder_input_train[:5])"
      ],
      "metadata": {
        "id": "FReL6jDzHCNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoder_target_train[:5])"
      ],
      "metadata": {
        "id": "TJpEoE93HDRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
        "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]"
      ],
      "metadata": {
        "id": "7_imyIwuHEeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ì‚­ì œí•  í›ˆë ¨ ë°ì´í„°ì˜ ê°œìˆ˜ :',len(drop_train))\n",
        "print('ì‚­ì œí•  í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ê°œìˆ˜ :',len(drop_test))"
      ],
      "metadata": {
        "id": "OvZ96X08HFB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
        "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
        "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
        "\n",
        "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
        "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
        "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
        "\n",
        "print('í›ˆë ¨ ë°ì´í„°ì˜ ê°œìˆ˜ :', len(encoder_input_train))\n",
        "print('í›ˆë ¨ ë ˆì´ë¸”ì˜ ê°œìˆ˜ :',len(decoder_input_train))\n",
        "print('í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ê°œìˆ˜ :',len(encoder_input_test))\n",
        "print('í…ŒìŠ¤íŠ¸ ë ˆì´ë¸”ì˜ ê°œìˆ˜ :',len(decoder_input_test))"
      ],
      "metadata": {
        "id": "oKD5tew8HI3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = pad_sequences(encoder_input_train, maxlen = text_max_len, padding='post')\n",
        "encoder_input_test = pad_sequences(encoder_input_test, maxlen = text_max_len, padding='post')\n",
        "decoder_input_train = pad_sequences(decoder_input_train, maxlen = summary_max_len, padding='post')\n",
        "decoder_target_train = pad_sequences(decoder_target_train, maxlen = summary_max_len, padding='post')\n",
        "decoder_input_test = pad_sequences(decoder_input_test, maxlen = summary_max_len, padding='post')\n",
        "decoder_target_test = pad_sequences(decoder_target_test, maxlen = summary_max_len, padding='post')"
      ],
      "metadata": {
        "id": "36ki-SX-HKCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "MbzteetcHKgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "hidden_size = 256\n",
        "\n",
        "# ì¸ì½”ë”\n",
        "encoder_inputs = Input(shape=(text_max_len,))\n",
        "\n",
        "# ì¸ì½”ë”ì˜ ì„ë² ë”© ì¸µ\n",
        "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
        "\n",
        "# ì¸ì½”ë”ì˜ LSTM 1\n",
        "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "# ì¸ì½”ë”ì˜ LSTM 2\n",
        "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# ì¸ì½”ë”ì˜ LSTM 3\n",
        "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
      ],
      "metadata": {
        "id": "6mTCwMufHRm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë””ì½”ë”\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# ë””ì½”ë”ì˜ ì„ë² ë”© ì¸µ\n",
        "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# ë””ì½”ë”ì˜ LSTM\n",
        "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])"
      ],
      "metadata": {
        "id": "AfFkBgZdHTWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë””ì½”ë”ì˜ ì¶œë ¥ì¸µ\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation = 'softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
        "\n",
        "# ëª¨ë¸ ì •ì˜\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "NqHVJffmHUr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/20.%20Text%20Summarization/attention.py\", filename=\"attention.py\")\n",
        "from attention import AttentionLayer"
      ],
      "metadata": {
        "id": "PRfgP7_aHWCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì–´í…ì…˜ ì¸µ(ì–´í…ì…˜ í•¨ìˆ˜)\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# ì–´í…ì…˜ì˜ ê²°ê³¼ì™€ ë””ì½”ë”ì˜ hidden stateë“¤ì„ ì—°ê²°\n",
        "decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# ë””ì½”ë”ì˜ ì¶œë ¥ì¸µ\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
        "\n",
        "# ëª¨ë¸ ì •ì˜\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qADafD2gHX1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "xqVxWsNTHYpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
        "history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
        "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size = 256, callbacks=[es], epochs = 50)"
      ],
      "metadata": {
        "id": "wI1XZ4U-HZwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ho-yuub3Hbt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_index_to_word = src_tokenizer.index_word # ì›ë¬¸ ë‹¨ì–´ ì§‘í•©ì—ì„œ ì •ìˆ˜ -> ë‹¨ì–´ë¥¼ ì–»ìŒ\n",
        "tar_word_to_index = tar_tokenizer.word_index # ìš”ì•½ ë‹¨ì–´ ì§‘í•©ì—ì„œ ë‹¨ì–´ -> ì •ìˆ˜ë¥¼ ì–»ìŒ\n",
        "tar_index_to_word = tar_tokenizer.index_word # ìš”ì•½ ë‹¨ì–´ ì§‘í•©ì—ì„œ ì •ìˆ˜ -> ë‹¨ì–´ë¥¼ ì–»ìŒ"
      ],
      "metadata": {
        "id": "wUcDtcmlHdP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì¸ì½”ë” ì„¤ê³„\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])"
      ],
      "metadata": {
        "id": "OtiWeSTdHecm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ì „ ì‹œì ì˜ ìƒíƒœë“¤ì„ ì €ì¥í•˜ëŠ” í…ì„œ\n",
        "decoder_state_input_h = Input(shape=(hidden_size,))\n",
        "decoder_state_input_c = Input(shape=(hidden_size,))\n",
        "\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "# ë¬¸ì¥ì˜ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ ì´ˆê¸° ìƒíƒœ(initial_state)ë¥¼ ì´ì „ ì‹œì ì˜ ìƒíƒœë¡œ ì‚¬ìš©. ì´ëŠ” ë’¤ì˜ í•¨ìˆ˜ decode_sequence()ì— êµ¬í˜„\n",
        "# í›ˆë ¨ ê³¼ì •ì—ì„œì™€ ë‹¬ë¦¬ LSTMì˜ ë¦¬í„´í•˜ëŠ” ì€ë‹‰ ìƒíƒœì™€ ì…€ ìƒíƒœì¸ state_hì™€ state_cë¥¼ ë²„ë¦¬ì§€ ì•ŠìŒ.\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
      ],
      "metadata": {
        "id": "YDERubWMHf95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì–´í…ì…˜ í•¨ìˆ˜\n",
        "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# ë””ì½”ë”ì˜ ì¶œë ¥ì¸µ\n",
        "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
        "\n",
        "# ìµœì¢… ë””ì½”ë” ëª¨ë¸\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "metadata": {
        "id": "UfskHJvAHhdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # ì…ë ¥ìœ¼ë¡œë¶€í„° ì¸ì½”ë”ì˜ ìƒíƒœë¥¼ ì–»ìŒ\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "     # <SOS>ì— í•´ë‹¹í•˜ëŠ” í† í° ìƒì„±\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition: # stop_conditionì´ Trueê°€ ë  ë•Œê¹Œì§€ ë£¨í”„ ë°˜ë³µ\n",
        "\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = tar_index_to_word[sampled_token_index]\n",
        "\n",
        "        if(sampled_token!='eostoken'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        #  <eos>ì— ë„ë‹¬í•˜ê±°ë‚˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ë„˜ìœ¼ë©´ ì¤‘ë‹¨.\n",
        "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # ê¸¸ì´ê°€ 1ì¸ íƒ€ê²Ÿ ì‹œí€€ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸ í•©ë‹ˆë‹¤.\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "OkDBZNvKHjfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì›ë¬¸ì˜ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "def seq2text(input_seq):\n",
        "    sentence=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            sentence = sentence + src_index_to_word[i]+' '\n",
        "    return sentence\n",
        "\n",
        "# ìš”ì•½ë¬¸ì˜ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "def seq2summary(input_seq):\n",
        "    sentence=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
        "            sentence = sentence + tar_index_to_word[i] + ' '\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "JnGJ5hZ-HlL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(500, 1000):\n",
        "    print(\"ì›ë¬¸ : \",seq2text(encoder_input_test[i]))\n",
        "    print(\"ì‹¤ì œ ìš”ì•½ë¬¸ :\",seq2summary(decoder_input_test[i]))\n",
        "    print(\"ì˜ˆì¸¡ ìš”ì•½ë¬¸ :\",decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "DoopvigpHlrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2ï¸âƒ£ Text summarization task**"
      ],
      "metadata": {
        "id": "HfTr_BPwGc8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 2-(1) Pororo - text summarization \n",
        "\n",
        "ğŸ“Œ [ê³µì‹ë¬¸ì„œ](https://kakaobrain.github.io/pororo/seq2seq/summary.html) \n",
        "\n",
        "ğŸ“Œ [ì˜ˆì œ ì‹¤ìŠµ](https://teddylee777.github.io/machine-learning/nlp-korean-pororo) \n",
        "\n",
        "* PORORO : ì¹´ì¹´ì˜¤ ë¸Œë ˆì¸ì—ì„œ ì œê³µí•œ ìì—°ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬"
      ],
      "metadata": {
        "id": "eu3adOV2bDs_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e2WbVbbsatD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NmVEEfAqdSa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 2-(2) BERT ë¥¼ ì´ìš©í•œ text summarization \n",
        "\n",
        "ğŸ“Œ [ë…¼ë¬¸ ë¦¬ë·°](https://medium.com/@eyfydsyd97/bert%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%9A%94%EC%95%BD-text-summary-b582b5cc7d) \n",
        "\n",
        "ğŸ“Œ [BERT Extractive summarizer Library](https://github.com/dmmiller612/bert-extractive-summarizer) \n",
        "\n",
        "\n",
        "ğŸ“Œ [Text summarization Github Repo](https://github.com/uoneway/Text-Summarization-Repo) \n",
        "\n",
        "\n",
        "\n",
        "â• [BERT ë¥¼ ì´ìš©í•œ ë‰´ìŠ¤ ìš”ì•½ ìë™í™” App êµ¬í˜„ Repo](https://github.com/huydang90/News-Summarization-with-BERT) ğŸ‘‰ í”„ë¡œì íŠ¸ ì˜ˆì‹œ ì°¸ê³  ìë£Œ\n",
        "\n"
      ],
      "metadata": {
        "id": "xsx6OgzwbnIt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "svtikbdZatBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mnfL6KIZdSuo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}