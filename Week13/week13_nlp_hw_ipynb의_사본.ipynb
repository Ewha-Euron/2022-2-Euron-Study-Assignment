{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "📌  **상단의 '파일-드라이브에 사본 저장' 해서 본인 드라이브에 저장된 사본 이용해서 실습 해주세요!!**\n",
        "\n",
        "📌 week13 복습습과제는 **NLG 실습**으로 구성되어 있습니다.\n",
        "\n",
        "📌 위키독스의 딥러닝을 이용한 자연어 처리 입문 교재 실습, 관련 블로그 등의 문서 자료로 구성되어 있는 과제입니다. \n",
        "\n",
        "📌 안내된 링크에 맞추어 **직접 코드를 따라 치면서 (필사)** 해당 nlp task 의 기본적인 라이브러리와 메서드를 숙지해보시면 좋을 것 같습니다😊 필수라고 체크한 부분은 과제에 반드시 포함시켜주시고, 선택으로 체크한 부분은 자율적으로 스터디 하시면 됩니다.\n",
        "\n",
        "📌 궁금한 사항은 깃허브 이슈나, 카톡방, 세션 발표 시작 이전 시간 등을 활용하여 자유롭게 공유해주세요!"
      ],
      "metadata": {
        "id": "BX3ac8Ag1RPC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppuTdZVWSo0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk colab 환경에서 실행시 필요한 코드입니다. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "4JEquLR91VBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c234f6-41d9-49f6-cfc9-20fb18732a9d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🥰 **이하 예제를 실습하시면 됩니다.**\n",
        "\n",
        "**1-(1)~(2)는 필수과제, 2는 선택과제입니다.**\n"
      ],
      "metadata": {
        "id": "Kq8aMYKGPQR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1️⃣ NLG task 실습**"
      ],
      "metadata": {
        "id": "SHTPAk95iNtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👀 내용 복습\n",
        "\n",
        "NLG 는 새로운 text 를 만들어 내는 모든 task 를 의미하며 기계번역, 텍스트 요약, 채팅, 스토리텔링, QA 등이 있다. "
      ],
      "metadata": {
        "id": "j5msd7Igjz9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 1-(1) RNN 을 이용한 text generation \n",
        "\n",
        "📌 [Text generation with RNN](https://wikidocs.net/45101) \n",
        "\n",
        "* Simple RNN 을 이용한 간단한 한국어 text generation 예제와 LSTM 을 이용한 뉴욕 타임즈 기사 헤드라인 생성 예제를 필사해주세요."
      ],
      "metadata": {
        "id": "9L-jAHPkiBV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "text = \"\"\"경마장에 있는 말이 뛰고 있다\\n\n",
        "그의 말이 법이다\\n\n",
        "가는 말이 고와야 오는 말이 곱다\\n\"\"\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기 : %d' % vocab_size)\n",
        "\n",
        "sequences = list()\n",
        "for line in text.split('\\n'):\n",
        "    encoded = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "print('학습에 사용할 샘플의 개수: %d' %len(sequences))\n"
      ],
      "metadata": {
        "id": "Klj6gjETZdgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894a5235-36a8-4eac-b07a-4e548769da38"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기 : 12\n",
            "학습에 사용할 샘플의 개수: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(l) for l in sequences) # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\n",
        "\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "X = sequences[:, :-1]\n",
        "y = sequences[:, -1]\n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "metadata": {
        "id": "MsJZ9ldFiARK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
        "\n",
        "embedding_dim = 10\n",
        "hidden_units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X,y, epochs=100, verbose=2)\n",
        "\n",
        "\n",
        "def sentence_generation(model, tokenizer, current_word, n):\n",
        "    init_word = current_word\n",
        "    sentence = ''\n",
        "\n",
        "    for _ in range(n):\n",
        "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n",
        "\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == result:\n",
        "                break\n",
        "\n",
        "        current_word = current_word + ' ' + word\n",
        "\n",
        "        sentence = sentence + ' ' + word\n",
        "\n",
        "    sentence = init_word + sentence\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "_jBuYL0y2za6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model, tokenizer, '경마장에', 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw2d8JLK5p3N",
        "outputId": "01068f83-eb03-4000-9b67-afad3154eb77"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "경마장에 말이 말이 뛰고 있다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 1-(2) Text summarization with attention\n",
        "\n",
        "📌 [아마존 리뷰 요약](https://wikidocs.net/72820) \n",
        "\n",
        "* seq2seq + attention 을 이용한 아마존 리뷰 글 text summarization 예제를 필사해주세요."
      ],
      "metadata": {
        "id": "ktAuag_Nk5Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hX45wYr6vM-",
        "outputId": "a913ee93-7fba-4ae7-9f35-5d5189ec49e6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import urllib.request\n",
        "np.random.seed(seed=0)\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Euron NLP/복습과제/Week13/Reviews.csv', nrows=100000)"
      ],
      "metadata": {
        "id": "mLmPLEYdZlzX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수 내 사용\n",
        "contractions = {\"'cause\": 'because',\n",
        " \"I'd\": 'I would',\n",
        " \"I'd've\": 'I would have',\n",
        " \"I'll\": 'I will',\n",
        " \"I'll've\": 'I will have',\n",
        " \"I'm\": 'I am',\n",
        " \"I've\": 'I have',\n",
        " \"ain't\": 'is not',\n",
        " \"aren't\": 'are not',\n",
        " \"can't\": 'cannot',\n",
        " \"could've\": 'could have',\n",
        " \"couldn't\": 'could not',\n",
        " \"didn't\": 'did not',\n",
        " \"doesn't\": 'does not',\n",
        " \"don't\": 'do not',\n",
        " \"hadn't\": 'had not',\n",
        " \"hasn't\": 'has not',\n",
        " \"haven't\": 'have not',\n",
        " \"he'd\": 'he would',\n",
        " \"he'll\": 'he will',\n",
        " \"he's\": 'he is',\n",
        " \"here's\": 'here is',\n",
        " \"how'd\": 'how did',\n",
        " \"how'd'y\": 'how do you',\n",
        " \"how'll\": 'how will',\n",
        " \"how's\": 'how is',\n",
        " \"i'd\": 'i would',\n",
        " \"i'd've\": 'i would have',\n",
        " \"i'll\": 'i will',\n",
        " \"i'll've\": 'i will have',\n",
        " \"i'm\": 'i am',\n",
        " \"i've\": 'i have',\n",
        " \"isn't\": 'is not',\n",
        " \"it'd\": 'it would',\n",
        " \"it'd've\": 'it would have',\n",
        " \"it'll\": 'it will',\n",
        " \"it'll've\": 'it will have',\n",
        " \"it's\": 'it is',\n",
        " \"let's\": 'let us',\n",
        " \"ma'am\": 'madam',\n",
        " \"mayn't\": 'may not',\n",
        " \"might've\": 'might have',\n",
        " \"mightn't\": 'might not',\n",
        " \"mightn't've\": 'might not have',\n",
        " \"must've\": 'must have',\n",
        " \"mustn't\": 'must not',\n",
        " \"mustn't've\": 'must not have',\n",
        " \"needn't\": 'need not',\n",
        " \"needn't've\": 'need not have',\n",
        " \"o'clock\": 'of the clock',\n",
        " \"oughtn't\": 'ought not',\n",
        " \"oughtn't've\": 'ought not have',\n",
        " \"sha'n't\": 'shall not',\n",
        " \"shan't\": 'shall not',\n",
        " \"shan't've\": 'shall not have',\n",
        " \"she'd\": 'she would',\n",
        " \"she'd've\": 'she would have',\n",
        " \"she'll\": 'she will',\n",
        " \"she'll've\": 'she will have',\n",
        " \"she's\": 'she is',\n",
        " \"should've\": 'should have',\n",
        " \"shouldn't\": 'should not',\n",
        " \"shouldn't've\": 'should not have',\n",
        " \"so's\": 'so as',\n",
        " \"so've\": 'so have',\n",
        " \"that'd\": 'that would',\n",
        " \"that'd've\": 'that would have',\n",
        " \"that's\": 'that is',\n",
        " \"there'd\": 'there would',\n",
        " \"there'd've\": 'there would have',\n",
        " \"there's\": 'there is',\n",
        " \"they'd\": 'they would',\n",
        " \"they'd've\": 'they would have',\n",
        " \"they'll\": 'they will',\n",
        " \"they'll've\": 'they will have',\n",
        " \"they're\": 'they are',\n",
        " \"they've\": 'they have',\n",
        " \"this's\": 'this is',\n",
        " \"to've\": 'to have',\n",
        " \"wasn't\": 'was not',\n",
        " \"we'd\": 'we would',\n",
        " \"we'd've\": 'we would have',\n",
        " \"we'll\": 'we will',\n",
        " \"we'll've\": 'we will have',\n",
        " \"we're\": 'we are',\n",
        " \"we've\": 'we have',\n",
        " \"weren't\": 'were not',\n",
        " \"what'll\": 'what will',\n",
        " \"what'll've\": 'what will have',\n",
        " \"what're\": 'what are',\n",
        " \"what's\": 'what is',\n",
        " \"what've\": 'what have',\n",
        " \"when's\": 'when is',\n",
        " \"when've\": 'when have',\n",
        " \"where'd\": 'where did',\n",
        " \"where's\": 'where is',\n",
        " \"where've\": 'where have',\n",
        " \"who'll\": 'who will',\n",
        " \"who'll've\": 'who will have',\n",
        " \"who's\": 'who is',\n",
        " \"who've\": 'who have',\n",
        " \"why's\": 'why is',\n",
        " \"why've\": 'why have',\n",
        " \"will've\": 'will have',\n",
        " \"won't\": 'will not',\n",
        " \"won't've\": 'will not have',\n",
        " \"would've\": 'would have',\n",
        " \"wouldn't\": 'would not',\n",
        " \"wouldn't've\": 'would not have',\n",
        " \"y'all\": 'you all',\n",
        " \"y'all'd\": 'you all would',\n",
        " \"y'all'd've\": 'you all would have',\n",
        " \"y'all're\": 'you all are',\n",
        " \"y'all've\": 'you all have',\n",
        " \"you'd\": 'you would',\n",
        " \"you'd've\": 'you would have',\n",
        " \"you'll\": 'you will',\n",
        " \"you'll've\": 'you will have',\n",
        " \"you're\": 'you are',\n",
        " \"you've\": 'you have'}"
      ],
      "metadata": {
        "id": "GXRxu3Sn8uZI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[['Text', 'Summary']]\n",
        "data.drop_duplicates(subset=['Text'], inplace=True)\n",
        "\n",
        "data.dropna(axis=0, inplace=True)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_sentence(sentence, remove_stopwords=True):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = BeautifulSoup(sentence, \"lxml\").text\n",
        "    sentence = re.sub(r'\\([^)]*\\)', '', sentence)\n",
        "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")])\n",
        "    sentence = re.sub(r\"'s\\b\", \"\", sentence)\n",
        "    sentence = re.sub('[m]{2,}', 'mm', sentence)\n",
        "\n",
        "    if remove_stopwords:\n",
        "         tokens = ' '.join(word for  word in sentence.split() if not word in stop_words if len(word)>1 )\n",
        "    else:\n",
        "         tokens = ' '.join(word for word in sentence.split() if len(word)>1)\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "VWINMRvq6tn9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPM8vAAS_Bqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
        "temp_summary = 'Great way to start (or finish) the day!!!'\n",
        "print(preprocess_sentence(temp_text))\n",
        "print(preprocess_sentence(temp_summary, 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUHqmIEv6ba2",
        "outputId": "fe5076d1-1388-4ab7-875f-e0d29cf24500"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "everything bought great, infact ordered twice third ordered wasfor mother father.\n",
            "great way to start the day!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text 열 전처리 \n",
        "clean_text = []\n",
        "for s in data['Text']:\n",
        "    clean_text.append(preprocess_sentence(s))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d7T1b4f96bc4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary 열 전처리\n",
        "clean_summary = []\n",
        "for s in data['Summary']:\n",
        "    clean_summary.append(preprocess_sentence(s,0))\n",
        "clean_summary[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbA5UPbB-3gr",
        "outputId": "14ace164-9e73-46de-de2c-11ea65556d7f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.amazon.com/gp/product/b007i7yygy/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['good quality dog food',\n",
              " 'not as advertised',\n",
              " '\"delight\" says it all',\n",
              " 'cough medicine',\n",
              " 'great taffy']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Text'] = clean_text\n",
        "data['Summary'] = clean_summary"
      ],
      "metadata": {
        "id": "xcgptSxJ3zhP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_max_len = 50\n",
        "summary_max_len = 8\n",
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "    cnt=0\n",
        "    for s in nested_list:\n",
        "        if(len(s.split()) <= max_len):\n",
        "            cnt = cnt + 1\n",
        "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n",
        "\n",
        "below_threshold_len(text_max_len, data['Text'])\n",
        "below_threshold_len(summary_max_len, data['Summary'])\n",
        "\n",
        "data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
        "data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
        "print('전체 샘플수 :',(len(data)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D70zP-O4_HQy",
        "outputId": "d85dc703-a7b9-4d7d-8cd9-f722e92c6f5a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 중 길이가 50 이하인 샘플의 비율: 0.7614701724625389\n",
            "전체 샘플 중 길이가 8 이하인 샘플의 비율: 0.9434322872490811\n",
            "전체 샘플수 : 64852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다. \n",
        "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken ' + x)\n",
        "data['decoder_target'] = data['Summary'].apply(lambda x : x + 'eostoken ')\n",
        "\n",
        "encoder_input = np.array(data['Text'])\n",
        "decoder_input = np.array(data['decoder_input'])\n",
        "decoder_target = np.array(data['decoder_target'])\n"
      ],
      "metadata": {
        "id": "W62xoyyx_HTD"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) 데이터의 분리\n",
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]\n",
        "\n",
        "n_of_val = int(len(encoder_input)*0.2)\n",
        "\n",
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]\n",
        "                                   "
      ],
      "metadata": {
        "id": "HQOWaFcMAHWl"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4)정수 인코딩\n",
        "\n",
        "src_tokenizer = Tokenizer()\n",
        "src_tokenizer.fit_on_texts(encoder_input_train)\n",
        "\n",
        "threshold = 7 \n",
        "total_cnt = len(src_tokenizer.word_index)\n",
        "rare_cnt = 0\n",
        "total_freq = 0 \n",
        "rare_freq = 0 \n",
        "\n",
        "for key, value in src_tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    if(value<threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq +value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqdDOn0JAHY5",
        "outputId": "221add8d-8746-43fe-a121-1dd1802c9a43"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합(vocabulary)의 크기 : 33781\n",
            "등장 빈도가 6번 이하인 희귀 단어의 수: 25324\n",
            "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 8457\n",
            "단어 집합에서 희귀 단어의 비율: 74.96521713389184\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.4570137129713476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = 8000\n",
        "src_tokenizer = Tokenizer(num_words = src_vocab)\n",
        "src_tokenizer.fit_on_texts(encoder_input_train)\n",
        "\n",
        "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train)\n",
        "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
        "\n",
        "tar_tokenizer = Tokenizer()\n",
        "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
        "\n",
        "threshold = 6\n",
        "total_cnt = len(tar_tokenizer.word_index)\n",
        "rare_cnt = 0\n",
        "total_freq = 0\n",
        "rare_freq = 0 \n",
        "\n",
        "# 단어와 빈도수의 쌍을 key와 value로 만든다.\n",
        "for key, value in tar_tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value<threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j5Ee8B7CsbW",
        "outputId": "ca384f9b-f59e-4b22-c802-747522508f43"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합(vocabulary)의 크기 : 13712\n",
            "등장 빈도가 5번 이하인 희귀 단어의 수: 10941\n",
            "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 2771\n",
            "단어 집합에서 희귀 단어의 비율: 79.7914235705951\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 10.224984532313403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tar_vocab = 2000\n",
        "tar_tokenizer = Tokenizer(num_words = tar_vocab) \n",
        "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
        "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
        "\n",
        "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
        "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train)\n",
        "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
        "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
        "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrGYrvIAD-Y6",
        "outputId": "4a0a4a75-0b1d-421b-ad65-f4e333324876"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6, 729, 5], [1752, 769], [650, 125, 543], [6, 940, 20, 29], [1049, 13, 8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) 빈 샘플 제거 \n",
        "\n",
        "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence)==1]\n",
        "\n",
        "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence)==1]\n",
        "\n",
        "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
        "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
        "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
        "\n",
        "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
        "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
        "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
        "\n",
        "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
        "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
        "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
        "print('테스트 레이블의 개수 :',len(decoder_input_test))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyLm-zDMEmlU",
        "outputId": "bc0c49d9-e04e-4fad-d336-a8d414386d02"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 개수 : 42945\n",
            "훈련 레이블의 개수 : 42945\n",
            "테스트 데이터의 개수 : 10740\n",
            "테스트 레이블의 개수 : 10740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) 패딩하기 \n",
        "\n",
        "encoder_input_train = pad_sequences(encoder_input_train, maxlen = text_max_len, padding='post')\n",
        "encoder_input_test = pad_sequences(encoder_input_test, maxlen = text_max_len, padding='post')\n",
        "decoder_input_train = pad_sequences(decoder_input_train, maxlen = summary_max_len, padding='post')\n",
        "decoder_target_train = pad_sequences(decoder_target_train, maxlen = summary_max_len, padding='post')\n",
        "decoder_input_test = pad_sequences(decoder_input_test, maxlen = summary_max_len, padding='post')\n",
        "decoder_target_test = pad_sequences(decoder_target_test, maxlen = summary_max_len, padding='post')"
      ],
      "metadata": {
        "id": "3y0eieNuFeFr"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. seq2seq + attention으로 요약 모델 설계 및 훈련시키기 \n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_size = 256\n",
        "\n",
        "# 인코더 \n",
        "encoder_inputs = Input(shape=(text_max_len,))\n",
        "\n",
        "# 인코더의 임베딩 층\n",
        "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
        "\n",
        "# 인코더의 LSTM 1\n",
        "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout =0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "# 인코더의 LSTM 2\n",
        "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# 인코더의 LSTM 3\n",
        "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# 디코더\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# 디코더의 임베딩 층\n",
        "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 디코더의 LSTM\n",
        "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])"
      ],
      "metadata": {
        "id": "wX_0iU76Fezu"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더의 출력층\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation = 'softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
        "\n",
        "# 모델 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InroKwOIFviV",
        "outputId": "955a1b3d-4a0a-4651-9ab9-0d99c905af0d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 50)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_7 (Embedding)        (None, 50, 128)      1024000     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 50, 256),    394240      ['embedding_7[0][0]']            \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, 50, 256),    525312      ['lstm_2[0][0]']                 \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " embedding_8 (Embedding)        (None, None, 128)    256000      ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)                  [(None, 50, 256),    525312      ['lstm_3[0][0]']                 \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_5 (LSTM)                  [(None, None, 256),  394240      ['embedding_8[0][0]',            \n",
            "                                 (None, 256),                     'lstm_4[0][1]',                 \n",
            "                                 (None, 256)]                     'lstm_4[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, None, 2000)   514000      ['lstm_5[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,633,104\n",
            "Trainable params: 3,633,104\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/drive/MyDrive/Euron NLP/복습과제/Week13/attention.py\"\n",
        "exec(open(\"/content/drive/MyDrive/Euron NLP/복습과제/Week13/attention.py\").read())"
      ],
      "metadata": {
        "id": "O3HkWdiKKDhZ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from attention.py import AttentionLayer\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# 어텐션의 결곽와 디코더의 hidden state들을 연결\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# 디코더의 출력층 \n",
        "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
        "\n",
        "# 모델 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3d8iSVb9MO6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
        "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
        "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
      ],
      "metadata": {
        "id": "gl0M48fGOKIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2️⃣ Text summarization task**"
      ],
      "metadata": {
        "id": "HfTr_BPwGc8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 2-(1) Pororo - text summarization \n",
        "\n",
        "📌 [공식문서](https://kakaobrain.github.io/pororo/seq2seq/summary.html) \n",
        "\n",
        "📌 [예제 실습](https://teddylee777.github.io/machine-learning/nlp-korean-pororo) \n",
        "\n",
        "* PORORO : 카카오 브레인에서 제공한 자연어 처리 라이브러리"
      ],
      "metadata": {
        "id": "eu3adOV2bDs_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e2WbVbbsatD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NmVEEfAqdSa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 2-(2) BERT 를 이용한 text summarization \n",
        "\n",
        "📌 [논문 리뷰](https://medium.com/@eyfydsyd97/bert%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%9A%94%EC%95%BD-text-summary-b582b5cc7d) \n",
        "\n",
        "📌 [BERT Extractive summarizer Library](https://github.com/dmmiller612/bert-extractive-summarizer) \n",
        "\n",
        "\n",
        "📌 [Text summarization Github Repo](https://github.com/uoneway/Text-Summarization-Repo) \n",
        "\n",
        "\n",
        "\n",
        "➕ [BERT 를 이용한 뉴스 요약 자동화 App 구현 Repo](https://github.com/huydang90/News-Summarization-with-BERT) 👉 프로젝트 예시 참고 자료\n",
        "\n"
      ],
      "metadata": {
        "id": "xsx6OgzwbnIt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "svtikbdZatBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mnfL6KIZdSuo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}