{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhUHfXkPAORh"
      },
      "source": [
        "📌 week5 내용 주차에 해당되는 과제는 Glove 모델 실습, NER task 실습, Dependency Parsing task 실습으로 구성되어 있습니다. (**참고** : **제출은 week6 branch 복습과제로!**)\n",
        "\n",
        "📌 위키독스의 딥러닝을 이용한 자연어 처리 입문 교재 실습, 캐글 노트북 등의 자료로 구성되어있는 과제입니다. \n",
        "\n",
        "📌 안내된 링크에 맞추어 **직접 코드를 따라 치면서 (필사)** 해당 nlp task 의 기본적인 라이브러리와 메서드를 숙지해보시면 좋을 것 같습니다😊 필수라고 체크한 부분은 과제에 반드시 포함시켜주시고, 선택으로 체크한 부분은 자율적으로 스터디 하시면 됩니다.\n",
        "\n",
        "📌 궁금한 사항은 깃허브 이슈나, 카톡방, 세션 발표 시작 이전 시간 등을 활용하여 자유롭게 공유해주세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XjTSbcxBB6o",
        "outputId": "3b224ca8-b18f-49de-a1b8-ec221ca85538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "import nltk\n",
        "# nltk colab 환경에서 실행시 필요한 코드입니다. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vPZn15zBHIv"
      },
      "source": [
        "### 1️⃣ **Glove**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P11biHcUuBaH"
      },
      "source": [
        "👀 **내용 복습** \n",
        "* 스탠포드 대학에서 개발한 카운트 기반과 예측 기반을 모두 사용하는 단어 임베딩 방법론 \n",
        "* word2vec 의 단점을 보완해서 나온 모델 \n",
        "* glove model 의 **input 은 반드시 동시등장행렬 형태**여야 한다 ⭐\n",
        "\n",
        "![1](https://www.dropbox.com/s/nz0ji4yzre56ifv/word_presentation.png?raw=1) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "🤔 한국어 예제는 없는 것 같습니다. 논문에서는 k-Glove 로 소개되는 연구가 있긴 한데, 좀 더 알아봐야 할 것 같아요!\n",
        "\n",
        "➕ [논문1](https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=NPAP13255003&dbt=NPAP)\n",
        "\n",
        "\n",
        "➕[논문2](https://scienceon.kisti.re.kr/commons/util/originalView.do?cn=CFKO201832073078664&oCn=NPAP13255064&dbt=CFKO&journal=NPRO00383361&keyword=%ED%95%9C%EA%B5%AD%EC%96%B4%20%EB%8C%80%ED%99%94%20%EC%97%94%EC%A7%84%EC%97%90%EC%84%9C%EC%9D%98%20%EB%AC%B8%EC%9E%A5%EB%B6%84%EB%A5%98)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asGcGy6fBM1E"
      },
      "source": [
        "🔹 **1-(1)** glove python\n",
        "\n",
        "* [실습 : basic code](https://wikidocs.net/22885) 👉 필수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V31NoJdu5t3p",
        "outputId": "4ca1868c-a793-4433-9c45-8a818b08eb2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: glove_python_binary in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.7.3)\n"
          ]
        }
      ],
      "source": [
        "pip install glove_python_binary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta6QgoKO5uXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e010ea-d368-4280-9864-c24f33d6ca56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing 20 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n"
          ]
        }
      ],
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "result = []\n",
        "\n",
        "corpus = Corpus()\n",
        "\n",
        "# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬을 생성\n",
        "corpus.fit(result, window=5)\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "\n",
        "#학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ADfVM9lO9NE"
      },
      "source": [
        "🔹 **1-(2)** pre-trained glove \n",
        "\n",
        "* **사전학습모델** : 임의의 값으로 초기화하던 모델의 가중치들을 다른 문제에 학습시킨 가중치들로 초기화하는 방법이다.사전 학습한 가중치를 활용해 학습하고자 하는 본래 문제를 하위문제라고 한다. \n",
        "\n",
        "* [실습 : 문장의 긍부정을 판단하는 감성 분류 모델 만들기](https://wikidocs.net/33793) 👉 필수\n",
        "  * [설명참고](https://omicro03.medium.com/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-16%EC%9D%BC%EC%B0%A8-pre-trained-word-embedding-bb30db424a35)\n",
        "* pre-trained data 를 가져오는데 시간이 오래걸림\n",
        "* kaggle 대회에서 주로 이 방식을 많이 사용함\n",
        "  * [참고](https://lsjsj92.tistory.com/455)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "output_dim = 128\n",
        "input_length = 500\n",
        "\n",
        "v = Embedding(vocab_size, output_dim, input_length=input_length)"
      ],
      "metadata": {
        "id": "o7nmboPKbBid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
        "y_train = [1, 0, 0, 1, 1, 0, 1]"
      ],
      "metadata": {
        "id": "q74XmiQOeTvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1 # 패딩을 고려하여 +1\n",
        "print('단어 집합 :',vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvIDqPw3ehDv",
        "outputId": "050bd619-5f5a-40bb-f594-c2f105a551a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 : 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print('정수 인코딩 결과 :',X_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHK3vDZ8ekC5",
        "outputId": "afcbbe96-5194-4ab4-96a9-87e1e1ef42c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 결과 : [[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(l) for l in X_encoded)\n",
        "print('최대 길이 :',max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TVWjLjqenIx",
        "outputId": "c7f76158-cbc7-432c-f752-4df3f573acf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최대 길이 : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
        "y_train = np.array(y_train)\n",
        "print('패딩 결과 :')\n",
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoWPn8kXeovw",
        "outputId": "f9bf0ddd-6815-40bf-fac6-e79811db475c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "패딩 결과 :\n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  0  0]\n",
            " [ 7  8  0  0]\n",
            " [ 9 10  0  0]\n",
            " [11 12  0  0]\n",
            " [13  0  0  0]\n",
            " [14 15  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "embedding_dim = 4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9zDkt3qeqL3",
        "outputId": "e94394a5-66eb-41f5-e177-1443ffacdb4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 7 samples\n",
            "Epoch 1/100\n",
            "7/7 - 0s - loss: 0.6916 - acc: 0.4286 - 53ms/epoch - 8ms/sample\n",
            "Epoch 2/100\n",
            "7/7 - 0s - loss: 0.6903 - acc: 0.4286 - 10ms/epoch - 1ms/sample\n",
            "Epoch 3/100\n",
            "7/7 - 0s - loss: 0.6889 - acc: 0.5714 - 5ms/epoch - 752us/sample\n",
            "Epoch 4/100\n",
            "7/7 - 0s - loss: 0.6876 - acc: 0.7143 - 6ms/epoch - 911us/sample\n",
            "Epoch 5/100\n",
            "7/7 - 0s - loss: 0.6862 - acc: 0.7143 - 5ms/epoch - 774us/sample\n",
            "Epoch 6/100\n",
            "7/7 - 0s - loss: 0.6849 - acc: 0.7143 - 4ms/epoch - 597us/sample\n",
            "Epoch 7/100\n",
            "7/7 - 0s - loss: 0.6835 - acc: 0.7143 - 4ms/epoch - 641us/sample\n",
            "Epoch 8/100\n",
            "7/7 - 0s - loss: 0.6821 - acc: 0.7143 - 3ms/epoch - 460us/sample\n",
            "Epoch 9/100\n",
            "7/7 - 0s - loss: 0.6808 - acc: 0.7143 - 4ms/epoch - 566us/sample\n",
            "Epoch 10/100\n",
            "7/7 - 0s - loss: 0.6794 - acc: 0.7143 - 5ms/epoch - 768us/sample\n",
            "Epoch 11/100\n",
            "7/7 - 0s - loss: 0.6780 - acc: 0.7143 - 4ms/epoch - 605us/sample\n",
            "Epoch 12/100\n",
            "7/7 - 0s - loss: 0.6767 - acc: 0.7143 - 4ms/epoch - 558us/sample\n",
            "Epoch 13/100\n",
            "7/7 - 0s - loss: 0.6753 - acc: 0.7143 - 3ms/epoch - 487us/sample\n",
            "Epoch 14/100\n",
            "7/7 - 0s - loss: 0.6739 - acc: 0.7143 - 3ms/epoch - 483us/sample\n",
            "Epoch 15/100\n",
            "7/7 - 0s - loss: 0.6725 - acc: 0.7143 - 4ms/epoch - 614us/sample\n",
            "Epoch 16/100\n",
            "7/7 - 0s - loss: 0.6711 - acc: 0.7143 - 8ms/epoch - 1ms/sample\n",
            "Epoch 17/100\n",
            "7/7 - 0s - loss: 0.6697 - acc: 0.7143 - 4ms/epoch - 510us/sample\n",
            "Epoch 18/100\n",
            "7/7 - 0s - loss: 0.6683 - acc: 0.7143 - 3ms/epoch - 478us/sample\n",
            "Epoch 19/100\n",
            "7/7 - 0s - loss: 0.6669 - acc: 0.7143 - 3ms/epoch - 488us/sample\n",
            "Epoch 20/100\n",
            "7/7 - 0s - loss: 0.6654 - acc: 0.7143 - 4ms/epoch - 596us/sample\n",
            "Epoch 21/100\n",
            "7/7 - 0s - loss: 0.6640 - acc: 0.7143 - 3ms/epoch - 485us/sample\n",
            "Epoch 22/100\n",
            "7/7 - 0s - loss: 0.6626 - acc: 0.7143 - 3ms/epoch - 486us/sample\n",
            "Epoch 23/100\n",
            "7/7 - 0s - loss: 0.6611 - acc: 0.7143 - 5ms/epoch - 683us/sample\n",
            "Epoch 24/100\n",
            "7/7 - 0s - loss: 0.6597 - acc: 0.7143 - 3ms/epoch - 496us/sample\n",
            "Epoch 25/100\n",
            "7/7 - 0s - loss: 0.6582 - acc: 0.7143 - 4ms/epoch - 642us/sample\n",
            "Epoch 26/100\n",
            "7/7 - 0s - loss: 0.6568 - acc: 0.7143 - 4ms/epoch - 512us/sample\n",
            "Epoch 27/100\n",
            "7/7 - 0s - loss: 0.6553 - acc: 0.7143 - 4ms/epoch - 535us/sample\n",
            "Epoch 28/100\n",
            "7/7 - 0s - loss: 0.6538 - acc: 0.7143 - 4ms/epoch - 617us/sample\n",
            "Epoch 29/100\n",
            "7/7 - 0s - loss: 0.6523 - acc: 0.7143 - 4ms/epoch - 536us/sample\n",
            "Epoch 30/100\n",
            "7/7 - 0s - loss: 0.6508 - acc: 0.7143 - 4ms/epoch - 625us/sample\n",
            "Epoch 31/100\n",
            "7/7 - 0s - loss: 0.6493 - acc: 0.7143 - 5ms/epoch - 655us/sample\n",
            "Epoch 32/100\n",
            "7/7 - 0s - loss: 0.6478 - acc: 0.7143 - 3ms/epoch - 482us/sample\n",
            "Epoch 33/100\n",
            "7/7 - 0s - loss: 0.6463 - acc: 0.7143 - 3ms/epoch - 473us/sample\n",
            "Epoch 34/100\n",
            "7/7 - 0s - loss: 0.6448 - acc: 0.7143 - 4ms/epoch - 542us/sample\n",
            "Epoch 35/100\n",
            "7/7 - 0s - loss: 0.6432 - acc: 0.7143 - 6ms/epoch - 847us/sample\n",
            "Epoch 36/100\n",
            "7/7 - 0s - loss: 0.6417 - acc: 0.7143 - 3ms/epoch - 495us/sample\n",
            "Epoch 37/100\n",
            "7/7 - 0s - loss: 0.6401 - acc: 0.7143 - 4ms/epoch - 501us/sample\n",
            "Epoch 38/100\n",
            "7/7 - 0s - loss: 0.6386 - acc: 0.7143 - 4ms/epoch - 504us/sample\n",
            "Epoch 39/100\n",
            "7/7 - 0s - loss: 0.6370 - acc: 0.7143 - 4ms/epoch - 638us/sample\n",
            "Epoch 40/100\n",
            "7/7 - 0s - loss: 0.6354 - acc: 0.7143 - 4ms/epoch - 621us/sample\n",
            "Epoch 41/100\n",
            "7/7 - 0s - loss: 0.6338 - acc: 0.7143 - 3ms/epoch - 498us/sample\n",
            "Epoch 42/100\n",
            "7/7 - 0s - loss: 0.6322 - acc: 0.7143 - 3ms/epoch - 474us/sample\n",
            "Epoch 43/100\n",
            "7/7 - 0s - loss: 0.6306 - acc: 0.7143 - 4ms/epoch - 572us/sample\n",
            "Epoch 44/100\n",
            "7/7 - 0s - loss: 0.6290 - acc: 0.7143 - 5ms/epoch - 676us/sample\n",
            "Epoch 45/100\n",
            "7/7 - 0s - loss: 0.6274 - acc: 0.7143 - 3ms/epoch - 451us/sample\n",
            "Epoch 46/100\n",
            "7/7 - 0s - loss: 0.6258 - acc: 0.7143 - 5ms/epoch - 677us/sample\n",
            "Epoch 47/100\n",
            "7/7 - 0s - loss: 0.6242 - acc: 0.7143 - 3ms/epoch - 450us/sample\n",
            "Epoch 48/100\n",
            "7/7 - 0s - loss: 0.6225 - acc: 0.7143 - 3ms/epoch - 466us/sample\n",
            "Epoch 49/100\n",
            "7/7 - 0s - loss: 0.6209 - acc: 0.7143 - 3ms/epoch - 476us/sample\n",
            "Epoch 50/100\n",
            "7/7 - 0s - loss: 0.6192 - acc: 0.7143 - 4ms/epoch - 566us/sample\n",
            "Epoch 51/100\n",
            "7/7 - 0s - loss: 0.6175 - acc: 0.7143 - 4ms/epoch - 514us/sample\n",
            "Epoch 52/100\n",
            "7/7 - 0s - loss: 0.6159 - acc: 0.7143 - 4ms/epoch - 533us/sample\n",
            "Epoch 53/100\n",
            "7/7 - 0s - loss: 0.6142 - acc: 0.7143 - 6ms/epoch - 855us/sample\n",
            "Epoch 54/100\n",
            "7/7 - 0s - loss: 0.6125 - acc: 0.8571 - 3ms/epoch - 456us/sample\n",
            "Epoch 55/100\n",
            "7/7 - 0s - loss: 0.6108 - acc: 0.8571 - 3ms/epoch - 491us/sample\n",
            "Epoch 56/100\n",
            "7/7 - 0s - loss: 0.6091 - acc: 0.8571 - 5ms/epoch - 643us/sample\n",
            "Epoch 57/100\n",
            "7/7 - 0s - loss: 0.6074 - acc: 0.8571 - 3ms/epoch - 468us/sample\n",
            "Epoch 58/100\n",
            "7/7 - 0s - loss: 0.6057 - acc: 0.8571 - 6ms/epoch - 869us/sample\n",
            "Epoch 59/100\n",
            "7/7 - 0s - loss: 0.6039 - acc: 0.8571 - 3ms/epoch - 480us/sample\n",
            "Epoch 60/100\n",
            "7/7 - 0s - loss: 0.6022 - acc: 0.8571 - 3ms/epoch - 481us/sample\n",
            "Epoch 61/100\n",
            "7/7 - 0s - loss: 0.6004 - acc: 0.8571 - 3ms/epoch - 489us/sample\n",
            "Epoch 62/100\n",
            "7/7 - 0s - loss: 0.5987 - acc: 0.8571 - 7ms/epoch - 1ms/sample\n",
            "Epoch 63/100\n",
            "7/7 - 0s - loss: 0.5969 - acc: 0.8571 - 3ms/epoch - 488us/sample\n",
            "Epoch 64/100\n",
            "7/7 - 0s - loss: 0.5952 - acc: 0.8571 - 3ms/epoch - 448us/sample\n",
            "Epoch 65/100\n",
            "7/7 - 0s - loss: 0.5934 - acc: 1.0000 - 3ms/epoch - 471us/sample\n",
            "Epoch 66/100\n",
            "7/7 - 0s - loss: 0.5916 - acc: 1.0000 - 3ms/epoch - 487us/sample\n",
            "Epoch 67/100\n",
            "7/7 - 0s - loss: 0.5898 - acc: 1.0000 - 4ms/epoch - 552us/sample\n",
            "Epoch 68/100\n",
            "7/7 - 0s - loss: 0.5880 - acc: 1.0000 - 6ms/epoch - 912us/sample\n",
            "Epoch 69/100\n",
            "7/7 - 0s - loss: 0.5862 - acc: 1.0000 - 4ms/epoch - 510us/sample\n",
            "Epoch 70/100\n",
            "7/7 - 0s - loss: 0.5844 - acc: 1.0000 - 5ms/epoch - 657us/sample\n",
            "Epoch 71/100\n",
            "7/7 - 0s - loss: 0.5826 - acc: 1.0000 - 4ms/epoch - 508us/sample\n",
            "Epoch 72/100\n",
            "7/7 - 0s - loss: 0.5807 - acc: 1.0000 - 5ms/epoch - 686us/sample\n",
            "Epoch 73/100\n",
            "7/7 - 0s - loss: 0.5789 - acc: 1.0000 - 3ms/epoch - 489us/sample\n",
            "Epoch 74/100\n",
            "7/7 - 0s - loss: 0.5770 - acc: 1.0000 - 4ms/epoch - 606us/sample\n",
            "Epoch 75/100\n",
            "7/7 - 0s - loss: 0.5752 - acc: 1.0000 - 6ms/epoch - 893us/sample\n",
            "Epoch 76/100\n",
            "7/7 - 0s - loss: 0.5733 - acc: 1.0000 - 3ms/epoch - 477us/sample\n",
            "Epoch 77/100\n",
            "7/7 - 0s - loss: 0.5715 - acc: 1.0000 - 3ms/epoch - 497us/sample\n",
            "Epoch 78/100\n",
            "7/7 - 0s - loss: 0.5696 - acc: 1.0000 - 4ms/epoch - 502us/sample\n",
            "Epoch 79/100\n",
            "7/7 - 0s - loss: 0.5677 - acc: 1.0000 - 4ms/epoch - 546us/sample\n",
            "Epoch 80/100\n",
            "7/7 - 0s - loss: 0.5658 - acc: 1.0000 - 3ms/epoch - 447us/sample\n",
            "Epoch 81/100\n",
            "7/7 - 0s - loss: 0.5639 - acc: 1.0000 - 3ms/epoch - 440us/sample\n",
            "Epoch 82/100\n",
            "7/7 - 0s - loss: 0.5621 - acc: 1.0000 - 4ms/epoch - 520us/sample\n",
            "Epoch 83/100\n",
            "7/7 - 0s - loss: 0.5601 - acc: 1.0000 - 5ms/epoch - 653us/sample\n",
            "Epoch 84/100\n",
            "7/7 - 0s - loss: 0.5582 - acc: 1.0000 - 6ms/epoch - 797us/sample\n",
            "Epoch 85/100\n",
            "7/7 - 0s - loss: 0.5563 - acc: 1.0000 - 3ms/epoch - 492us/sample\n",
            "Epoch 86/100\n",
            "7/7 - 0s - loss: 0.5544 - acc: 1.0000 - 4ms/epoch - 630us/sample\n",
            "Epoch 87/100\n",
            "7/7 - 0s - loss: 0.5525 - acc: 1.0000 - 3ms/epoch - 479us/sample\n",
            "Epoch 88/100\n",
            "7/7 - 0s - loss: 0.5505 - acc: 1.0000 - 4ms/epoch - 565us/sample\n",
            "Epoch 89/100\n",
            "7/7 - 0s - loss: 0.5486 - acc: 1.0000 - 3ms/epoch - 481us/sample\n",
            "Epoch 90/100\n",
            "7/7 - 0s - loss: 0.5466 - acc: 1.0000 - 3ms/epoch - 471us/sample\n",
            "Epoch 91/100\n",
            "7/7 - 0s - loss: 0.5447 - acc: 1.0000 - 3ms/epoch - 471us/sample\n",
            "Epoch 92/100\n",
            "7/7 - 0s - loss: 0.5427 - acc: 1.0000 - 3ms/epoch - 500us/sample\n",
            "Epoch 93/100\n",
            "7/7 - 0s - loss: 0.5408 - acc: 1.0000 - 3ms/epoch - 437us/sample\n",
            "Epoch 94/100\n",
            "7/7 - 0s - loss: 0.5388 - acc: 1.0000 - 3ms/epoch - 486us/sample\n",
            "Epoch 95/100\n",
            "7/7 - 0s - loss: 0.5368 - acc: 1.0000 - 4ms/epoch - 634us/sample\n",
            "Epoch 96/100\n",
            "7/7 - 0s - loss: 0.5349 - acc: 1.0000 - 3ms/epoch - 489us/sample\n",
            "Epoch 97/100\n",
            "7/7 - 0s - loss: 0.5329 - acc: 1.0000 - 5ms/epoch - 682us/sample\n",
            "Epoch 98/100\n",
            "7/7 - 0s - loss: 0.5309 - acc: 1.0000 - 5ms/epoch - 673us/sample\n",
            "Epoch 99/100\n",
            "7/7 - 0s - loss: 0.5289 - acc: 1.0000 - 3ms/epoch - 497us/sample\n",
            "Epoch 100/100\n",
            "7/7 - 0s - loss: 0.5269 - acc: 1.0000 - 5ms/epoch - 665us/sample\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb87e3a2150>"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktGZyzrVevaj",
        "outputId": "9f418b23-30f2-4ea0-9d29-984a7327d7f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  2  3  4]\n",
            " [ 5  6  0  0]\n",
            " [ 7  8  0  0]\n",
            " [ 9 10  0  0]\n",
            " [11 12  0  0]\n",
            " [13  0  0  0]\n",
            " [14 15  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_3Dsd0mexQ3",
        "outputId": "dbec013b-90c9-4448-d7a2-b7f372451e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve, urlopen\n",
        "import gzip\n",
        "import zipfile\n",
        "\n",
        "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
        "zf = zipfile.ZipFile('glove.6B.zip')\n",
        "zf.extractall() \n",
        "zf.close()"
      ],
      "metadata": {
        "id": "HJ6Yp5bTezn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict = dict()\n",
        "\n",
        "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "\n",
        "    # 100개의 값을 가지는 array로 변환\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "\n",
        "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNnwrdzie0if",
        "outputId": "dfce0adf-9032-47c2-a02b-618fac1c9b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000개의 Embedding vector가 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_dict['respectable'])\n",
        "print('벡터의 차원 수 :',len(embedding_dict['respectable']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR4DhJioe2we",
        "outputId": "abd75671-27b8-4e28-ef95-ad7e597ffc24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.049773   0.19903    0.10585    0.1391    -0.32395    0.44053\n",
            "  0.3947    -0.22805   -0.25793    0.49768    0.15384   -0.08831\n",
            "  0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\n",
            "  0.74756    0.98256    0.81872    0.28507    0.16178   -0.48626\n",
            " -0.006265  -0.92469   -0.30625   -0.067318  -0.046762  -0.76291\n",
            " -0.0025264 -0.018795   0.12882   -0.52457    0.3586     0.43119\n",
            " -0.89477   -0.057421  -0.53724    0.25587    0.55195    0.44698\n",
            " -0.24252    0.29946    0.25776   -0.8717     0.68426   -0.05688\n",
            " -0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488\n",
            " -0.32507   -0.90171    0.17672    0.075601   0.54896   -0.21488\n",
            " -0.54018   -0.45882   -0.79536    0.26331    0.18879   -0.16363\n",
            "  0.3975     0.1099     0.1164    -0.083499   0.50159    0.35802\n",
            "  0.25677    0.088546   0.42108    0.28674   -0.71285   -0.82915\n",
            "  0.15297   -0.82712    0.022112   1.067     -0.31776    0.1211\n",
            " -0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694\n",
            " -0.0090944  0.1109     0.62543   -0.23682   -0.44928   -0.3667\n",
            " -0.21616   -0.19187   -0.032502   0.38025  ]\n",
            "벡터의 차원 수 : 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "print('임베딩 행렬의 크기(shape) :',np.shape(embedding_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKiXXqj2e5mR",
        "outputId": "5bce9e1c-a599-45f1-9e2d-30602ab02dac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 행렬의 크기(shape) : (16, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBqszD2ze7A0",
        "outputId": "fa8d2761-1165-4ce5-d5da-ea1e8bc6e311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('highly', 14), ('respectable', 15)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 great의 맵핑된 정수 :',tokenizer.word_index['great'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXlLy47fe9n_",
        "outputId": "d09211db-8a0f-4736-9ae2-21e06cee051c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 great의 맵핑된 정수 : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_dict['great'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozUcIcsXe_LK",
        "outputId": "e9bf477d-3f50-484f-df39-881106db9b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.013786   0.38216    0.53236    0.15261   -0.29694   -0.20558\n",
            " -0.41846   -0.58437   -0.77355   -0.87866   -0.37858   -0.18516\n",
            " -0.128     -0.20584   -0.22925   -0.42599    0.3725     0.26077\n",
            " -1.0702     0.62916   -0.091469   0.70348   -0.4973    -0.77691\n",
            "  0.66045    0.09465   -0.44893    0.018917   0.33146   -0.35022\n",
            " -0.35789    0.030313   0.22253   -0.23236   -0.19719   -0.0053125\n",
            " -0.25848    0.58081   -0.10705   -0.17845   -0.16206    0.087086\n",
            "  0.63029   -0.76649    0.51619    0.14073    1.019     -0.43136\n",
            "  0.46138   -0.43585   -0.47568    0.19226    0.36065    0.78987\n",
            "  0.088945  -2.7814    -0.15366    0.01015    1.1798     0.15168\n",
            " -0.050112   1.2626    -0.77527    0.36031    0.95761   -0.11385\n",
            "  0.28035   -0.02591    0.31246   -0.15424    0.3778    -0.13599\n",
            "  0.2946    -0.31579    0.42943    0.086969   0.019169  -0.27242\n",
            " -0.31696    0.37327    0.61997    0.13889    0.17188    0.30363\n",
            " -1.2776     0.044423  -0.52736   -0.88536   -0.19428   -0.61947\n",
            " -0.10146   -0.26301   -0.061707   0.36627   -0.95223   -0.39346\n",
            " -0.69183   -1.0426     0.28855    0.63056  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in tokenizer.word_index.items():\n",
        "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
        "    vector_value = embedding_dict.get(word)\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value"
      ],
      "metadata": {
        "id": "RysjIzg0fA9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJhD8cfYfCBs",
        "outputId": "9fdb0af2-70ba-4742-94e3-3ba68b42d831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.013786  ,  0.38216001,  0.53236002,  0.15261   , -0.29694   ,\n",
              "       -0.20558   , -0.41846001, -0.58437002, -0.77354997, -0.87866002,\n",
              "       -0.37858   , -0.18516   , -0.12800001, -0.20584001, -0.22925   ,\n",
              "       -0.42598999,  0.3725    ,  0.26076999, -1.07019997,  0.62915999,\n",
              "       -0.091469  ,  0.70348001, -0.4973    , -0.77691001,  0.66044998,\n",
              "        0.09465   , -0.44893   ,  0.018917  ,  0.33146   , -0.35021999,\n",
              "       -0.35789001,  0.030313  ,  0.22253001, -0.23236001, -0.19719   ,\n",
              "       -0.0053125 , -0.25848001,  0.58081001, -0.10705   , -0.17845   ,\n",
              "       -0.16205999,  0.087086  ,  0.63028997, -0.76648998,  0.51618999,\n",
              "        0.14072999,  1.01900005, -0.43136001,  0.46138   , -0.43584999,\n",
              "       -0.47567999,  0.19226   ,  0.36065   ,  0.78987002,  0.088945  ,\n",
              "       -2.78139997, -0.15366   ,  0.01015   ,  1.17980003,  0.15167999,\n",
              "       -0.050112  ,  1.26259995, -0.77526999,  0.36030999,  0.95761001,\n",
              "       -0.11385   ,  0.28035   , -0.02591   ,  0.31246001, -0.15424   ,\n",
              "        0.37779999, -0.13598999,  0.29460001, -0.31579   ,  0.42943001,\n",
              "        0.086969  ,  0.019169  , -0.27241999, -0.31696001,  0.37327   ,\n",
              "        0.61997002,  0.13889   ,  0.17188001,  0.30362999, -1.27760005,\n",
              "        0.044423  , -0.52736002, -0.88536   , -0.19428   , -0.61947   ,\n",
              "       -0.10146   , -0.26301   , -0.061707  ,  0.36627001, -0.95222998,\n",
              "       -0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "output_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, output_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIvV1_r0fKSQ",
        "outputId": "eada9319-003d-4d4d-bc03-204ae8643dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 1s - loss: 0.7756 - acc: 0.4286 - 563ms/epoch - 563ms/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.7528 - acc: 0.4286 - 6ms/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.7309 - acc: 0.4286 - 9ms/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.7097 - acc: 0.4286 - 8ms/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.6893 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.6696 - acc: 0.5714 - 7ms/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.6508 - acc: 0.5714 - 8ms/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.6326 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.6152 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.5984 - acc: 0.7143 - 7ms/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.5823 - acc: 0.7143 - 6ms/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.5669 - acc: 0.7143 - 7ms/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.5520 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.5377 - acc: 0.7143 - 8ms/epoch - 8ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.5239 - acc: 0.7143 - 5ms/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.5107 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.4979 - acc: 0.7143 - 9ms/epoch - 9ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.4856 - acc: 0.7143 - 5ms/epoch - 5ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.4737 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.4623 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.4512 - acc: 0.8571 - 7ms/epoch - 7ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.4405 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.4302 - acc: 0.8571 - 8ms/epoch - 8ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.4202 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.4106 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.4012 - acc: 0.8571 - 12ms/epoch - 12ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.3921 - acc: 0.8571 - 6ms/epoch - 6ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.3834 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.3748 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.3666 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.3585 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.3507 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.3432 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.3358 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.3286 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.3217 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.3149 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.3084 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.3020 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.2957 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.2897 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.2838 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.2781 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.2725 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.2671 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.2619 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.2567 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.2517 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.2469 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.2422 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.2376 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.2331 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.2288 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.2245 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.2204 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.2164 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.2125 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.2087 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.2050 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.2014 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.1979 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.1944 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.1911 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.1879 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.1847 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.1816 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.1786 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.1757 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.1728 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.1701 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.1674 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.1647 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.1621 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.1596 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.1571 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.1547 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.1524 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.1501 - acc: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.1479 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.1457 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.1436 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.1415 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.1394 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.1375 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.1355 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.1336 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.1318 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.1299 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.1282 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.1264 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.1247 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.1230 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.1214 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.1198 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.1183 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.1167 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.1152 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.1138 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.1123 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.1109 - acc: 1.0000 - 5ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb8e8b78710>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_wcrE5PtLMI"
      },
      "source": [
        "🔹 **1-(3)** fine tuning glove\n",
        "* 미세조정 : 사전 학습한 모든 가중치와 더불어 하위 문제를 위한 최소한의 가중치를 추가해 모델을 추가로 학습하는 방법이다. \n",
        "\n",
        "* fine tuning 이 필요한 경우 \n",
        "  * pretrained model 에 데이터셋에 있는 단어가 포함되지 않은 경우 \n",
        "  * 데이터 집합이 너무 작아서 전체 모델을 훈련시키기 어려운 경우 \n",
        "\n",
        "* [Mittens 라이브러리로 fine tuning](https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39) 👉 필수\n",
        "  *  GloVe 임베딩을 fine-tuning 하기 위한 파이썬 라이브러리\n",
        "  * [github](https://github.com/roamanalytics/mittens)\n",
        "\n",
        "* [한국어 소설 텍스트 데이터 미세조정 모델 학습 - GPT2](https://m.blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=horajjan&logNo=222104684132&categoryNo=120&proxyReferer=) 👉 선택 (glove 모델 예제는 아닙니다. fine-tuning 에 초점을 두어서 참고해주시면 좋을 것 같습니다.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U mittens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ9wrD9HhBks",
        "outputId": "c79164e8-8427-43f9-be1a-20b43982f51b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mittens\n",
            "  Downloading mittens-0.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mittens) (1.21.6)\n",
            "Installing collected packages: mittens\n",
            "Successfully installed mittens-0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "from nltk.corpus import brown\n",
        "from mittens import GloVe, Mittens\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "Hsc6oP8XikDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def glove2dict(glove_filename):\n",
        "    with open(glove_filename, encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
        "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
        "                for line in reader}\n",
        "    return embed"
      ],
      "metadata": {
        "id": "3DqRk2vWjdvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_-OB9Siga3G"
      },
      "source": [
        "* (참고) word2vec pretrained example\n",
        "\n",
        "➕ [word2vec 사전학습 모델 -한국어1](http://doc.mindscale.kr/km/unstructured/11.html)\n",
        "\n",
        "➕ [word2vec 사전학습 - 한국어2](https://monetd.github.io/python/nlp/Word-Embedding-Word2Vec-%EC%8B%A4%EC%8A%B5/#%ED%95%9C%EA%B5%AD%EC%96%B4-word2vec-%EB%A7%8C%EB%93%A4%EA%B8%B0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUWWDwdiPLS9"
      },
      "source": [
        "### **2️⃣ NER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N0B4VknPkTk"
      },
      "source": [
        "👀 **내용 복습** \n",
        "* 개체명 인식을 사용하면 코퍼스로부터 어떤 단어가 사람, 장소, 조직 등을 의미하는 단어인지를 찾을 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWgla1BuPRqJ"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "🔹 **2-(1)** NER task by nltk library\n",
        "\n",
        "\n",
        "* nltk 에서는 개체명 인식기 (NER chunker) 를 지원하고 있다. \n",
        "* ne_chunk 는 개체명을 태깅하기 위해서 앞서 품사 태깅 pos_tag 가 수행되어야 한다. \n",
        "\n",
        "\n",
        "📌 [basic code](https://wikidocs.net/30682) 👉 필수 \n",
        "\n",
        "📌 [BIO 표현, LSTM을 활용한 NER 실습](https://wikidocs.net/24682) 👉 선택\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "#nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap4vHDQ1NwC7",
        "outputId": "e232e1d4-0b32-4d53-c78e-58e7983a6d92"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "diaZweMyAxJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f8b69bc-43c6-486d-89c8-901c63bad79c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "sentence = \"James is working at Disney in London\"\n",
        "\n",
        "#토큰화 후 품사 태깅\n",
        "\n",
        "tokenized_sentence = pos_tag(word_tokenize(sentence))\n",
        "print(tokenized_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1k09tKha3Lgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e789097-e1ee-4221-b424-5d336cfd854a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON James/NNP)\n",
            "  is/VBZ\n",
            "  working/VBG\n",
            "  at/IN\n",
            "  (ORGANIZATION Disney/NNP)\n",
            "  in/IN\n",
            "  (GPE London/NNP))\n"
          ]
        }
      ],
      "source": [
        "#개체명 인식\n",
        "\n",
        "ner_sentence = ne_chunk(tokenized_sentence)\n",
        "print(ner_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPX-WtSvPmm6"
      },
      "source": [
        "🔹 **2-(2)** NER task by spacy library\n",
        "\n",
        "\n",
        "* spaCy 는 자연어처리를 위한 파이썬 기반의 오픈 소스 라이브러리로 다음과 같은 기능을 제공한다. \n",
        "  * Tokenization \n",
        "  * POS tagging \n",
        "  * Lemmatization \n",
        "  * Sentence Boundary Detection (SBD)\n",
        "  * Named Entity Recognition (NER)\n",
        "  * Similarity\n",
        "  * Text Classification\n",
        "  * Rule-based Matching\n",
        "  * Training\n",
        "  * Serialization\n",
        "\n",
        "* spaCy 와 NER\n",
        "  * .ents → .label_\n",
        "\n",
        "\n",
        "📌 [basic code](https://frhyme.github.io/python-lib/nlp_spacy_1/) 👉 필수 (NER 부분만)\n",
        "\n",
        "📌 [kaggle_Custom NER using SpaCy](https://www.kaggle.com/code/amarsharma768/custom-ner-using-spacy/notebook) 👉 선택\n",
        "\n",
        "  * 훈련되지 않은 데이터 세트에 명명된 엔티티를 학습하는 방법 : 이력서 pdf 데이터 활용 \n",
        "  * manually labelled \n",
        "\n",
        "📌 [한국어 NER](https://github.com/monologg/KoBERT-NER) 👉 참고하면 좋을 자료\n",
        "\n",
        "➕ [참고](http://aispiration.com/nlp2/nlp-ner-python.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXjRfz-qP0Xx",
        "outputId": "9760e83a-3d65-48e7-dbf7-0d0b1a0ebee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "U.K. GPE\n",
            "$1 billion MONEY\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp('Apple is looking at buying U.K. startup for $1 billion')\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"\"\"But Google is starting from behind. The company made a late push\n",
        "into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa\n",
        "software, which runs on its Echo and Dot devices, have clear leads in\n",
        "consumer adoption.\"\"\".replace(\"\\n\", \" \").strip())\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSsHTiVrOyaH",
        "outputId": "7ef1a332-eb88-4489-d3d0-485a745038c7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "Apple’s Siri ORG\n",
            "iPhones ORG\n",
            "Amazon ORG\n",
            "Alexa ORG\n",
            "Echo GPE\n",
            "Dot ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "008-V5QsQG25"
      },
      "source": [
        "###**3️⃣ Dependency Parsing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQfcodHQQPlt"
      },
      "source": [
        "👀 **내용 복습** \n",
        "* 문장의 전체적인 구성/구조 보다는 각 개별단어 간의 '의존관계' 또는 '수식관계' 와 같은 단어간 관계를 파악하는 것이 목적인 NLP Task\n",
        "* 문장 해석의 모호성을 없애기 위해 Parsing 을 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJLAzZnbRNlL"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "🔹 **3-(1)** Dependency Parsing by spacy library\n",
        "\n",
        "\n",
        "* [basic](https://frhyme.github.io/python-lib/nlp_spacy_1/#navigating-parse-tree) 👉 dependecy parsing 부분만 필수\n",
        "* .dep_ 메서드\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HbQEYt76bJXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e1a861-4a84-44b5-cd01-db2f6a81f877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'generator'>\n",
            "<class 'spacy.tokens.span.Span'>\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "============================================================\n",
            "Text: The original noun chunk text.\n",
            "Root text: The original text of the word connecting the noun chunk to the rest of the parse.\n",
            "Root dep: Dependency relation connecting the root to its head.\n",
            "Root head text: The text of the root token's head.\n",
            "============================================================\n",
            "          Autonomous cars                     cars                    nsubj                    shift\n",
            "      insurance liability                liability                     dobj                    shift\n",
            "            manufacturers            manufacturers                     pobj                   toward\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "\n",
        "noun_chunks = doc.noun_chunks\n",
        "print(type(noun_chunks))\n",
        "noun_chunk = list(noun_chunks)[0]\n",
        "print(type(noun_chunk))\n",
        "token = noun_chunk[0]\n",
        "print(type(token))\n",
        "\n",
        "print(\"==\"*30)\n",
        "print(\"\"\"\n",
        "Text: The original noun chunk text.\n",
        "Root text: The original text of the word connecting the noun chunk to the rest of the parse.\n",
        "Root dep: Dependency relation connecting the root to its head.\n",
        "Root head text: The text of the root token's head.\n",
        "\"\"\".strip())\n",
        "print(\"==\"*30)\n",
        "str_format = \"{:>25}\"*4\n",
        "for chunk in doc.noun_chunks:\n",
        "  print(str_format.format(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9QAEsrLAxHP"
      },
      "outputs": [],
      "source": [
        "#nevigating parse tree\n",
        "\n",
        "doc=nlp(\"Autonomous cars shift insurance liability toward manufacurers\")\n",
        "for tok in doc:\n",
        "  print(tok.text)\n",
        "  children = list(tok.children)\n",
        "  print('childre:', children, 'head:', tok.head if tok.head != tok else \"!this is root node\")\n",
        "  print(\"==\"*16)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#이를 간단하게 네트워크로 표현\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nG = nx.Graph()\n",
        "doc[2] ##root node\n",
        "\n",
        "def add_n_to_g(inputG, tok):\n",
        "  inputG.add_node(tok)\n",
        "  children = list(tok.children)\n",
        "  if children != []:\n",
        "    inputG.add_nodes_from(children)\n",
        "    for c in children:\n",
        "      inputG.add_edges_from([(tok, c, {'dependency':c.dep_})])\n",
        "      add_n_to_g(inputG, c)\n",
        "\n",
        "add_n_to_g(nG, doc[2])\n",
        "print(nG.nodes(data=True))\n",
        "print(\"==\"*20)\n",
        "for e in nG.edges(data=True):\n",
        "  print(f\"{e[0]}, {e[1]}, ### dependency: {e[2]['dependency']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6JlyuP_Sc3y",
        "outputId": "0dfb5014-52a0-4925-a801-ada62da5d80b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(shift, {}), (cars, {}), (liability, {}), (toward, {}), (Autonomous, {}), (insurance, {}), (manufacturers, {})]\n",
            "========================================\n",
            "shift, cars, ### dependency: nsubj\n",
            "shift, liability, ### dependency: dobj\n",
            "shift, toward, ### dependency: prep\n",
            "cars, Autonomous, ### dependency: amod\n",
            "liability, insurance, ### dependency: compound\n",
            "toward, manufacturers, ### dependency: pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQD5oiGgRfHe"
      },
      "source": [
        "🔹 **3-(2)** Spacy (kaggle) \n",
        "\n",
        "* 캐글 노트북 환경에서 실습해보는 것을 권장드립니다!\n",
        "\n",
        "* [kaggle_spaCy](https://www.kaggle.com/code/nirant/hitchhiker-s-guide-to-nlp-in-spacy) 👉 필수\n",
        "  * 도날드 트럼프 트위터 트윗 내용 데이터 분석\n",
        "\n",
        "\n",
        "👀 **노트북 키포인트** \n",
        "  1. spacy.display 메서드를 사용한 NER 시각화 \n",
        "  2. Tagging 을 통한 트럼프 트윗 분석 : noun_chunks 는 dependency graph를 고려하여, noun phrase를 뽑아준다. \n",
        "  3. [spacy Match](https://yujuwon.tistory.com/entry/spaCy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-Rule-based-Matching) : 직접 문장/단어 패턴을 등록하여 parsing\n",
        "  4. Question and answering task using Dependency Parsing\n",
        "    * spacy display :  ``style = 'dep'``\n",
        "    * .dep_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuHGKITRbKYq"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "!python -m spacy download en_vectors_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh9aW0ztYog9",
        "outputId": "051df29c-27ee-48a9-e52b-15a1ba92dacf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "uMArOUrxAJ8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f7510a3-b1dc-4b00-c328-f139b876b1a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (0,1,2,3,4,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "tweets = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/all_djt_tweets.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_text_entities(text):\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        print(f'Entity: {ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')"
      ],
      "metadata": {
        "id": "MZMShAaIZgx5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explain_text_entities(tweets['text'][9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JatTPATKZj3C",
        "outputId": "3a4aa607-b602-4699-eb6e-f3955832105f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Friday, Label: DATE, Absolute or relative dates or periods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_sentence = tweets['text'][0]\n",
        "doc = nlp(one_sentence)\n",
        "spacy.displacy.render(doc, style='ent',jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "MJ_PGUF-Zmc1",
        "outputId": "4edf1551-e5e6-49a8-d31c-08c3c707756e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Over 90%\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
              "</mark>\n",
              " approval rating for your all time favorite (I hope) President within \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Republican Party\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    52%\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
              "</mark>\n",
              " overall. This despite all of the made up stories by \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Fake News Media\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " trying endlessly to make me look as bad and evil as possible. Look at the real villains please!</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def redact_names(text):\n",
        "    doc = nlp(text)\n",
        "    redacted_sentence = []\n",
        "    for ent in doc.ents:\n",
        "        ent.merge()\n",
        "    for token in doc:\n",
        "        if token.ent_type_ == \"PERSON\":\n",
        "            redacted_sentence.append(\"[REDACTED]\")\n",
        "        else:\n",
        "            redacted_sentence.append(token.string)\n",
        "    return \"\".join(redacted_sentence)"
      ],
      "metadata": {
        "id": "Bvimts5jZ9Vv"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = tweets['text'][9]\n",
        "doc = nlp(example_text)\n",
        "spacy.displacy.render(doc, style='ent', jupyter=True)\n",
        "\n",
        "for idx, sentence in enumerate(doc.sents):\n",
        "    for noun in sentence.noun_chunks:\n",
        "        print(f\"sentence {idx+1} has noun chunk '{noun}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "pO341b4YabIV",
        "outputId": "c1962348-d9bb-493c-e924-18390b77f94a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Stock Market hit all time high on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Friday\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". Congratulations U.S.A.!</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence 1 has noun chunk 'Stock Market'\n",
            "sentence 1 has noun chunk 'all time'\n",
            "sentence 1 has noun chunk 'Friday'\n",
            "sentence 2 has noun chunk 'Congratulations U.S.A.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_sentence = tweets['text'][300]\n",
        "doc = nlp(one_sentence)\n",
        "spacy.displacy.render(doc, style='ent', jupyter=True)\n",
        "\n",
        "for token in doc:\n",
        "    print(token, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 922
        },
        "id": "eiDOcTfoado1",
        "outputId": "b411aca0-5864-41f8-ea2a-8a1a4ec839fe"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Incredibly beautiful ceremony as \n",
              "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.S. Korean War\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
              "</mark>\n",
              " remains are returned to \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    American\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " soil. Thank you to \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Honolulu\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " and all of our great Military participants on a job well done. A special thanks to Vice President \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mike Pence\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " on delivering a truly magnificent tribute!</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Incredibly ADV\n",
            "beautiful ADJ\n",
            "ceremony NOUN\n",
            "as SCONJ\n",
            "U.S. PROPN\n",
            "Korean PROPN\n",
            "War PROPN\n",
            "remains VERB\n",
            "are AUX\n",
            "returned VERB\n",
            "to ADP\n",
            "American ADJ\n",
            "soil NOUN\n",
            ". PUNCT\n",
            "Thank VERB\n",
            "you PRON\n",
            "to ADP\n",
            "Honolulu PROPN\n",
            "and CCONJ\n",
            "all PRON\n",
            "of ADP\n",
            "our PRON\n",
            "great ADJ\n",
            "Military ADJ\n",
            "participants NOUN\n",
            "on ADP\n",
            "a DET\n",
            "job NOUN\n",
            "well ADV\n",
            "done VERB\n",
            ". PUNCT\n",
            "A DET\n",
            "special ADJ\n",
            "thanks NOUN\n",
            "to ADP\n",
            "Vice PROPN\n",
            "President PROPN\n",
            "Mike PROPN\n",
            "Pence PROPN\n",
            "on ADP\n",
            "delivering VERB\n",
            "a DET\n",
            "truly ADV\n",
            "magnificent ADJ\n",
            "tribute NOUN\n",
            "! PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tweets['text'].str.cat(sep=' ')\n",
        "# spaCy enforces a max limit of 1000000 characters for NER and similar use cases.\n",
        "# Since `text` might be longer than that, we will slice it off here\n",
        "max_length = 1000000-1\n",
        "text = text[:max_length]\n",
        "\n",
        "# removing URLs and '&amp' substrings using regex\n",
        "import re\n",
        "url_reg  = r'[a-z]*[:.]+\\S+'\n",
        "text   = re.sub(url_reg, '', text)\n",
        "noise_reg = r'\\&amp'\n",
        "text   = re.sub(noise_reg, '', text)"
      ],
      "metadata": {
        "id": "MZyH-K3raf5h"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "BKhqDtU_ahGR"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "items_of_interest = list(doc.noun_chunks)\n",
        "items_of_interest = [str(x) for x in items_of_interest]"
      ],
      "metadata": {
        "id": "N6K_Slf9azlh"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trump_topics = []\n",
        "for token in doc:\n",
        "    if (not token.is_stop) and (token.pos_ == \"NOUN\") and (len(str(token))>2):\n",
        "        trump_topics.append(token)\n",
        "        \n",
        "trump_topics = [str(x) for x in trump_topics]"
      ],
      "metadata": {
        "id": "bjuNOwzDbUHj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trump_topics = []\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ not in [\"PERCENT\", \"CARDINAL\", \"DATE\"]:\n",
        "#         print(ent.text,ent.label_)\n",
        "        trump_topics.append(ent.text.strip())"
      ],
      "metadata": {
        "id": "NLjo4qnVbWsp"
      },
      "execution_count": 47,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}