{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“Œ  **ìƒë‹¨ì˜ 'íŒŒì¼-ë“œë¼ì´ë¸Œì— ì‚¬ë³¸ ì €ì¥' í•´ì„œ ë³¸ì¸ ë“œë¼ì´ë¸Œì— ì €ì¥ëœ ì‚¬ë³¸ ì´ìš©í•´ì„œ ì‹¤ìŠµ í•´ì£¼ì„¸ìš”!!**\n",
        "\n",
        "ğŸ“Œ week13 ë³µìŠµìŠµê³¼ì œëŠ” **NLG ì‹¤ìŠµ**ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ìœ„í‚¤ë…ìŠ¤ì˜ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ êµì¬ ì‹¤ìŠµ, ê´€ë ¨ ë¸”ë¡œê·¸ ë“±ì˜ ë¬¸ì„œ ìë£Œë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” ê³¼ì œì…ë‹ˆë‹¤. \n",
        "\n",
        "ğŸ“Œ ì•ˆë‚´ëœ ë§í¬ì— ë§ì¶”ì–´ **ì§ì ‘ ì½”ë“œë¥¼ ë”°ë¼ ì¹˜ë©´ì„œ (í•„ì‚¬)** í•´ë‹¹ nlp task ì˜ ê¸°ë³¸ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë©”ì„œë“œë¥¼ ìˆ™ì§€í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ğŸ˜Š í•„ìˆ˜ë¼ê³  ì²´í¬í•œ ë¶€ë¶„ì€ ê³¼ì œì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì£¼ì‹œê³ , ì„ íƒìœ¼ë¡œ ì²´í¬í•œ ë¶€ë¶„ì€ ììœ¨ì ìœ¼ë¡œ ìŠ¤í„°ë”” í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ê¶ê¸ˆí•œ ì‚¬í•­ì€ ê¹ƒí—ˆë¸Œ ì´ìŠˆë‚˜, ì¹´í†¡ë°©, ì„¸ì…˜ ë°œí‘œ ì‹œì‘ ì´ì „ ì‹œê°„ ë“±ì„ í™œìš©í•˜ì—¬ ììœ ë¡­ê²Œ ê³µìœ í•´ì£¼ì„¸ìš”!"
      ],
      "metadata": {
        "id": "BX3ac8Ag1RPC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppuTdZVWSo0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk colab í™˜ê²½ì—ì„œ ì‹¤í–‰ì‹œ í•„ìš”í•œ ì½”ë“œì…ë‹ˆë‹¤. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "4JEquLR91VBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c234f6-41d9-49f6-cfc9-20fb18732a9d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ¥° **ì´í•˜ ì˜ˆì œë¥¼ ì‹¤ìŠµí•˜ì‹œë©´ ë©ë‹ˆë‹¤.**\n",
        "\n",
        "**1-(1)~(2)ëŠ” í•„ìˆ˜ê³¼ì œ, 2ëŠ” ì„ íƒê³¼ì œì…ë‹ˆë‹¤.**\n"
      ],
      "metadata": {
        "id": "Kq8aMYKGPQR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1ï¸âƒ£ NLG task ì‹¤ìŠµ**"
      ],
      "metadata": {
        "id": "SHTPAk95iNtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‘€ ë‚´ìš© ë³µìŠµ\n",
        "\n",
        "NLG ëŠ” ìƒˆë¡œìš´ text ë¥¼ ë§Œë“¤ì–´ ë‚´ëŠ” ëª¨ë“  task ë¥¼ ì˜ë¯¸í•˜ë©° ê¸°ê³„ë²ˆì—­, í…ìŠ¤íŠ¸ ìš”ì•½, ì±„íŒ…, ìŠ¤í† ë¦¬í…”ë§, QA ë“±ì´ ìˆë‹¤. "
      ],
      "metadata": {
        "id": "j5msd7Igjz9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 1-(1) RNN ì„ ì´ìš©í•œ text generation \n",
        "\n",
        "ğŸ“Œ [Text generation with RNN](https://wikidocs.net/45101) \n",
        "\n",
        "* Simple RNN ì„ ì´ìš©í•œ ê°„ë‹¨í•œ í•œêµ­ì–´ text generation ì˜ˆì œì™€ LSTM ì„ ì´ìš©í•œ ë‰´ìš• íƒ€ì„ì¦ˆ ê¸°ì‚¬ í—¤ë“œë¼ì¸ ìƒì„± ì˜ˆì œë¥¼ í•„ì‚¬í•´ì£¼ì„¸ìš”."
      ],
      "metadata": {
        "id": "9L-jAHPkiBV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "text = \"\"\"ê²½ë§ˆì¥ì— ìˆëŠ” ë§ì´ ë›°ê³  ìˆë‹¤\\n\n",
        "ê·¸ì˜ ë§ì´ ë²•ì´ë‹¤\\n\n",
        "ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ê³±ë‹¤\\n\"\"\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° : %d' % vocab_size)\n",
        "\n",
        "sequences = list()\n",
        "for line in text.split('\\n'):\n",
        "    encoded = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "print('í•™ìŠµì— ì‚¬ìš©í•  ìƒ˜í”Œì˜ ê°œìˆ˜: %d' %len(sequences))\n"
      ],
      "metadata": {
        "id": "Klj6gjETZdgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894a5235-36a8-4eac-b07a-4e548769da38"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° : 12\n",
            "í•™ìŠµì— ì‚¬ìš©í•  ìƒ˜í”Œì˜ ê°œìˆ˜: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(l) for l in sequences) # ëª¨ë“  ìƒ˜í”Œì—ì„œ ê¸¸ì´ê°€ ê°€ì¥ ê¸´ ìƒ˜í”Œì˜ ê¸¸ì´ ì¶œë ¥\n",
        "\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "X = sequences[:, :-1]\n",
        "y = sequences[:, -1]\n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "metadata": {
        "id": "MsJZ9ldFiARK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
        "\n",
        "embedding_dim = 10\n",
        "hidden_units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X,y, epochs=100, verbose=2)\n",
        "\n",
        "\n",
        "def sentence_generation(model, tokenizer, current_word, n):\n",
        "    init_word = current_word\n",
        "    sentence = ''\n",
        "\n",
        "    for _ in range(n):\n",
        "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n",
        "\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == result:\n",
        "                break\n",
        "\n",
        "        current_word = current_word + ' ' + word\n",
        "\n",
        "        sentence = sentence + ' ' + word\n",
        "\n",
        "    sentence = init_word + sentence\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "_jBuYL0y2za6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model, tokenizer, 'ê²½ë§ˆì¥ì—', 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw2d8JLK5p3N",
        "outputId": "01068f83-eb03-4000-9b67-afad3154eb77"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ê²½ë§ˆì¥ì— ë§ì´ ë§ì´ ë›°ê³  ìˆë‹¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 1-(2) Text summarization with attention\n",
        "\n",
        "ğŸ“Œ [ì•„ë§ˆì¡´ ë¦¬ë·° ìš”ì•½](https://wikidocs.net/72820) \n",
        "\n",
        "* seq2seq + attention ì„ ì´ìš©í•œ ì•„ë§ˆì¡´ ë¦¬ë·° ê¸€ text summarization ì˜ˆì œë¥¼ í•„ì‚¬í•´ì£¼ì„¸ìš”."
      ],
      "metadata": {
        "id": "ktAuag_Nk5Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hX45wYr6vM-",
        "outputId": "a913ee93-7fba-4ae7-9f35-5d5189ec49e6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import urllib.request\n",
        "np.random.seed(seed=0)\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Euron NLP/á„‡á…©á†¨á„‰á…³á†¸á„€á…ªá„Œá…¦/Week13/Reviews.csv', nrows=100000)"
      ],
      "metadata": {
        "id": "mLmPLEYdZlzX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì „ì²˜ë¦¬ í•¨ìˆ˜ ë‚´ ì‚¬ìš©\n",
        "contractions = {\"'cause\": 'because',\n",
        " \"I'd\": 'I would',\n",
        " \"I'd've\": 'I would have',\n",
        " \"I'll\": 'I will',\n",
        " \"I'll've\": 'I will have',\n",
        " \"I'm\": 'I am',\n",
        " \"I've\": 'I have',\n",
        " \"ain't\": 'is not',\n",
        " \"aren't\": 'are not',\n",
        " \"can't\": 'cannot',\n",
        " \"could've\": 'could have',\n",
        " \"couldn't\": 'could not',\n",
        " \"didn't\": 'did not',\n",
        " \"doesn't\": 'does not',\n",
        " \"don't\": 'do not',\n",
        " \"hadn't\": 'had not',\n",
        " \"hasn't\": 'has not',\n",
        " \"haven't\": 'have not',\n",
        " \"he'd\": 'he would',\n",
        " \"he'll\": 'he will',\n",
        " \"he's\": 'he is',\n",
        " \"here's\": 'here is',\n",
        " \"how'd\": 'how did',\n",
        " \"how'd'y\": 'how do you',\n",
        " \"how'll\": 'how will',\n",
        " \"how's\": 'how is',\n",
        " \"i'd\": 'i would',\n",
        " \"i'd've\": 'i would have',\n",
        " \"i'll\": 'i will',\n",
        " \"i'll've\": 'i will have',\n",
        " \"i'm\": 'i am',\n",
        " \"i've\": 'i have',\n",
        " \"isn't\": 'is not',\n",
        " \"it'd\": 'it would',\n",
        " \"it'd've\": 'it would have',\n",
        " \"it'll\": 'it will',\n",
        " \"it'll've\": 'it will have',\n",
        " \"it's\": 'it is',\n",
        " \"let's\": 'let us',\n",
        " \"ma'am\": 'madam',\n",
        " \"mayn't\": 'may not',\n",
        " \"might've\": 'might have',\n",
        " \"mightn't\": 'might not',\n",
        " \"mightn't've\": 'might not have',\n",
        " \"must've\": 'must have',\n",
        " \"mustn't\": 'must not',\n",
        " \"mustn't've\": 'must not have',\n",
        " \"needn't\": 'need not',\n",
        " \"needn't've\": 'need not have',\n",
        " \"o'clock\": 'of the clock',\n",
        " \"oughtn't\": 'ought not',\n",
        " \"oughtn't've\": 'ought not have',\n",
        " \"sha'n't\": 'shall not',\n",
        " \"shan't\": 'shall not',\n",
        " \"shan't've\": 'shall not have',\n",
        " \"she'd\": 'she would',\n",
        " \"she'd've\": 'she would have',\n",
        " \"she'll\": 'she will',\n",
        " \"she'll've\": 'she will have',\n",
        " \"she's\": 'she is',\n",
        " \"should've\": 'should have',\n",
        " \"shouldn't\": 'should not',\n",
        " \"shouldn't've\": 'should not have',\n",
        " \"so's\": 'so as',\n",
        " \"so've\": 'so have',\n",
        " \"that'd\": 'that would',\n",
        " \"that'd've\": 'that would have',\n",
        " \"that's\": 'that is',\n",
        " \"there'd\": 'there would',\n",
        " \"there'd've\": 'there would have',\n",
        " \"there's\": 'there is',\n",
        " \"they'd\": 'they would',\n",
        " \"they'd've\": 'they would have',\n",
        " \"they'll\": 'they will',\n",
        " \"they'll've\": 'they will have',\n",
        " \"they're\": 'they are',\n",
        " \"they've\": 'they have',\n",
        " \"this's\": 'this is',\n",
        " \"to've\": 'to have',\n",
        " \"wasn't\": 'was not',\n",
        " \"we'd\": 'we would',\n",
        " \"we'd've\": 'we would have',\n",
        " \"we'll\": 'we will',\n",
        " \"we'll've\": 'we will have',\n",
        " \"we're\": 'we are',\n",
        " \"we've\": 'we have',\n",
        " \"weren't\": 'were not',\n",
        " \"what'll\": 'what will',\n",
        " \"what'll've\": 'what will have',\n",
        " \"what're\": 'what are',\n",
        " \"what's\": 'what is',\n",
        " \"what've\": 'what have',\n",
        " \"when's\": 'when is',\n",
        " \"when've\": 'when have',\n",
        " \"where'd\": 'where did',\n",
        " \"where's\": 'where is',\n",
        " \"where've\": 'where have',\n",
        " \"who'll\": 'who will',\n",
        " \"who'll've\": 'who will have',\n",
        " \"who's\": 'who is',\n",
        " \"who've\": 'who have',\n",
        " \"why's\": 'why is',\n",
        " \"why've\": 'why have',\n",
        " \"will've\": 'will have',\n",
        " \"won't\": 'will not',\n",
        " \"won't've\": 'will not have',\n",
        " \"would've\": 'would have',\n",
        " \"wouldn't\": 'would not',\n",
        " \"wouldn't've\": 'would not have',\n",
        " \"y'all\": 'you all',\n",
        " \"y'all'd\": 'you all would',\n",
        " \"y'all'd've\": 'you all would have',\n",
        " \"y'all're\": 'you all are',\n",
        " \"y'all've\": 'you all have',\n",
        " \"you'd\": 'you would',\n",
        " \"you'd've\": 'you would have',\n",
        " \"you'll\": 'you will',\n",
        " \"you'll've\": 'you will have',\n",
        " \"you're\": 'you are',\n",
        " \"you've\": 'you have'}"
      ],
      "metadata": {
        "id": "GXRxu3Sn8uZI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[['Text', 'Summary']]\n",
        "data.drop_duplicates(subset=['Text'], inplace=True)\n",
        "\n",
        "data.dropna(axis=0, inplace=True)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_sentence(sentence, remove_stopwords=True):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = BeautifulSoup(sentence, \"lxml\").text\n",
        "    sentence = re.sub(r'\\([^)]*\\)', '', sentence)\n",
        "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")])\n",
        "    sentence = re.sub(r\"'s\\b\", \"\", sentence)\n",
        "    sentence = re.sub('[m]{2,}', 'mm', sentence)\n",
        "\n",
        "    if remove_stopwords:\n",
        "         tokens = ' '.join(word for  word in sentence.split() if not word in stop_words if len(word)>1 )\n",
        "    else:\n",
        "         tokens = ' '.join(word for word in sentence.split() if len(word)>1)\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "VWINMRvq6tn9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPM8vAAS_Bqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
        "temp_summary = 'Great way to start (or finish) the day!!!'\n",
        "print(preprocess_sentence(temp_text))\n",
        "print(preprocess_sentence(temp_summary, 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUHqmIEv6ba2",
        "outputId": "fe5076d1-1388-4ab7-875f-e0d29cf24500"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "everything bought great, infact ordered twice third ordered wasfor mother father.\n",
            "great way to start the day!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text ì—´ ì „ì²˜ë¦¬ \n",
        "clean_text = []\n",
        "for s in data['Text']:\n",
        "    clean_text.append(preprocess_sentence(s))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d7T1b4f96bc4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary ì—´ ì „ì²˜ë¦¬\n",
        "clean_summary = []\n",
        "for s in data['Summary']:\n",
        "    clean_summary.append(preprocess_sentence(s,0))\n",
        "clean_summary[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbA5UPbB-3gr",
        "outputId": "14ace164-9e73-46de-de2c-11ea65556d7f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.amazon.com/gp/product/b007i7yygy/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['good quality dog food',\n",
              " 'not as advertised',\n",
              " '\"delight\" says it all',\n",
              " 'cough medicine',\n",
              " 'great taffy']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Text'] = clean_text\n",
        "data['Summary'] = clean_summary"
      ],
      "metadata": {
        "id": "xcgptSxJ3zhP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_max_len = 50\n",
        "summary_max_len = 8\n",
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "    cnt=0\n",
        "    for s in nested_list:\n",
        "        if(len(s.split()) <= max_len):\n",
        "            cnt = cnt + 1\n",
        "    print('ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ %s ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: %s'%(max_len, (cnt / len(nested_list))))\n",
        "\n",
        "below_threshold_len(text_max_len, data['Text'])\n",
        "below_threshold_len(summary_max_len, data['Summary'])\n",
        "\n",
        "data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
        "data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
        "print('ì „ì²´ ìƒ˜í”Œìˆ˜ :',(len(data)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D70zP-O4_HQy",
        "outputId": "d85dc703-a7b9-4d7d-8cd9-f722e92c6f5a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ 50 ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: 0.7614701724625389\n",
            "ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ 8 ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: 0.9434322872490811\n",
            "ì „ì²´ ìƒ˜í”Œìˆ˜ : 64852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ìš”ì•½ ë°ì´í„°ì—ëŠ” ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€í•œë‹¤. \n",
        "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken ' + x)\n",
        "data['decoder_target'] = data['Summary'].apply(lambda x : x + 'eostoken ')\n",
        "\n",
        "encoder_input = np.array(data['Text'])\n",
        "decoder_input = np.array(data['decoder_input'])\n",
        "decoder_target = np.array(data['decoder_target'])\n"
      ],
      "metadata": {
        "id": "W62xoyyx_HTD"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) ë°ì´í„°ì˜ ë¶„ë¦¬\n",
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]\n",
        "\n",
        "n_of_val = int(len(encoder_input)*0.2)\n",
        "\n",
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]\n",
        "                                   "
      ],
      "metadata": {
        "id": "HQOWaFcMAHWl"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4)ì •ìˆ˜ ì¸ì½”ë”©\n",
        "\n",
        "src_tokenizer = Tokenizer()\n",
        "src_tokenizer.fit_on_texts(encoder_input_train)\n",
        "\n",
        "threshold = 7 \n",
        "total_cnt = len(src_tokenizer.word_index)\n",
        "rare_cnt = 0\n",
        "total_freq = 0 \n",
        "rare_freq = 0 \n",
        "\n",
        "for key, value in src_tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    if(value<threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq +value\n",
        "\n",
        "print('ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° :',total_cnt)\n",
        "print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))\n",
        "print('ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ë¥¼ ì œì™¸ì‹œí‚¬ ê²½ìš°ì˜ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° %s'%(total_cnt - rare_cnt))\n",
        "print(\"ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:\", (rare_freq / total_freq)*100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqdDOn0JAHY5",
        "outputId": "221add8d-8746-43fe-a121-1dd1802c9a43"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° : 33781\n",
            "ë“±ì¥ ë¹ˆë„ê°€ 6ë²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: 25324\n",
            "ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ë¥¼ ì œì™¸ì‹œí‚¬ ê²½ìš°ì˜ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° 8457\n",
            "ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨: 74.96521713389184\n",
            "ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨: 3.4570137129713476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = 8000\n",
        "src_tokenizer = Tokenizer(num_words = src_vocab)\n",
        "src_tokenizer.fit_on_texts(encoder_input_train)\n",
        "\n",
        "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train)\n",
        "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
        "\n",
        "tar_tokenizer = Tokenizer()\n",
        "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
        "\n",
        "threshold = 6\n",
        "total_cnt = len(tar_tokenizer.word_index)\n",
        "rare_cnt = 0\n",
        "total_freq = 0\n",
        "rare_freq = 0 \n",
        "\n",
        "# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ì˜ ìŒì„ keyì™€ valueë¡œ ë§Œë“ ë‹¤.\n",
        "for key, value in tar_tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ìœ¼ë©´\n",
        "    if(value<threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "\n",
        "print('ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° :',total_cnt)\n",
        "print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))\n",
        "print('ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ë¥¼ ì œì™¸ì‹œí‚¬ ê²½ìš°ì˜ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° %s'%(total_cnt - rare_cnt))\n",
        "print(\"ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:\", (rare_freq / total_freq)*100)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j5Ee8B7CsbW",
        "outputId": "ca384f9b-f59e-4b22-c802-747522508f43"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° : 13712\n",
            "ë“±ì¥ ë¹ˆë„ê°€ 5ë²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: 10941\n",
            "ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ë¥¼ ì œì™¸ì‹œí‚¬ ê²½ìš°ì˜ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° 2771\n",
            "ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨: 79.7914235705951\n",
            "ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨: 10.224984532313403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tar_vocab = 2000\n",
        "tar_tokenizer = Tokenizer(num_words = tar_vocab) \n",
        "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
        "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train)\n",
        "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
        "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
        "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrGYrvIAD-Y6",
        "outputId": "4a0a4a75-0b1d-421b-ad65-f4e333324876"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6, 729, 5], [1752, 769], [650, 125, 543], [6, 940, 20, 29], [1049, 13, 8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) ë¹ˆ ìƒ˜í”Œ ì œê±° \n",
        "\n",
        "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence)==1]\n",
        "\n",
        "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence)==1]\n",
        "\n",
        "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
        "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
        "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
        "\n",
        "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
        "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
        "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
        "\n",
        "print('í›ˆë ¨ ë°ì´í„°ì˜ ê°œìˆ˜ :', len(encoder_input_train))\n",
        "print('í›ˆë ¨ ë ˆì´ë¸”ì˜ ê°œìˆ˜ :',len(decoder_input_train))\n",
        "print('í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ê°œìˆ˜ :',len(encoder_input_test))\n",
        "print('í…ŒìŠ¤íŠ¸ ë ˆì´ë¸”ì˜ ê°œìˆ˜ :',len(decoder_input_test))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyLm-zDMEmlU",
        "outputId": "bc0c49d9-e04e-4fad-d336-a8d414386d02"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í›ˆë ¨ ë°ì´í„°ì˜ ê°œìˆ˜ : 42945\n",
            "í›ˆë ¨ ë ˆì´ë¸”ì˜ ê°œìˆ˜ : 42945\n",
            "í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ê°œìˆ˜ : 10740\n",
            "í…ŒìŠ¤íŠ¸ ë ˆì´ë¸”ì˜ ê°œìˆ˜ : 10740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) íŒ¨ë”©í•˜ê¸° \n",
        "\n",
        "encoder_input_train = pad_sequences(encoder_input_train, maxlen = text_max_len, padding='post')\n",
        "encoder_input_test = pad_sequences(encoder_input_test, maxlen = text_max_len, padding='post')\n",
        "decoder_input_train = pad_sequences(decoder_input_train, maxlen = summary_max_len, padding='post')\n",
        "decoder_target_train = pad_sequences(decoder_target_train, maxlen = summary_max_len, padding='post')\n",
        "decoder_input_test = pad_sequences(decoder_input_test, maxlen = summary_max_len, padding='post')\n",
        "decoder_target_test = pad_sequences(decoder_target_test, maxlen = summary_max_len, padding='post')"
      ],
      "metadata": {
        "id": "3y0eieNuFeFr"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. seq2seq + attentionìœ¼ë¡œ ìš”ì•½ ëª¨ë¸ ì„¤ê³„ ë° í›ˆë ¨ì‹œí‚¤ê¸° \n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_size = 256\n",
        "\n",
        "# ì¸ì½”ë” \n",
        "encoder_inputs = Input(shape=(text_max_len,))\n",
        "\n",
        "# ì¸ì½”ë”ì˜ ì„ë² ë”© ì¸µ\n",
        "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
        "\n",
        "# ì¸ì½”ë”ì˜ LSTM 1\n",
        "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout =0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "# ì¸ì½”ë”ì˜ LSTM 2\n",
        "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# ì¸ì½”ë”ì˜ LSTM 3\n",
        "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# ë””ì½”ë”\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# ë””ì½”ë”ì˜ ì„ë² ë”© ì¸µ\n",
        "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# ë””ì½”ë”ì˜ LSTM\n",
        "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])"
      ],
      "metadata": {
        "id": "wX_0iU76Fezu"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë””ì½”ë”ì˜ ì¶œë ¥ì¸µ\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation = 'softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
        "\n",
        "# ëª¨ë¸ ì •ì˜\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InroKwOIFviV",
        "outputId": "955a1b3d-4a0a-4651-9ab9-0d99c905af0d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 50)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_7 (Embedding)        (None, 50, 128)      1024000     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 50, 256),    394240      ['embedding_7[0][0]']            \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, 50, 256),    525312      ['lstm_2[0][0]']                 \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " embedding_8 (Embedding)        (None, None, 128)    256000      ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)                  [(None, 50, 256),    525312      ['lstm_3[0][0]']                 \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_5 (LSTM)                  [(None, None, 256),  394240      ['embedding_8[0][0]',            \n",
            "                                 (None, 256),                     'lstm_4[0][1]',                 \n",
            "                                 (None, 256)]                     'lstm_4[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, None, 2000)   514000      ['lstm_5[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,633,104\n",
            "Trainable params: 3,633,104\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/drive/MyDrive/Euron NLP/á„‡á…©á†¨á„‰á…³á†¸á„€á…ªá„Œá…¦/Week13/attention.py\"\n",
        "exec(open(\"/content/drive/MyDrive/Euron NLP/á„‡á…©á†¨á„‰á…³á†¸á„€á…ªá„Œá…¦/Week13/attention.py\").read())"
      ],
      "metadata": {
        "id": "O3HkWdiKKDhZ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from attention.py import AttentionLayer\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# ì–´í…ì…˜ì˜ ê²°ê³½ì™€ ë””ì½”ë”ì˜ hidden stateë“¤ì„ ì—°ê²°\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# ë””ì½”ë”ì˜ ì¶œë ¥ì¸µ \n",
        "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
        "\n",
        "# ëª¨ë¸ ì •ì˜\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3d8iSVb9MO6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_index_to_word = src_tokenizer.index_word # ì›ë¬¸ ë‹¨ì–´ ì§‘í•©ì—ì„œ ì •ìˆ˜ -> ë‹¨ì–´ë¥¼ ì–»ìŒ\n",
        "tar_word_to_index = tar_tokenizer.word_index # ìš”ì•½ ë‹¨ì–´ ì§‘í•©ì—ì„œ ë‹¨ì–´ -> ì •ìˆ˜ë¥¼ ì–»ìŒ\n",
        "tar_index_to_word = tar_tokenizer.index_word # ìš”ì•½ ë‹¨ì–´ ì§‘í•©ì—ì„œ ì •ìˆ˜ -> ë‹¨ì–´ë¥¼ ì–»ìŒ"
      ],
      "metadata": {
        "id": "gl0M48fGOKIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2ï¸âƒ£ Text summarization task**"
      ],
      "metadata": {
        "id": "HfTr_BPwGc8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 2-(1) Pororo - text summarization \n",
        "\n",
        "ğŸ“Œ [ê³µì‹ë¬¸ì„œ](https://kakaobrain.github.io/pororo/seq2seq/summary.html) \n",
        "\n",
        "ğŸ“Œ [ì˜ˆì œ ì‹¤ìŠµ](https://teddylee777.github.io/machine-learning/nlp-korean-pororo) \n",
        "\n",
        "* PORORO : ì¹´ì¹´ì˜¤ ë¸Œë ˆì¸ì—ì„œ ì œê³µí•œ ìì—°ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬"
      ],
      "metadata": {
        "id": "eu3adOV2bDs_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e2WbVbbsatD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NmVEEfAqdSa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 2-(2) BERT ë¥¼ ì´ìš©í•œ text summarization \n",
        "\n",
        "ğŸ“Œ [ë…¼ë¬¸ ë¦¬ë·°](https://medium.com/@eyfydsyd97/bert%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%9A%94%EC%95%BD-text-summary-b582b5cc7d) \n",
        "\n",
        "ğŸ“Œ [BERT Extractive summarizer Library](https://github.com/dmmiller612/bert-extractive-summarizer) \n",
        "\n",
        "\n",
        "ğŸ“Œ [Text summarization Github Repo](https://github.com/uoneway/Text-Summarization-Repo) \n",
        "\n",
        "\n",
        "\n",
        "â• [BERT ë¥¼ ì´ìš©í•œ ë‰´ìŠ¤ ìš”ì•½ ìë™í™” App êµ¬í˜„ Repo](https://github.com/huydang90/News-Summarization-with-BERT) ğŸ‘‰ í”„ë¡œì íŠ¸ ì˜ˆì‹œ ì°¸ê³  ìë£Œ\n",
        "\n"
      ],
      "metadata": {
        "id": "xsx6OgzwbnIt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "svtikbdZatBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mnfL6KIZdSuo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}